- en: 'Spoken language recognition on Mozilla Common Voice — Part II: Models.'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/spoken-language-recognition-on-mozilla-common-voice-part-ii-models-b32780ea1ee4?source=collection_archive---------6-----------------------#2023-08-06](https://towardsdatascience.com/spoken-language-recognition-on-mozilla-common-voice-part-ii-models-b32780ea1ee4?source=collection_archive---------6-----------------------#2023-08-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sergeyvilov?source=post_page-----b32780ea1ee4--------------------------------)[![Sergey
    Vilov](../Images/42efe223e2aa575250e050cf3660cf20.png)](https://medium.com/@sergeyvilov?source=post_page-----b32780ea1ee4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b32780ea1ee4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b32780ea1ee4--------------------------------)
    [Sergey Vilov](https://medium.com/@sergeyvilov?source=post_page-----b32780ea1ee4--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33297faf768d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-part-ii-models-b32780ea1ee4&user=Sergey+Vilov&userId=33297faf768d&source=post_page-33297faf768d----b32780ea1ee4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b32780ea1ee4--------------------------------)
    ·7 min read·Aug 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb32780ea1ee4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-part-ii-models-b32780ea1ee4&user=Sergey+Vilov&userId=33297faf768d&source=-----b32780ea1ee4---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb32780ea1ee4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-part-ii-models-b32780ea1ee4&source=-----b32780ea1ee4---------------------bookmark_footer-----------)![](../Images/bba82297c280c131ff3cdf040af52ec2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Jonathan Velasquez](https://unsplash.com/@jonathanvez?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This is the second article on spoken language recognition based on [Mozilla
    Common Voice](https://commonvoice.mozilla.org/) dataset. In the [first part](https://medium.com/towards-data-science/spoken-language-recognition-on-mozilla-common-voice-part-i-3f5400bbbcd8)
    we discussed data selection and chose optimal embedding. Let us now train several
    models and select the best one.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '**Model comparison**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now train and evaluate the following models on the full data (40K samples,
    see the [first part](https://medium.com/towards-data-science/spoken-language-recognition-on-mozilla-common-voice-part-i-3f5400bbbcd8)
    for more info on data selection and preprocessing):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将对以下模型在完整数据（40K 样本，请参见[第一部分](https://medium.com/towards-data-science/spoken-language-recognition-on-mozilla-common-voice-part-i-3f5400bbbcd8)获取更多数据选择和预处理信息）上进行训练和评估：
- en: · Convolutional neural network (CNN) model. We simply treat language classification
    problem as classification of 2-dimensional images. CNN-based classifiers [showed](https://github.com/pietz/language-recognition)
    promising results in a language recognition TopCoder competition.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: · 卷积神经网络（CNN）模型。我们简单地将语言分类问题视为 2 维图像的分类。基于 CNN 的分类器在语言识别 TopCoder 比赛中[显示](https://github.com/pietz/language-recognition)了有希望的结果。
- en: '![](../Images/946148853376e3d3621d76ab90638954.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/946148853376e3d3621d76ab90638954.png)'
- en: CNN architecture (Image by the author, created with [PlotNeuralNet](https://github.com/HarisIqbal88/PlotNeuralNet))
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 架构（图像由作者提供，使用[PlotNeuralNet](https://github.com/HarisIqbal88/PlotNeuralNet)创建）
- en: · CRNN model from Bartz et al. 2017\. A CRNN combines the descriptive power
    of CNNs with the ability to capture temporal features of RNN.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: · 来自 Bartz 等人 2017 的 CRNN 模型。CRNN 结合了 CNN 的描述能力和 RNN 捕捉时间特征的能力。
- en: '![](../Images/a53b69e2a5f1cd4996c2bcd779507843.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a53b69e2a5f1cd4996c2bcd779507843.png)'
- en: CRNN architecture (image from Bartz et al., 2017)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CRNN 架构（图像来自 Bartz 等，2017）
- en: · CRNN model from Alashban et al. 2022\. This is just another variation of the
    CRNN architecture.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: · 来自 Alashban 等人 2022 的 CRNN 模型。这只是 CRNN 架构的另一个变体。
- en: '· AttNN: model from De Andrade et al. 2018\. This model was initially proposed
    for speech recognition and subsequently [applied](https://github.com/zkmkarlsruhe/language-identification/tree/main)
    for spoken language recognition in the Intelligent Museum project. In addition
    to convolution and LSTM units, this model has a subsequent attention block that
    is trained to weigh parts of the input sequence (namely frames on which Fourier
    transform is computed) according to their relevance for classification.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: · AttNN：来自 De Andrade 等人 2018 的模型。该模型最初用于语音识别，后来在智能博物馆项目中[应用](https://github.com/zkmkarlsruhe/language-identification/tree/main)于口语语言识别。除了卷积和
    LSTM 单元，该模型还有一个后续的注意力块，经过训练以根据其分类相关性对输入序列的部分（即计算傅里叶变换的帧）进行加权。
- en: '· CRNN* model: same architecture as AttNN, but no attention block.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: · CRNN* 模型：与 AttNN 相同的架构，但没有注意力块。
- en: · Time-delay neural network (TDNN) model. The model we test here was used to
    generate X-vector embeddings for spoken language recognition in Snyder et al.
    2018\. In our study, we bypass X-vector generation and directly train the network
    to classify languages.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: · 时间延迟神经网络（TDNN）模型。我们在这里测试的模型用于生成 Snyder 等人 2018 的口语语言识别的 X-vector 嵌入。在我们的研究中，我们绕过
    X-vector 生成，直接训练网络来分类语言。
- en: All models were trained based on the same train/val/test split and the same
    mel spectrogram embeddings with the first 13 mel filterbank coefficients. The
    models can be found [here](https://github.com/sergeyvilov/MCV-spoken-language-recognition/tree/master).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型均基于相同的训练/验证/测试拆分和相同的梅尔谱图嵌入（前 13 个梅尔滤波器组系数）进行训练。模型可以在[这里](https://github.com/sergeyvilov/MCV-spoken-language-recognition/tree/master)找到。
- en: The resulting learning curves on the validation set are shown on the figure
    below (each “epoch” refers to 1/8 of the dataset).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集上的学习曲线如下图所示（每个“epoch”指的是数据集的 1/8）。
- en: '![](../Images/8e8f005eadcc598f049df62253bcee0e.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e8f005eadcc598f049df62253bcee0e.png)'
- en: Performance of different models on Mozilla Common Voice dataset (image by the
    author).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 各种模型在 Mozilla Common Voice 数据集上的表现（图像由作者提供）。
- en: The following table shows mean and standard deviation for the accuracy based
    on 10 runs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了基于 10 次运行的准确率的均值和标准差。
- en: '![](../Images/39ff126b8bec2ef8f66efedd7c7d1502.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39ff126b8bec2ef8f66efedd7c7d1502.png)'
- en: accuracy for each model (image by the author)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型的准确性（图像由作者提供）
- en: It can be clearly seen that AttNN, TDNN, and our CRNN* model perform similarly,
    with AttNN scoring the 1st with 92.4% accuracy. On the other hand, CRNN (Bartz
    et al. 2017), CNN, and CRNN (Alashban et al. 2022) showed very modest performance
    with CRNN (Alashban et al. 2022) closing the list with only 58.5% accuracy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 可以清楚地看到，AttNN、TDNN 和我们的 CRNN* 模型表现相似，其中 AttNN 以 92.4% 的准确率排名第一。另一方面，CRNN（Bartz
    等人 2017）、CNN 和 CRNN（Alashban 等人 2022）表现相当逊色，CRNN（Alashban 等人 2022）以仅 58.5% 的准确率位列末尾。
- en: We then trained the winning AttNN model on the train and val sets and evaluated
    on the test set. The test accuracy of 92.4% (92.4% for men and 92.3% for women)
    turned out to be close to validation accuracy, which indicates that the model
    did not overfit on the validation set.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: To understand the performance difference between the evaluated models, we first
    note that TDNN and AttNN were specifically designed for speech recognition tasks
    and already tested against previous benchmarks. This might be the reason why these
    models come out on top.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The performance gap between AttNN and our CRNN model (the same architecture
    but no attention block) proves the relevance of the attention mechanism for spoken
    language recognition. The following CRNN model (Bartz et al. 2017) performs worse
    despite its similar architecture. This is probably just because the default model
    hyperparameters are not optimal for the MCV dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The CNN model does not possess any specific memory mechanism and comes next.
    Strictly speaking, the CNN has some notion of memory since computing convolution
    involves a fixed number of consecutive frames. Higher layers thus encapsulate
    information of even longer time intervals due to the hierarchical nature of CNNs.
    In fact, the TDNN model, which scored the second, might be viewed as a 1-D CNN.
    So, with more time invested in CNN architecture search, the CNN model might have
    performed closely to TDNN.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: The CRNN model from Alashban et al. 2022 surprisingly shows the worst accuracy.
    It is interesting that this model was initially designed to recognize languages
    in MCV and showed accuracy of about 97%, as reported in the original study. Since
    the original code is not publicly available, it would be difficult to determine
    the source of this large discrepancy.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '**Pairwise accuracy**'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many cases the user employs regularly no more than 2 languages. In this case,
    a more appropriate metric of model performance is pairwise accuracy, which is
    nothing more than accuracy computed on a given pair of languages ignoring the
    scores for all other languages.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The pairwise accuracy for the AttNN model on the test set is shown in the table
    below next to the confusion matrix, the recall for individual languages being
    on diagonal. The average pairwise accuracy is 97%. Pairwise accuracy will always
    be higher than accuracy since only 2 languages need to be distinguished.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4a5f2b120865a9f601351d8953ff0117.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix (left) and pairwise accuracy (right) of the AttNN model (image
    by the author).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: So, the model distinguishes the best between German (de) and Spanish (es) as
    well as French (fr) and English (en) (98%). This is not surprising as the sound
    system is quite different in these languages.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Although we used softmax loss to train the model, it was previously reported
    that higher accuracy might be achieved in pairwise classification with [tuplemax
    loss](https://arxiv.org/abs/1811.12290) (Wan et al. 2019).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: To study the effect of tuplemax loss, we retrained our model after implementing
    tuplemax loss in PyTorch (see [here](https://github.com/sergeyvilov/MCV-spoken-language-recognition/blob/master/tuplemax_loss.py)
    for implementation). The figure below compares the effect of softmax loss and
    tuplemax loss on accuracy and on pairwise accuracy when evaluated on the validation
    set.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究tuplemax损失的影响，我们在PyTorch中实现了tuplemax损失，并重新训练了我们的模型（详见[这里](https://github.com/sergeyvilov/MCV-spoken-language-recognition/blob/master/tuplemax_loss.py)）。下图比较了在验证集上评估时softmax损失和tuplemax损失对准确率和成对准确率的影响。
- en: '![](../Images/d6dc42550952b2ee707391fcc6dabec3.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6dc42550952b2ee707391fcc6dabec3.png)'
- en: Accuracy and pairwise accuracy of the AttNN model computed with softmax and
    tuplemax loss (image by the author).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用softmax和tuplemax损失计算的AttNN模型的准确率和成对准确率（作者提供的图片）。
- en: As can be observed, tuplemax loss performs worse when overall accuracy (paired
    t-test pvalue=0.002) or pairwise accuracy is compared (paired t-test pvalue=0.2).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 可以观察到，当比较整体准确率（成对t检验p值=0.002）或成对准确率时，tuplemax损失的表现更差（成对t检验p值=0.2）。
- en: 'In fact, even the original study fails to explain clearly why tuplemax loss
    should do better. Here is the example that the authors make:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，即使原始研究也未能清楚地解释为何tuplemax损失应该表现更好。以下是作者提出的例子：
- en: '![](../Images/e5e9425456b5bdb7161468ace272e815.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5e9425456b5bdb7161468ace272e815.png)'
- en: Explanation of tuplemax loss (image from Wan et al., 2019)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: tuplemax损失的解释（来自于Wan等人，2019年的图片）
- en: The absolute value of loss does not actually mean much. With enough training
    iterations, this example might be classified correctly with one or the other loss.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 损失的绝对值实际上并不意味着太多。通过足够的训练迭代，这个例子可能会用一个或另一个损失函数正确分类。
- en: Anyways, tuplemax loss is not a versatile solution and the choice of loss function
    should be carefully leveraged for each given problem.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，tuplemax损失并非一种通用解决方案，损失函数的选择应该针对每个特定的问题进行谨慎利用。
- en: '**Conclusion**'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: We reached 92% accuracy and 97% pairwise accuracy in spoken language recognition
    of short audio clips from the Mozilla Common Voice (MCV) dataset. German, English,
    Spanish, French, and Russian languages were considered.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Mozilla Common Voice（MCV）数据集的短音频剪辑中实现了92%的准确率和97%的成对准确率，涉及德语、英语、西班牙语、法语和俄语。
- en: In a preliminary study comparing mel spectrogram, MFCC, RASTA-PLP, and GFCC
    embeddings we found out that mel spectrograms with the first 13 filterbank coefficients
    resulted in the highest recognition accuracy.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步研究中，比较了mel频谱图、MFCC、RASTA-PLP和GFCC嵌入，我们发现带有前13个滤波器组分的mel频谱图具有最高的识别准确率。
- en: 'We next compared the generalization performance of 5 neural network models:
    CNN, CRNN (Bartz et al. 2017), CRNN (Alashban et al. 2022), AttNN (De Andrade
    et al. 2018), CRNN*, and TDNN (Snyder et al. 2018). Among all the models, AttNN
    showed the best performance, which highlights the importance of LSTM and attention
    blocks for spoken language recognition.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较了5个神经网络模型的泛化性能：CNN、CRNN（Bartz等人，2017）、CRNN（Alashban等人，2022）、AttNN（De
    Andrade等人，2018）、CRNN*和TDNN（Snyder等人，2018）。在所有模型中，AttNN展示了最佳性能，突显了LSTM和注意力模块在语音语言识别中的重要性。
- en: Finally, we computed the pairwise accuracy and studied the effect of tuplemax
    loss. It turns out, that tuplemax loss degrades both accuracy and pairwise accuracy
    compared to softmax.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算了成对准确率并研究了tuplemax损失的影响。结果表明，与softmax相比，tuplemax损失同时降低了准确率和成对准确率。
- en: In conclusion, our results constitute a new benchmark for spoken language recognition
    on the Mozilla Common Voice dataset. Better results could be achieved in future
    studies by combining different embeddings and extensively investigating promising
    neural network architectures, e.g. transformers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的结果为Mozilla Common Voice数据集上的语音语言识别建立了新的基准。通过结合不同的嵌入和广泛探索有前景的神经网络架构，例如变压器，未来研究可以取得更好的结果。
- en: '**In Part III we will discuss which audio transformations might help to improve
    model performance.**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**在第三部分中，我们将讨论哪些音频转换可能有助于提高模型性能。**'
- en: '**References**'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'Alashban, Adal A., et al. “Spoken language identification system using convolutional
    recurrent neural network.” *Applied Sciences* 12.18 (2022): 9181.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alashban, Adal A., 等人。"使用卷积递归神经网络进行语音识别系统。" *应用科学* 12.18 (2022): 9181。'
- en: 'Bartz, Christian, et al. “Language identification using deep convolutional
    recurrent neural networks.” *Neural Information Processing: 24th International
    Conference, ICONIP 2017, Guangzhou, China, November 14–18, 2017, Proceedings,
    Part VI 24*. Springer International Publishing, 2017.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: De Andrade, Douglas Coimbra, et al. “A neural attention model for speech command
    recognition.” *arXiv preprint arXiv:1808.08929* (2018).
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snyder, David, et al. “Spoken language recognition using x-vectors.” *Odyssey*.
    Vol. 2018\. 2018.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan, Li, et al. “Tuplemax loss for language identification.” *ICASSP 2019–2019
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.
    IEEE, 2019.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
