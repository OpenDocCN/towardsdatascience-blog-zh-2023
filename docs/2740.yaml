- en: Depth-Aware Object Insertion in Videos Using Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python 进行深度感知的对象插入视频
- en: 原文：[https://towardsdatascience.com/depth-aware-object-insertion-in-videos-using-python-60eeee7d17b5?source=collection_archive---------6-----------------------#2023-08-29](https://towardsdatascience.com/depth-aware-object-insertion-in-videos-using-python-60eeee7d17b5?source=collection_archive---------6-----------------------#2023-08-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/depth-aware-object-insertion-in-videos-using-python-60eeee7d17b5?source=collection_archive---------6-----------------------#2023-08-29](https://towardsdatascience.com/depth-aware-object-insertion-in-videos-using-python-60eeee7d17b5?source=collection_archive---------6-----------------------#2023-08-29)
- en: Instructions for placing 3D models in videos with a depth-aware method using
    Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 进行深度感知的 3D 模型插入视频的指南
- en: '[](https://medium.com/@berkanzorlubas?source=post_page-----60eeee7d17b5--------------------------------)[![Berkan
    Zorlubas](../Images/6d13c115064dfa1bf3918ef009a30797.png)](https://medium.com/@berkanzorlubas?source=post_page-----60eeee7d17b5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----60eeee7d17b5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----60eeee7d17b5--------------------------------)
    [Berkan Zorlubas](https://medium.com/@berkanzorlubas?source=post_page-----60eeee7d17b5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@berkanzorlubas?source=post_page-----60eeee7d17b5--------------------------------)[![Berkan
    Zorlubas](../Images/6d13c115064dfa1bf3918ef009a30797.png)](https://medium.com/@berkanzorlubas?source=post_page-----60eeee7d17b5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----60eeee7d17b5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----60eeee7d17b5--------------------------------)
    [Berkan Zorlubas](https://medium.com/@berkanzorlubas?source=post_page-----60eeee7d17b5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d74427941be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdepth-aware-object-insertion-in-videos-using-python-60eeee7d17b5&user=Berkan+Zorlubas&userId=7d74427941be&source=post_page-7d74427941be----60eeee7d17b5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----60eeee7d17b5--------------------------------)
    ·9 min read·Aug 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F60eeee7d17b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdepth-aware-object-insertion-in-videos-using-python-60eeee7d17b5&user=Berkan+Zorlubas&userId=7d74427941be&source=-----60eeee7d17b5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d74427941be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdepth-aware-object-insertion-in-videos-using-python-60eeee7d17b5&user=Berkan+Zorlubas&userId=7d74427941be&source=post_page-7d74427941be----60eeee7d17b5---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----60eeee7d17b5--------------------------------)
    ·9分钟阅读·2023年8月29日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F60eeee7d17b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdepth-aware-object-insertion-in-videos-using-python-60eeee7d17b5&user=Berkan+Zorlubas&userId=7d74427941be&source=-----60eeee7d17b5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60eeee7d17b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdepth-aware-object-insertion-in-videos-using-python-60eeee7d17b5&source=-----60eeee7d17b5---------------------bookmark_footer-----------)![](../Images/da925dea9141c2ebf33ce1f3fdaaa451.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60eeee7d17b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdepth-aware-object-insertion-in-videos-using-python-60eeee7d17b5&source=-----60eeee7d17b5---------------------bookmark_footer-----------)![](../Images/da925dea9141c2ebf33ce1f3fdaaa451.png)'
- en: Image by the author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In the field of computer vision, the consistent estimation of depth and camera
    pose in videos has laid the foundation for more advanced operations, such as depth-aware
    object insertion in videos. Building on my previous [article](https://medium.com/towards-data-science/creating-3d-videos-from-rgb-videos-491a09fa1e79)
    that explored these fundamental techniques, this article shifts focus toward depth-aware
    object insertion. Employing Python-based computational methods, I will outline
    a strategy for adding objects into pre-existing video frames in accordance with
    depth and camera orientation data. This methodology not only elevates the realism
    of edited video content but also has broad applications in video post-production.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉领域，一致的深度和相机姿态估计为更高级的操作奠定了基础，例如在视频中插入深度感知对象。基于我之前探讨这些基本技术的[文章](https://medium.com/towards-data-science/creating-3d-videos-from-rgb-videos-491a09fa1e79)，本文将重点转向深度感知对象的插入。通过基于
    Python 的计算方法，我将概述一种将对象添加到现有视频帧中的策略，以符合深度和相机方向数据。这种方法不仅提升了编辑视频内容的真实感，还有广泛的视频后期制作应用。
- en: 'In summary, the approach involves two main steps: first, estimating consistent
    depth and camera position in a video, and second, overlaying a mesh object onto
    the video frames. To make the object appear stationary in the 3D space of the
    video, it is moved in the opposite direction of any camera movement. This counter-movement
    ensures the object looks like it''s fixed in place throughout the video.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，该方法包括两个主要步骤：首先，估计视频中的一致深度和相机位置；其次，将网格对象叠加到视频帧上。为了使对象在视频的三维空间中看起来是静止的，它会沿着相反的方向移动以抵消相机的移动。这种反向移动确保对象在整个视频中看起来像是固定的。
- en: '*You can check my code on my* [*GitHub*](https://github.com/berkanz/dynamic-video-depth)
    *page, which I will be referring to throughout this article.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以在我的* [*GitHub*](https://github.com/berkanz/dynamic-video-depth) *页面查看我的代码，本文中将引用这些代码。*'
- en: 'Step 1: Generating camera pose matrices and consistent depth estimation of
    the video'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 1：生成相机姿态矩阵和视频的一致深度估计
- en: In my previous [article](https://medium.com/towards-data-science/creating-3d-videos-from-rgb-videos-491a09fa1e79),
    I thoroughly explained how to estimate consistent depth frames of videos and the
    corresponding camera pose matrices throughout videos.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的[文章](https://medium.com/towards-data-science/creating-3d-videos-from-rgb-videos-491a09fa1e79)中，我详细解释了如何估计视频的一致深度帧及其对应的相机姿态矩阵。
- en: For this article, I selected a [video](https://www.videvo.net/video/tracking-shot-following-a-person-walking-down-a-street-in-jakarta/1008167/)
    featuring a man walking on a street, chosen specifically for its pronounced camera
    movement along an axis. This will allow for a clear evaluation of whether inserted
    objects maintain their fixed positions within the 3D space of the video.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这篇文章，我选择了一段[视频](https://www.videvo.net/video/tracking-shot-following-a-person-walking-down-a-street-in-jakarta/1008167/)，视频内容是一个人在街上行走，特别选择了这一视频是因为其明显的相机运动轴线。这将有助于清楚地评估插入的对象是否在视频的三维空间中保持固定位置。
- en: I followed all the steps that I explained in my previous article to get the
    depth frames and the estimated camera pose matrices. We will especially need the
    “custom.matrices.txt” file generated by COLMAP.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我按照我在之前的文章中解释的所有步骤获取了深度帧和估计的相机姿态矩阵。我们特别需要由 COLMAP 生成的“custom.matrices.txt”文件。
- en: Original footage and its estimated depth video are given below.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 原始视频及其估计深度视频如下所示。
- en: '![](../Images/481112adecd7d179ca4fce4f311ca3cf.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/481112adecd7d179ca4fce4f311ca3cf.png)'
- en: '**(Left)** Stock footage provided by [Videvo](https://www.videvo.net/video/tracking-shot-following-a-person-walking-down-a-street-in-jakarta/1008167/),
    downloaded from [www.videvo.net](http://www.videvo.net/) | **(Right)** Estimated
    depth video created by the author'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**（左）** 由[Videvo](https://www.videvo.net/video/tracking-shot-following-a-person-walking-down-a-street-in-jakarta/1008167/)提供的库存视频，从[www.videvo.net](http://www.videvo.net/)下载
    | **（右）** 作者创建的估计深度视频'
- en: The point cloud visualization corresponding to the first frame is presented
    below. The white gaps indicate shadow regions that are obscured from the camera’s
    view due to the presence of foreground objects.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对应于第一帧的点云可视化如下。白色间隙表示由于前景对象的存在而被相机视角遮挡的阴影区域。
- en: '![](../Images/3169763ff52c88b76f708140e44fa57d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3169763ff52c88b76f708140e44fa57d.png)'
- en: Generated point cloud of the first frame of the video
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的视频第一帧的点云
- en: 'Step 2: Picking mesh files you want to insert'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2：选择你想插入的网格文件
- en: Now, we select the mesh files to be inserted into the video sequence. Various
    platforms such as [Sketchfab.com](https://sketchfab.com/) and [GrabCAD.com](http://grabcad.com)
    offer a wide range of 3D models to choose from.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们选择要插入到视频序列中的网格文件。各种平台，如[Sketchfab.com](https://sketchfab.com/)和[GrabCAD.com](http://grabcad.com)，提供了丰富的3D模型供选择。
- en: 'For my demo video, I have chosen two 3D models, the links for which are provided
    in the image captions below:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我的演示视频，我选择了两个3D模型，相关链接在下图说明中提供：
- en: '![](../Images/876f043fe1f89f5a2c3955ef54d81e89.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/876f043fe1f89f5a2c3955ef54d81e89.png)'
- en: '**(Left)** [3D model](https://sketchfab.com/3d-models/elephant-in-the-rotunda-26ee59c981964681bf9f4e5eae2a3a26)
    provided by Abby Gancz (CC BY 4.0), downloaded from [www.sketchfab.com](http://sketchfab.com)
    | **(Right)** [3D model](https://sketchfab.com/3d-models/1952-chevrolet-free-raw-scan-de300880665f4e8b86cbd2a9a656265f)
    provided by Renafox (CC BY 4.0), downloaded from [www.sketchfab.com](http://sketchfab.com)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**(左)** [3D模型](https://sketchfab.com/3d-models/elephant-in-the-rotunda-26ee59c981964681bf9f4e5eae2a3a26)由Abby
    Gancz提供（CC BY 4.0），下载自[www.sketchfab.com](http://sketchfab.com) | **(右)** [3D模型](https://sketchfab.com/3d-models/1952-chevrolet-free-raw-scan-de300880665f4e8b86cbd2a9a656265f)由Renafox提供（CC
    BY 4.0），下载自[www.sketchfab.com](http://sketchfab.com)'
- en: I preprocessed the 3D models using [CloudCompare](https://www.danielgm.net/cc/),
    an open-source tool for 3D point cloud manipulation. Specifically, I removed the
    ground portions from the objects to enhance their integration into the video.
    While this step is optional, if you wish to modify certain aspects of your 3D
    model, CloudCompare comes highly recommended.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了[CloudCompare](https://www.danielgm.net/cc/)，这是一个用于3D点云处理的开源工具，对3D模型进行了预处理。具体而言，我去除了对象的地面部分，以增强其在视频中的整合效果。虽然这一步是可选的，但如果你希望修改你的3D模型的某些方面，强烈推荐使用CloudCompare。
- en: After pre-processing the mesh files, save them as .ply or .obj files. (Please
    note that not all 3D model file extensions support colored meshes, such as .stl).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完网格文件后，将其保存为.ply或.obj文件。（请注意，并非所有的3D模型文件扩展名都支持彩色网格，例如.stl）。
- en: 'Step 3: Re-rendering the frames with depth-aware object insertion'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3步：重新渲染带有深度感知物体插入的帧
- en: 'We now arrive at the core component of the project: video processing. In my
    repository, two key scripts are provided — `video_processing_utils.py` and `depth_aware_object_insertion.py`.
    As implied by their names, `video_processing_utils.py` houses all the essential
    functions for object insertion, while `depth_aware_object_insertion.py` serves
    as the primary script that executes these functions to each video frame within
    a loop.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在来到了项目的核心部分：视频处理。在我的仓库中，提供了两个关键脚本——`video_processing_utils.py`和`depth_aware_object_insertion.py`。顾名思义，`video_processing_utils.py`包含了所有用于物体插入的必要功能，而`depth_aware_object_insertion.py`作为主要脚本，在循环中对每一帧视频执行这些功能。
- en: A snipped version of the main section of `depth_aware_object_insertion.py` is
    given below. In a for loop that runs as many as the count of frames in the input
    video, we load batched information of the depth computation pipeline from which
    we get the original RGB frame and its depth estimation. Then we compute the inverse
    of the camera pose matrix. Afterwards, we feed the mesh, depth, and intrinsics
    of the camera into a function named `render_mesh_with_depth()`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是`depth_aware_object_insertion.py`主要部分的一个片段。在一个循环中，该循环运行的次数与输入视频中的帧数相同，我们从深度计算管道中加载批量信息，从中获取原始RGB帧及其深度估计。然后我们计算相机姿态矩阵的逆。接下来，我们将网格、深度和相机的内参输入名为`render_mesh_with_depth()`的函数中。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `render_mesh_with_depth` function takes a 3D mesh, represented by its vertices,
    vertex colors, and triangles, and renders it onto a 2D depth frame. The function
    starts by initializing depth and color buffers to hold the rendered output. It
    then projects the 3D mesh vertices onto the 2D frame using camera intrinsic parameters.
    The function uses scanline rendering to loop through each triangle in the mesh,
    rasterizing it into pixels on the 2D frame. During this process, the function
    computes barycentric coordinates for each pixel to interpolate depth and color
    values. These interpolated values are then used to update the depth and color
    buffers, but only if the pixel's interpolated depth is closer to the camera than
    the existing value in the depth buffer. Finally, the function returns the color
    and depth buffers as the rendered output, with the color buffer converted to a
    uint8 format suitable for image display.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`render_mesh_with_depth`函数接受一个由顶点、顶点颜色和三角形表示的 3D 网格，并将其渲染到 2D 深度帧上。函数首先通过初始化深度和颜色缓冲区来保存渲染输出。然后，使用相机内参将
    3D 网格顶点投影到 2D 帧上。函数使用扫描线渲染来循环遍历网格中的每个三角形，并将其光栅化到 2D 帧上的像素中。在此过程中，函数计算每个像素的重心坐标以插值深度和颜色值。然后，仅当像素的插值深度比深度缓冲区中的现有值更接近相机时，使用这些插值值更新深度和颜色缓冲区。最后，函数将色彩和深度缓冲区作为渲染输出返回，其中色彩缓冲区转换为适合图像显示的
    uint8 格式。'
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Color and depth buffers of the transformed mesh are then fed into `combine_frames()`
    function along with the original RGB image and its estimated depthmap. This function
    is designed to merge two sets of image and depth frames. It uses depth information
    to decide which pixels in the original frame should be replaced by the corresponding
    pixels in the rendered mesh frame. Specifically, for each pixel, the function
    checks if the depth value of the rendered mesh is less than the depth value of
    the original scene. If it is, that pixel is considered to be "closer" to the camera
    in the rendered mesh frame, and the pixel values in both the color and depth frames
    are replaced accordingly. The function returns the combined color and depth frames,
    effectively overlaying the rendered mesh onto the original scene based on depth
    information.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的网格的色彩和深度缓冲区与原始 RGB 图像及其估计的深度图一起输入到`combine_frames()`函数中。该函数旨在合并两组图像和深度帧。它使用深度信息来决定原始帧中哪些像素应该由渲染网格帧中的对应像素替换。具体来说，对于每个像素，函数检查渲染网格的深度值是否小于原始场景的深度值。如果是，认为该像素在渲染网格帧中更“接近”相机，并相应地替换色彩和深度帧中的像素值。函数返回合并后的色彩和深度帧，有效地根据深度信息将渲染网格叠加到原始场景中。
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here is how the `mesh_color_buffer`, `mesh_depth_buffer` and the `combined_frame`
    look like the first object, an elephant. Since the elephant object is not occluded
    by any other elements within the frame, it remains fully visible. In different
    placements, occlusion would occur.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第一个对象——大象的`mesh_color_buffer`、`mesh_depth_buffer`和`combined_frame`的视觉效果。由于大象对象在帧内没有被任何其他元素遮挡，因此完全可见。在不同的放置方式下，会发生遮挡。
- en: '![](../Images/94c208c87f7a0ec403c76776ad5e4bf3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94c208c87f7a0ec403c76776ad5e4bf3.png)'
- en: '**(Left)** Computed color buffer of the elephant mesh | **(Right)** Computed
    depth buffer of the elephant mesh | **(Bottom)** Combined frame'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**(左)** 大象网格的计算色缓冲区 | **(右)** 大象网格的计算深度缓冲区 | **(底部)** 组合帧'
- en: Accordingly, I placed the second mesh, the car, on the curbside of the road.
    I also adjusted its initial orientation such that it looks like it has been parked
    there. The following visuals are the corresponding `mesh_color_buffer`, `mesh_depth_buffer`
    and the `combined_frame` for this mesh.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地，我将第二个网格——汽车，放置在路边的路缘。我还调整了它的初始方向，使其看起来像是被停放在那里。以下是该网格的`mesh_color_buffer`、`mesh_depth_buffer`和`combined_frame`的视觉效果。
- en: '![](../Images/288af5c4cadd4a503c17cffb28bce534.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/288af5c4cadd4a503c17cffb28bce534.png)'
- en: '**(Left)** Computed color buffer of the car mesh | **(Right)** Computed depth
    buffer of the car mesh | **(Bottom)** Combined frame'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**(左)** 小汽车网格的计算色缓冲区 | **(右)** 小汽车网格的计算深度缓冲区 | **(底部)** 组合帧'
- en: The point cloud visualization with both objects inserted is given below. More
    white gaps are introduced due to the new occlusion areas which came up with new
    objects.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是插入了两个对象的点云可视化。由于新对象引入了新的遮挡区域，出现了更多的白色间隙。
- en: '![](../Images/d26a7fcee0a76ce72f8e4e067ad13740.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d26a7fcee0a76ce72f8e4e067ad13740.png)'
- en: Generated point cloud of the first frame with inserted objects
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 插入对象后第一帧的生成点云
- en: After calculating the overlayed images for each one of the video frames, we
    are now ready to render our video.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了每个视频帧的叠加图像后，我们现在准备渲染我们的视频。
- en: '**Step 4: Rendering video from processed frames**'
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**第4步：从处理过的帧渲染视频**'
- en: 'In the last section of `depth_aware_object_insertion.py`, we simply render
    a video from the object-inserted frames using the`render_video_from_frames` function.
    You can also adjust the fps of the output video at this step. The code is given
    below:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在`depth_aware_object_insertion.py`的最后一部分，我们仅需使用`render_video_from_frames`函数从插入了物体的帧渲染视频。你也可以在此步骤调整输出视频的帧率。代码如下：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is my demo video:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的演示视频：
- en: '![](../Images/524e53fda6e91629d3590fdaa749eb62.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/524e53fda6e91629d3590fdaa749eb62.png)'
- en: '**(Left)** Stock footage provided by [Videvo](https://www.videvo.net/video/tracking-shot-following-a-person-walking-down-a-street-in-jakarta/1008167/),
    downloaded from [www.videvo.net](http://www.videvo.net/) | **(Right)** Stock footage
    with two inserted objects'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**（左）** 由 [Videvo](https://www.videvo.net/video/tracking-shot-following-a-person-walking-down-a-street-in-jakarta/1008167/)
    提供的素材，从 [www.videvo.net](http://www.videvo.net/) 下载 | **（右）** 插入了两个物体的素材'
- en: '*A higher-resolution version of this animation is uploaded to* [*YouTube*](https://youtu.be/4euMXXLxLoQ)*.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*此动画的高分辨率版本已上传至* [*YouTube*](https://youtu.be/4euMXXLxLoQ)*。*'
- en: Overall, the object integrity appears to be well-maintained; for instance, the
    car object is convincingly occluded by the streetlight pole in the scene. While
    there is a slight perceptible jitter in the car’s position throughout the video
    —most likely due to imperfections in the camera pose estimation — the world-locking
    mechanism generally performs as expected in the demonstration video.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，物体完整性似乎得到了很好的保持；例如，在场景中，汽车物体被街灯杆有效地遮挡。尽管在整个视频中汽车的位置有轻微的抖动——很可能是由于相机姿态估计的不完美——但世界锁定机制在演示视频中通常表现如预期。
- en: While the concept of object insertion in videos is far from novel, with established
    tools like After Effects offering feature-tracking-based methods, these traditional
    approaches often can be very challenging and costly for those unfamiliar with
    video editing tools. This is where the promise of Python-based algorithms comes
    into play. Leveraging machine learning and basic programming constructs, these
    algorithms have the potential to democratize advanced video editing tasks, making
    them accessible even to individuals with limited experience in the field. Thus,
    as technology continues to evolve, I anticipate that software-based approaches
    will serve as powerful enablers, leveling the playing field and opening up new
    avenues for creative expression in video editing.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然视频中的物体插入概念并不新颖，已有如 After Effects 这样的工具提供基于特征跟踪的方法，但这些传统方法对于不熟悉视频编辑工具的人来说通常非常具有挑战性和成本高昂。这时，基于
    Python 的算法就显得很有前景。利用机器学习和基本的编程构造，这些算法有潜力使高级视频编辑任务变得更加平易近人，即使是对这一领域经验有限的个人也能接触到。因此，随着技术的不断发展，我预期基于软件的方法将作为强有力的推动者，平衡竞争格局，开辟视频编辑中的创意表达新途径。
- en: Have a great day ahead!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你有美好的一天！
