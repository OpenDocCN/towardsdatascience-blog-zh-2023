- en: Depth-Aware Object Insertion in Videos Using Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/depth-aware-object-insertion-in-videos-using-python-60eeee7d17b5?source=collection_archive---------6-----------------------#2023-08-29](https://towardsdatascience.com/depth-aware-object-insertion-in-videos-using-python-60eeee7d17b5?source=collection_archive---------6-----------------------#2023-08-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Instructions for placing 3D models in videos with a depth-aware method using
    Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@berkanzorlubas?source=post_page-----60eeee7d17b5--------------------------------)[![Berkan
    Zorlubas](../Images/6d13c115064dfa1bf3918ef009a30797.png)](https://medium.com/@berkanzorlubas?source=post_page-----60eeee7d17b5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----60eeee7d17b5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----60eeee7d17b5--------------------------------)
    [Berkan Zorlubas](https://medium.com/@berkanzorlubas?source=post_page-----60eeee7d17b5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d74427941be&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdepth-aware-object-insertion-in-videos-using-python-60eeee7d17b5&user=Berkan+Zorlubas&userId=7d74427941be&source=post_page-7d74427941be----60eeee7d17b5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----60eeee7d17b5--------------------------------)
    ·9 min read·Aug 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F60eeee7d17b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdepth-aware-object-insertion-in-videos-using-python-60eeee7d17b5&user=Berkan+Zorlubas&userId=7d74427941be&source=-----60eeee7d17b5---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60eeee7d17b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdepth-aware-object-insertion-in-videos-using-python-60eeee7d17b5&source=-----60eeee7d17b5---------------------bookmark_footer-----------)![](../Images/da925dea9141c2ebf33ce1f3fdaaa451.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: In the field of computer vision, the consistent estimation of depth and camera
    pose in videos has laid the foundation for more advanced operations, such as depth-aware
    object insertion in videos. Building on my previous [article](https://medium.com/towards-data-science/creating-3d-videos-from-rgb-videos-491a09fa1e79)
    that explored these fundamental techniques, this article shifts focus toward depth-aware
    object insertion. Employing Python-based computational methods, I will outline
    a strategy for adding objects into pre-existing video frames in accordance with
    depth and camera orientation data. This methodology not only elevates the realism
    of edited video content but also has broad applications in video post-production.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the approach involves two main steps: first, estimating consistent
    depth and camera position in a video, and second, overlaying a mesh object onto
    the video frames. To make the object appear stationary in the 3D space of the
    video, it is moved in the opposite direction of any camera movement. This counter-movement
    ensures the object looks like it''s fixed in place throughout the video.'
  prefs: []
  type: TYPE_NORMAL
- en: '*You can check my code on my* [*GitHub*](https://github.com/berkanz/dynamic-video-depth)
    *page, which I will be referring to throughout this article.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Generating camera pose matrices and consistent depth estimation of
    the video'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In my previous [article](https://medium.com/towards-data-science/creating-3d-videos-from-rgb-videos-491a09fa1e79),
    I thoroughly explained how to estimate consistent depth frames of videos and the
    corresponding camera pose matrices throughout videos.
  prefs: []
  type: TYPE_NORMAL
- en: For this article, I selected a [video](https://www.videvo.net/video/tracking-shot-following-a-person-walking-down-a-street-in-jakarta/1008167/)
    featuring a man walking on a street, chosen specifically for its pronounced camera
    movement along an axis. This will allow for a clear evaluation of whether inserted
    objects maintain their fixed positions within the 3D space of the video.
  prefs: []
  type: TYPE_NORMAL
- en: I followed all the steps that I explained in my previous article to get the
    depth frames and the estimated camera pose matrices. We will especially need the
    “custom.matrices.txt” file generated by COLMAP.
  prefs: []
  type: TYPE_NORMAL
- en: Original footage and its estimated depth video are given below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/481112adecd7d179ca4fce4f311ca3cf.png)'
  prefs: []
  type: TYPE_IMG
- en: '**(Left)** Stock footage provided by [Videvo](https://www.videvo.net/video/tracking-shot-following-a-person-walking-down-a-street-in-jakarta/1008167/),
    downloaded from [www.videvo.net](http://www.videvo.net/) | **(Right)** Estimated
    depth video created by the author'
  prefs: []
  type: TYPE_NORMAL
- en: The point cloud visualization corresponding to the first frame is presented
    below. The white gaps indicate shadow regions that are obscured from the camera’s
    view due to the presence of foreground objects.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3169763ff52c88b76f708140e44fa57d.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated point cloud of the first frame of the video
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Picking mesh files you want to insert'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we select the mesh files to be inserted into the video sequence. Various
    platforms such as [Sketchfab.com](https://sketchfab.com/) and [GrabCAD.com](http://grabcad.com)
    offer a wide range of 3D models to choose from.
  prefs: []
  type: TYPE_NORMAL
- en: 'For my demo video, I have chosen two 3D models, the links for which are provided
    in the image captions below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/876f043fe1f89f5a2c3955ef54d81e89.png)'
  prefs: []
  type: TYPE_IMG
- en: '**(Left)** [3D model](https://sketchfab.com/3d-models/elephant-in-the-rotunda-26ee59c981964681bf9f4e5eae2a3a26)
    provided by Abby Gancz (CC BY 4.0), downloaded from [www.sketchfab.com](http://sketchfab.com)
    | **(Right)** [3D model](https://sketchfab.com/3d-models/1952-chevrolet-free-raw-scan-de300880665f4e8b86cbd2a9a656265f)
    provided by Renafox (CC BY 4.0), downloaded from [www.sketchfab.com](http://sketchfab.com)'
  prefs: []
  type: TYPE_NORMAL
- en: I preprocessed the 3D models using [CloudCompare](https://www.danielgm.net/cc/),
    an open-source tool for 3D point cloud manipulation. Specifically, I removed the
    ground portions from the objects to enhance their integration into the video.
    While this step is optional, if you wish to modify certain aspects of your 3D
    model, CloudCompare comes highly recommended.
  prefs: []
  type: TYPE_NORMAL
- en: After pre-processing the mesh files, save them as .ply or .obj files. (Please
    note that not all 3D model file extensions support colored meshes, such as .stl).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Re-rendering the frames with depth-aware object insertion'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now arrive at the core component of the project: video processing. In my
    repository, two key scripts are provided — `video_processing_utils.py` and `depth_aware_object_insertion.py`.
    As implied by their names, `video_processing_utils.py` houses all the essential
    functions for object insertion, while `depth_aware_object_insertion.py` serves
    as the primary script that executes these functions to each video frame within
    a loop.'
  prefs: []
  type: TYPE_NORMAL
- en: A snipped version of the main section of `depth_aware_object_insertion.py` is
    given below. In a for loop that runs as many as the count of frames in the input
    video, we load batched information of the depth computation pipeline from which
    we get the original RGB frame and its depth estimation. Then we compute the inverse
    of the camera pose matrix. Afterwards, we feed the mesh, depth, and intrinsics
    of the camera into a function named `render_mesh_with_depth()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `render_mesh_with_depth` function takes a 3D mesh, represented by its vertices,
    vertex colors, and triangles, and renders it onto a 2D depth frame. The function
    starts by initializing depth and color buffers to hold the rendered output. It
    then projects the 3D mesh vertices onto the 2D frame using camera intrinsic parameters.
    The function uses scanline rendering to loop through each triangle in the mesh,
    rasterizing it into pixels on the 2D frame. During this process, the function
    computes barycentric coordinates for each pixel to interpolate depth and color
    values. These interpolated values are then used to update the depth and color
    buffers, but only if the pixel's interpolated depth is closer to the camera than
    the existing value in the depth buffer. Finally, the function returns the color
    and depth buffers as the rendered output, with the color buffer converted to a
    uint8 format suitable for image display.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Color and depth buffers of the transformed mesh are then fed into `combine_frames()`
    function along with the original RGB image and its estimated depthmap. This function
    is designed to merge two sets of image and depth frames. It uses depth information
    to decide which pixels in the original frame should be replaced by the corresponding
    pixels in the rendered mesh frame. Specifically, for each pixel, the function
    checks if the depth value of the rendered mesh is less than the depth value of
    the original scene. If it is, that pixel is considered to be "closer" to the camera
    in the rendered mesh frame, and the pixel values in both the color and depth frames
    are replaced accordingly. The function returns the combined color and depth frames,
    effectively overlaying the rendered mesh onto the original scene based on depth
    information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here is how the `mesh_color_buffer`, `mesh_depth_buffer` and the `combined_frame`
    look like the first object, an elephant. Since the elephant object is not occluded
    by any other elements within the frame, it remains fully visible. In different
    placements, occlusion would occur.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94c208c87f7a0ec403c76776ad5e4bf3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**(Left)** Computed color buffer of the elephant mesh | **(Right)** Computed
    depth buffer of the elephant mesh | **(Bottom)** Combined frame'
  prefs: []
  type: TYPE_NORMAL
- en: Accordingly, I placed the second mesh, the car, on the curbside of the road.
    I also adjusted its initial orientation such that it looks like it has been parked
    there. The following visuals are the corresponding `mesh_color_buffer`, `mesh_depth_buffer`
    and the `combined_frame` for this mesh.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/288af5c4cadd4a503c17cffb28bce534.png)'
  prefs: []
  type: TYPE_IMG
- en: '**(Left)** Computed color buffer of the car mesh | **(Right)** Computed depth
    buffer of the car mesh | **(Bottom)** Combined frame'
  prefs: []
  type: TYPE_NORMAL
- en: The point cloud visualization with both objects inserted is given below. More
    white gaps are introduced due to the new occlusion areas which came up with new
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d26a7fcee0a76ce72f8e4e067ad13740.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated point cloud of the first frame with inserted objects
  prefs: []
  type: TYPE_NORMAL
- en: After calculating the overlayed images for each one of the video frames, we
    are now ready to render our video.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4: Rendering video from processed frames**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last section of `depth_aware_object_insertion.py`, we simply render
    a video from the object-inserted frames using the`render_video_from_frames` function.
    You can also adjust the fps of the output video at this step. The code is given
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is my demo video:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/524e53fda6e91629d3590fdaa749eb62.png)'
  prefs: []
  type: TYPE_IMG
- en: '**(Left)** Stock footage provided by [Videvo](https://www.videvo.net/video/tracking-shot-following-a-person-walking-down-a-street-in-jakarta/1008167/),
    downloaded from [www.videvo.net](http://www.videvo.net/) | **(Right)** Stock footage
    with two inserted objects'
  prefs: []
  type: TYPE_NORMAL
- en: '*A higher-resolution version of this animation is uploaded to* [*YouTube*](https://youtu.be/4euMXXLxLoQ)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the object integrity appears to be well-maintained; for instance, the
    car object is convincingly occluded by the streetlight pole in the scene. While
    there is a slight perceptible jitter in the car’s position throughout the video
    —most likely due to imperfections in the camera pose estimation — the world-locking
    mechanism generally performs as expected in the demonstration video.
  prefs: []
  type: TYPE_NORMAL
- en: While the concept of object insertion in videos is far from novel, with established
    tools like After Effects offering feature-tracking-based methods, these traditional
    approaches often can be very challenging and costly for those unfamiliar with
    video editing tools. This is where the promise of Python-based algorithms comes
    into play. Leveraging machine learning and basic programming constructs, these
    algorithms have the potential to democratize advanced video editing tasks, making
    them accessible even to individuals with limited experience in the field. Thus,
    as technology continues to evolve, I anticipate that software-based approaches
    will serve as powerful enablers, leveling the playing field and opening up new
    avenues for creative expression in video editing.
  prefs: []
  type: TYPE_NORMAL
- en: Have a great day ahead!
  prefs: []
  type: TYPE_NORMAL
