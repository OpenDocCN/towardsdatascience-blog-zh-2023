- en: 'Linear Algebra 4: Matrix Equations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/linear-algebra-4-matrix-equations-914ebb371950?source=collection_archive---------5-----------------------#2023-11-10](https://towardsdatascience.com/linear-algebra-4-matrix-equations-914ebb371950?source=collection_archive---------5-----------------------#2023-11-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e020de936494e3bab2467f8217eced6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Solving matrix equations Ax= b
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@t9nz?source=post_page-----914ebb371950--------------------------------)[![tenzin
    migmar (t9nz)](../Images/d9a3e1fe10afba1f1dc0fc7e4d241d73.png)](https://medium.com/@t9nz?source=post_page-----914ebb371950--------------------------------)[](https://towardsdatascience.com/?source=post_page-----914ebb371950--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----914ebb371950--------------------------------)
    [tenzin migmar (t9nz)](https://medium.com/@t9nz?source=post_page-----914ebb371950--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6ff685c466&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-algebra-4-matrix-equations-914ebb371950&user=tenzin+migmar+%28t9nz%29&userId=d6ff685c466&source=post_page-d6ff685c466----914ebb371950---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----914ebb371950--------------------------------)
    ·7 min read·Nov 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F914ebb371950&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-algebra-4-matrix-equations-914ebb371950&user=tenzin+migmar+%28t9nz%29&userId=d6ff685c466&source=-----914ebb371950---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F914ebb371950&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flinear-algebra-4-matrix-equations-914ebb371950&source=-----914ebb371950---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: Preface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Welcome back to the fourth edition of my ongoing series on the basics of Linear
    Algebra, the foundational math behind machine learning. In my previous [article](https://medium.com/@t9nz/linear-algebra-1-1-15b70e48bab9),
    I introduced vectors, linear combinations, and vector spans. This essay will take
    a look at the matrix equation *A*x = **b** and we’ll see how the very principle
    of solving a system of linear equations is linked to the matrix equation.
  prefs: []
  type: TYPE_NORMAL
- en: This article would best serve readers if read in accompaniment with Linear Algebra
    and Its Applications by David C. Lay, Steven R. Lay, and Judi J. McDonald. Consider
    this series as a companion resource.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to share thoughts, questions, and critique.
  prefs: []
  type: TYPE_NORMAL
- en: The Intuition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We last left off on learning about linear combinations which I promised would
    have important implications. Recall that given vectors v₁, v₂, … vₐ in ℝⁿ and
    scalars (also known as weights) c₁, c₂, … cₐ, the **linear combination** is the
    vector defined by the sum of the scalar multiples, c₁v₁ + c₂v₂ + … + cₐvₐ.¹
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3331d6f445bd06ed295fc84671874d0a.png)'
  prefs: []
  type: TYPE_IMG
- en: We say that a vector **b** is a linear combination of a set of vectors v₁, v₂,
    .. vₐₚ in Rⁿ, if there exists a set of weights c₁, c₂, … cₐ (a solution) such
    that c₁v₁ + c₂v₂ + … + cₐvₐ = **b**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine if **b** is a linear combination of some given vectors v₁, v₂,
    .. vₐ we arranged our vectors into a system of linear equations, then created
    an augmented matrix of our equations and used row reduction operations to reduce
    the matrix to reduced echelon form. If the row reduced echelon form had an inconsistency,
    that is, a row that looked like this: [0, 0, … | **m**] where **m** ≠ 0, that
    meant that our vector **b** is not a linear combination of the vectors because
    no set of weights exist for the equation c₁v₁ + c₂v₂ + … + cₐvₐ = **b** tohold
    true.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/122a31f721cbab27ab07d70b4186cf65.png)'
  prefs: []
  type: TYPE_IMG
- en: If there was no such inconsistency, that meant that we could write the vector
    b as a linear combination of a set of vectors, such as the example above. Do you
    remember how we verified our answer at the end? We’d multiply each vector by its
    respective scalar and then find the vector sum. If the vector sum equalled **b**,
    we knew that we had done our calculations correctly and that **b** was indeed
    a linear combination.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8bb411a027318f9feb0d86c6aa5f8df9.png)'
  prefs: []
  type: TYPE_IMG
- en: This verification process is the matrix equation *A*x = **b** in disguise!
  prefs: []
  type: TYPE_NORMAL
- en: Ax = b
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If *A* is an *m* x *n* matrix, and x is in Rⁿ (you’ll see why it’s important
    x is in Rⁿ next section), then the product *A*x is the linear combination of the
    vectors (columns) in *A*, using the corresponding scalars in x.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e8e39515c441ef8c37be3b91288c48f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that none of this is new material, we’ve already unknowingly computed
    *A*x when verifying our linear combinations in my previous article. The *A*x =
    **b** matrix equation is still fundamental though because it formalizes all of
    this into a compact notation and will resurface later on in new ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we know that if we are given an *m* x *n* matrix *A* and x and we compute
    the matrix product *A*x and it is equal to **b**, then **b** can be written as
    a linear combination of the vectors (columns) in A and the scalars/entries in
    x. So in summary: the equation *A*x = **b** will only have a solution (x) if b
    can be written as a linear combination of the columns of A.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bef0724127bb92a8abb968e30906c440.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix Multiplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I’ve introduced *A*x = **b** as a matrix product, but I haven’t yet explained
    matrix multiplication (which is what *A*x is)!
  prefs: []
  type: TYPE_NORMAL
- en: '**Matrix multiplication** is the operation of multiplying two matrices to produce
    one, their product. We’ve already seen matrix addition where two matrices are
    added to produce their sum. In order for matrix addition to be defined, the two
    matrices that are being added, matrix A and matrix B must be of the same size.
    Similarly, matrix multiplication also has a requirement. To multiply matrix *A*
    and matrix *B* and produce *AB*, the number of columns in matrix *A* must be equal
    to the number of rows in matrix *B*. The size of the product of matrix *A* and
    *B*, which we’ll call matrix *C* will depend on the number of rows in matrix *A*
    and number of columns in matrix *B*. Matrix *C* will have m (# of rows in matrix
    *A*) rows and p (# of columns in matrix *B*) columns.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c21d4d9d3df65d4438c5b2faab8ad4d4.png)'
  prefs: []
  type: TYPE_IMG
- en: So, how does matrix multiplication work? If we were to multiply matrix A and
    B, each of the i-th row, j-th column entries in the matrix product is the **dot
    product** of the i-th row in matrix A and the j-th row in matrix B.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b1b2472ac07d18a01f163d641da0190.png)'
  prefs: []
  type: TYPE_IMG
- en: For now, all you need to know is that the **dot product** is the summation of
    product of corresponding entries between two vectors and that it is only defined
    when the two vectors have the same number of entries. This explanation is far
    from doing the dot product justice, but I’ll save the full geometric intuition
    for later.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a326f62154c7efd4753bce02075e7234.png)'
  prefs: []
  type: TYPE_IMG
- en: For brevity, I’ve computed the matrix product of two 2 x 2 matrices, but the
    same procedure generalizes for matrices of any size as long as the matrices meet
    the criteria for matrix multiplication, otherwise their product will be undefined.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of the Matrix Multiplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If *A*, *B* and *C* are *n* x *n* matrices and **c** and **d** are scalars,
    then the following properties are true.³
  prefs: []
  type: TYPE_NORMAL
- en: '*AB* **≠** BA (not commutative in general)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (AB)C = A(BC) (associative)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A(B+C) = AB + AC and (B+C)A = BA + CA (distributive)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 0A = 0 (multiplicative property of zero)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take care in noting that matrix multiplication is not commutative, this property
    might take a while to stick given we are intuitively used to commutativity with
    real numbers.
  prefs: []
  type: TYPE_NORMAL
- en: These properties are useful for computing matrix products, which will be a recurring
    subject throughout Linear Algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Matrix multiplication is a fundamental mathematical operation that underpins
    the core functionality of neural networks, particularly in their feedforward and
    back propagation phases.
  prefs: []
  type: TYPE_NORMAL
- en: In the feedforward phase of a neural network, data is processed through its
    various layers, and matrix multiplication is at the heart of this operation. Each
    layer in a neural network is composed of neurons, which are represented as weighted
    sums of the inputs, followed by an activation function. These weighted sums are
    calculated using matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: During the back propagation pass, the neural network is learning from its mistakes.
    It adjusts the weights of neurons to minimize the error between predicted and
    actual outputs. Matrix multiplication is again a key component of this process,
    specifically in calculating gradients, which indicate how much each weight should
    be adjusted to minimize the error.
  prefs: []
  type: TYPE_NORMAL
- en: Learning math is an exciting venture purely on its own merit, but learning about
    the applications of Linear Algebra alongside the theory can make the journey up
    a steep learning curve even more inspiring.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The intuition behind linear combinations and the matrix product *A*x = **b**:
    how the matrix product isn’t necessarily a new concept, but one that formalizes
    a procedure we’d already been using!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A*x = **b**: the matrix product has a solution x if **b** is a linear combination
    of a the set of vectors (columns) in *A*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Matrix multiplication: the operation behind *A*x = **b** that is widely used
    in machine learning applications, specific examples including in neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Properties of matrix multiplication: non-commutativity, associativity, distributive
    and the multiplicative property of zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*All images created by the author unless otherwise noted.'
  prefs: []
  type: TYPE_NORMAL
- en: '*I apologize for taking a while to continue where we last left off. I am currently
    in the midst of taking midterm exams (including one for Linear Algebra haha!)'
  prefs: []
  type: TYPE_NORMAL
- en: ¹Definition for linear combinations referenced from Linear Algebra and Its Applications
    6th Edition by David C. Lay, Steven R. Lay, and Judi J. McDonald
  prefs: []
  type: TYPE_NORMAL
- en: ²Definition for matrix product properties referenced from Linear Algebra and
    Its Applications 6th Edition by David C. Lay, Steven R. Lay, and Judi J. McDonald
  prefs: []
  type: TYPE_NORMAL
- en: ³Matrix properties referenced from [src](https://www.khanacademy.org/math/precalculus/x9e81a4f98389efdf:matrices/x9e81a4f98389efdf:properties-of-matrix-multiplication/a/properties-of-matrix-multiplication).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ccb6930f4dbcc5c08c98bc5c0ff6cff1.png)'
  prefs: []
  type: TYPE_IMG
