- en: 'Image Segmentation: An In-Depth Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分割：深入指南
- en: 原文：[https://towardsdatascience.com/image-segmentation-an-in-depth-guide-5e56512eea2e?source=collection_archive---------2-----------------------#2023-10-06](https://towardsdatascience.com/image-segmentation-an-in-depth-guide-5e56512eea2e?source=collection_archive---------2-----------------------#2023-10-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/image-segmentation-an-in-depth-guide-5e56512eea2e?source=collection_archive---------2-----------------------#2023-10-06](https://towardsdatascience.com/image-segmentation-an-in-depth-guide-5e56512eea2e?source=collection_archive---------2-----------------------#2023-10-06)
- en: How can you get a computer to distinguish between different types of objects
    in an image? A step-by-step guide.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何让计算机区分图像中的不同对象？一个逐步指南。
- en: '[](https://medium.com/@ed.izaguirre?source=post_page-----5e56512eea2e--------------------------------)[![Ed
    Izaguirre](../Images/c9eded1f06c47571baa662107428483f.png)](https://medium.com/@ed.izaguirre?source=post_page-----5e56512eea2e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5e56512eea2e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5e56512eea2e--------------------------------)
    [Ed Izaguirre](https://medium.com/@ed.izaguirre?source=post_page-----5e56512eea2e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ed.izaguirre?source=post_page-----5e56512eea2e--------------------------------)[![Ed
    Izaguirre](../Images/c9eded1f06c47571baa662107428483f.png)](https://medium.com/@ed.izaguirre?source=post_page-----5e56512eea2e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5e56512eea2e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5e56512eea2e--------------------------------)
    [Ed Izaguirre](https://medium.com/@ed.izaguirre?source=post_page-----5e56512eea2e--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33a47cfa4187&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-segmentation-an-in-depth-guide-5e56512eea2e&user=Ed+Izaguirre&userId=33a47cfa4187&source=post_page-33a47cfa4187----5e56512eea2e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5e56512eea2e--------------------------------)
    ·21 min read·Oct 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e56512eea2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-segmentation-an-in-depth-guide-5e56512eea2e&user=Ed+Izaguirre&userId=33a47cfa4187&source=-----5e56512eea2e---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33a47cfa4187&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-segmentation-an-in-depth-guide-5e56512eea2e&user=Ed+Izaguirre&userId=33a47cfa4187&source=post_page-33a47cfa4187----5e56512eea2e---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5e56512eea2e--------------------------------)
    ·21 分钟阅读·2023年10月6日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e56512eea2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-segmentation-an-in-depth-guide-5e56512eea2e&user=Ed+Izaguirre&userId=33a47cfa4187&source=-----5e56512eea2e---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e56512eea2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-segmentation-an-in-depth-guide-5e56512eea2e&source=-----5e56512eea2e---------------------bookmark_footer-----------)![](../Images/18ea8243d356c00038d27bbaa2202afd.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e56512eea2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-segmentation-an-in-depth-guide-5e56512eea2e&source=-----5e56512eea2e---------------------bookmark_footer-----------)![](../Images/18ea8243d356c00038d27bbaa2202afd.png)'
- en: An image of a cat in front of a white picket fence. From [DALL·E 3](https://openai.com/dall-e-3).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一张猫咪在白色栅栏前的图像。来源于 [DALL·E 3](https://openai.com/dall-e-3)。
- en: '**Table of Contents**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**目录**'
- en: '[Introduction, Motivation](#4522)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[介绍、动机](#4522)'
- en: '[Extracting Data](#6b76)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[提取数据](#6b76)'
- en: '[Visualizing the Images](#af63)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[可视化图像](#af63)'
- en: '[Building a Simple U-Net Model](#2142)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[构建简单的 U-Net 模型](#2142)'
- en: '[Metrics and the Loss Function](#c836)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[指标和损失函数](#c836)'
- en: '[Building the Complete U-Net Model](#978d)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[构建完整的 U-Net 模型](#978d)'
- en: '[Summary](#a780)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[总结](#a780)'
- en: '[References](#08e4)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[参考文献](#08e4)'
- en: '**Relevant Links**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关链接**'
- en: '[Working Kaggle notebook](https://www.kaggle.com/code/edizaguirre/carvana-image-segmentation)
    (**recommended;** register for the Kaggle competition, make a copy of the notebook,
    and use that free GPU)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[工作中的 Kaggle 笔记本](https://www.kaggle.com/code/edizaguirre/carvana-image-segmentation)
    (**推荐；** 注册 Kaggle 竞赛，复制笔记本，并使用该免费的 GPU)'
- en: '[Colab notebook](https://colab.research.google.com/drive/18sHAbbb4XNZlK0_rzH47DpIXukabiwe5?usp=sharing)
    (need to register for the Kaggle competition to get access to the dataset)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Colab 笔记本](https://colab.research.google.com/drive/18sHAbbb4XNZlK0_rzH47DpIXukabiwe5?usp=sharing)（需要注册
    Kaggle 比赛以获取数据集访问权限）'
- en: '[Carvana Image Masking Challenge Kaggle Competition](https://www.kaggle.com/competitions/carvana-image-masking-challenge/overview)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Carvana 图像遮罩挑战 Kaggle 比赛](https://www.kaggle.com/competitions/carvana-image-masking-challenge/overview)'
- en: Introduction, Motivation
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍，动机
- en: '**Image segmentation** refers to the ability of computers (or more accurately
    models stored on computers) to take an image and assign each pixel in the image
    to a corresponding category. For example, it is possible to run the image of a
    cat in front of a white fence shown above through an image segmenter and have
    it spit out the segmented image below:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像分割**是指计算机（更准确地说是存储在计算机上的模型）将图像中的每个像素分配到相应类别的能力。例如，可以将上面展示的猫的图像通过图像分割器处理，并得到如下的分割图像：'
- en: '![](../Images/9385336d4aa4e3b361a97f77bff85580.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9385336d4aa4e3b361a97f77bff85580.png)'
- en: The cat image, segmented into ‘cat’ pixels and ‘background’ pixels. Modified
    image from [DALL·E 3](https://openai.com/dall-e-3).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 猫的图像，分割成‘猫’像素和‘背景’像素。修改后的图像来自 [DALL·E 3](https://openai.com/dall-e-3)。
- en: In this example, I segmented the image by hand. This is a tedious operation,
    and one that we would love to automate. In this guide I will walk you through
    the process of training an algorithm to conduct image segmentation. Many guides
    on the internet and in textbooks are helpful to a certain extent, but they all
    fail to go into the nitty gritty details of the implementation. Here I will leave
    as few stones unturned as possible, to help you save time when implementing image
    segmentation on your own datasets.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我手动分割了图像。这是一个繁琐的操作，我们希望能自动化。在本指南中，我将带你逐步了解训练算法进行图像分割的过程。互联网上和教科书中有许多指南在一定程度上是有帮助的，但它们都未能深入到实现的细节。在这里，我将尽可能不留任何遗漏，帮助你在自己数据集上实现图像分割时节省时间。
- en: 'First, let us place our task in the broader context of **machine learning**.
    The definition of machine learning is self-evident: we are teaching machines to
    learn how to solve problems that we would love to automate. There are many problems
    humans would like to automate; in this article we focus on a subset of problems
    in **computer vision**. Computer vision seeks to teach a computer *how to see*.
    It is trivial to give a six-year-old child an image of a cat in front of a white
    picket fence and ask them to segment the image into ‘cat’ pixels and ‘background’
    pixels (after you explain what ‘segment’ means to the confused child, of course.)
    And yet for decades computers have struggled mightily with this problem.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将任务放在**机器学习**的更广泛背景中。机器学习的定义不言而喻：我们在教机器如何解决我们希望自动化的问题。人类希望自动化的问题有很多；在本文中，我们关注的是**计算机视觉**中的一个子集。计算机视觉旨在教计算机*如何看见*。给一个六岁的小孩一张猫在白色栅栏前的图像，并让他们将图像分割成‘猫’像素和‘背景’像素（当然，在你向困惑的孩子解释‘分割’的意思之后）是很简单的。然而，几十年来，计算机在这个问题上却一直挣扎。
- en: Why do computers struggle to do what a six-year-old can do? We can empathize
    with the computer by thinking about how one learns to read via braille. Imagine
    you are handed an essay written in braille, and assume you have no knowledge of
    how to read it. How would you proceed? What would you need to decipher the braille
    into English?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么计算机在做一个六岁小孩能做的事情时会挣扎？我们可以通过考虑一个人如何通过盲文学习阅读来设身处地为计算机着想。假设你拿到了一篇用盲文写的文章，并且假设你不知道如何阅读它。你会怎么做？你需要什么来将盲文解码成英文？
- en: '![](../Images/23797d6f1750c9e5b7acd23806fbec45.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23797d6f1750c9e5b7acd23806fbec45.png)'
- en: A small passage written in braille. From [Unsplash](https://unsplash.com/).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一小段用盲文书写的文字。来源于 [Unsplash](https://unsplash.com/)。
- en: What you require is a method of transforming this input into an output that
    is legible to you. In mathematics we call this a **mapping.** We say that we would
    like to learn a function *f(x)* that maps our input *x* that is illegible into
    an output *y* that is legible.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要的是一种将输入转换为对你可读的输出的方法。在数学中，我们称之为**映射**。我们说我们希望学习一个函数 *f(x)*，它将我们无法读取的输入 *x*
    映射到一个可读取的输出 *y*。
- en: '![](../Images/7732d26a356f0a39a3135c97f833928b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7732d26a356f0a39a3135c97f833928b.png)'
- en: With many months of practice and a good tutor, anyone can learn the necessary
    mapping from braille to English. By analogy, a computer processing an image is
    a bit like someone encountering braille for the first time; it appears like a
    bunch of nonsense. The computer needs to learn the necessary mapping *f(x)* to
    transform a bunch of numbers corresponding to pixels into something that it can
    use to segment the image. And unfortunately the computer model doesn’t have thousands
    of years of evolution, biology, and years of experience seeing the world; it is
    essentially ‘born’ when you start up your program. **This is what we hope to teach
    our model in computer vision.**
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过长时间的练习和良好的导师，任何人都可以学会从盲文到英文的必要映射。类比而言，处理图像的计算机有点像初次接触盲文的人；它看起来像一堆无意义的东西。计算机需要学习必要的映射
    *f(x)*，将与像素对应的一堆数字转换为可以用来分割图像的内容。不幸的是，计算机模型没有数千年的进化、生物学以及多年看世界的经验；它在您启动程序时基本上‘出生’。**这就是我们希望在计算机视觉中教给我们模型的内容。**
- en: Why would we want to conduct image segmentation in the first place? One of the
    more obvious use cases is Zoom. Many people favor using virtual backgrounds when
    video conferencing to avoid having their co-workers see their dog doing cartwheels
    in the living room. *Image segmentation is crucial to this task*. Another powerful
    use case is medical imaging. When taking CT scans of patient’s organs, it could
    be helpful to have an algorithm automatically segment the organs in the images
    so that medical professionals can determine things like injury, the presence of
    tumors, etc. [Here is a great example](https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection)
    of a Kaggle competition focused on this task.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为什么要进行图像分割呢？其中一个更明显的用例是Zoom。许多人在视频会议中喜欢使用虚拟背景，以避免同事看到他们的狗在客厅里做花式。*图像分割对于这项任务至关重要*。另一个强大的用例是医学成像。在对患者器官进行CT扫描时，自动分割图像中的器官可能有助于医疗专业人员确定诸如损伤、肿瘤存在等问题。[这里有一个很好的例子](https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection)
    是一个专注于此任务的Kaggle竞赛。
- en: 'There are several flavors of image segmentation, ranging from simple to complex.
    In this article we will be dealing with the simplest kind of image segmentation:
    *binary segmentation*. This means that there will only be two different classes
    of objects e.g. ‘cat’ and ‘background’. No more, no less.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割有几种不同的类型，从简单到复杂不等。在本文中，我们将处理最简单的图像分割类型：*二进制分割*。这意味着只会有两类不同的对象，例如‘猫’和‘背景’。不多也不少。
- en: Note that the code I present here has been slightly rearranged and edited for
    clarity. To run some working code, please see the code links at the top of the
    article. We will be using the Carvana Image Masking Challenge dataset from Kaggle.
    You will need to sign up for this challenge to get access to the dataset, and
    plug in your Kaggle API key into the Colab notebook to get it to work (if you
    don’t want to use the Kaggle notebook). [Please see this discussion post](https://www.kaggle.com/discussions/general/74235)
    for details on how to do this.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我在此呈现的代码已经稍作整理和编辑以增加清晰度。要运行一些可工作的代码，请查看文章顶部的代码链接。我们将使用Kaggle的Carvana图像掩模挑战数据集。您需要注册此挑战以获取数据集，并将您的Kaggle
    API密钥插入Colab笔记本以使其正常工作（如果您不想使用Kaggle笔记本）。[请查看此讨论帖](https://www.kaggle.com/discussions/general/74235)
    获取详细信息。
- en: One more thing; as much as I would like to dive into detail on every idea in
    this code, I will presume you have some working knowledge of convolutional neural
    networks, max pooling layers, densely connected layers, dropout layers, and residual
    connectors. Unfortunately discussing these concepts at length would require a
    new article, and is outside the scope of this one, where we focus on the nuts
    and bolts of implementation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事；尽管我很想详细讨论代码中的每个想法，但我假设您对卷积神经网络、最大池化层、密集连接层、dropout层和残差连接器有一些工作知识。不幸的是，长时间讨论这些概念需要一篇新文章，超出了我们专注于实现细节的范围。
- en: Extracting Data
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取数据
- en: 'The relevant data for this article will be housed in the following folders:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的相关数据将存储在以下文件夹中：
- en: '**train_hq.zip**: Folder containing high quality training images of cars'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**train_hq.zip**: 包含汽车高质量训练图像的文件夹'
- en: '**test_hq.zip**: Folder containing high quality test images of cars'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**test_hq.zip**: 包含汽车高质量测试图像的文件夹'
- en: '**train_masks.zip:** Folder containing masks for the training set'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**train_masks.zip:** 包含训练集掩模的文件夹'
- en: In the context of image segmentation, a **mask** is the segmented image. We
    are trying to get our model to learn how to map an input image to an output segmentation
    mask. It is usually assumed that the true mask (a.k.a. ground truth) is hand-drawn
    by a human expert.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像分割的上下文中，**mask** 是分割后的图像。我们尝试让模型学习如何将输入图像映射到输出分割掩模。通常假设真实的掩模（即地面真相）是由人类专家手工绘制的。
- en: '![](../Images/535db816963d7b6d14f8c1a293d957d3.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/535db816963d7b6d14f8c1a293d957d3.png)'
- en: An example of an image along with it’s corresponding true mask, hand-drawn by
    a human. From the Carvana Image Masking Challenge dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图像及其相应的真实掩模的例子，是真实的人手工绘制的。来自 Carvana 图像掩模挑战数据集。
- en: 'Your first step will be to unzip the folders from your */kaggle/input* source:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你的第一步是从你的 */kaggle/input* 源中解压文件夹：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code gets the file paths for all .zip files in your input, and extracts
    them into your */kaggle/output* directory. Notice that I purposely don’t extract
    the non-high quality photos; the Kaggle repository can only hold 20 GB worth of
    data, and this step is necessary to prevent going over this limit.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码获取了输入中所有 .zip 文件的文件路径，并将它们提取到 */kaggle/output* 目录中。请注意，我故意不提取非高质量照片；Kaggle
    存储库只能容纳 20 GB 的数据，这一步是为了防止超过此限制。
- en: Visualizing the Images
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化图像
- en: The first step in most computer vision problems is to inspect your dataset.
    What exactly are we dealing with? We first need to assemble our images into organized
    datasets for viewing. This guide will be using TensorFlow; conversion to PyTorch
    shouldn’t be too difficult.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数计算机视觉问题中的第一步是检查数据集。我们究竟在处理什么？我们首先需要将图像组装成组织良好的数据集以进行查看。本指南将使用 TensorFlow；转换为
    PyTorch 应该不会太困难。
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s break this down:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来拆解一下：
- en: We first create a sorted list of all file paths to all images in the training
    set, test set, and ground truth masks. Note that these are *not* images yet; we
    are only looking at file paths to images up to this point.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先创建一个所有训练集、测试集和真实掩模图像的文件路径的排序列表。注意，这些*还不是*图像；我们目前只是在查看图像的文件路径。
- en: We then only take the first 1000 file paths to images/masks in the Carvana dataset.
    This is done to reduce the computational load and speed up training. If you have
    access to multiple powerful GPUs (lucky you!) feel free to use all of the images
    for even better performance. We also create a train/validation split of 80/20\.
    The more data (images) you include, the greater this split should lean towards
    the training set. It is not uncommon to see splits of 98/1/1 for training/validation/test
    splits when dealing with very large datasets. The more data in the training set,
    the better your model will be in general.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们只取 Carvana 数据集中前 1000 个图像/掩模的文件路径。这是为了减少计算负载并加快训练速度。如果你有多个强大的 GPU（真幸运！），可以使用所有图像以获得更好的性能。我们还创建了
    80/20 的训练/验证划分。包含的数据（图像）越多，这一划分就应该越倾向于训练集。在处理非常大的数据集时，训练/验证/测试的划分中看到 98/1/1 的情况并不少见。训练集中的数据越多，模型的表现通常会更好。
- en: We then create TensorFlow (TF) **Dataset** objects using the tf.data.Dataset.from_tensor_slices()
    method. Using a Dataset object is a common method of handling training, validation,
    and testing sets, as opposed to keeping them as Numpy arrays. In my experience,
    pre-processing of data is much faster and easier when using Dataset objects. [See
    this link](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices)
    for the documentation.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们使用 tf.data.Dataset.from_tensor_slices() 方法创建 TensorFlow (TF) **Dataset**
    对象。使用 Dataset 对象是一种处理训练、验证和测试集的常见方法，而不是将它们保留为 Numpy 数组。根据我的经验，使用 Dataset 对象时数据预处理要快得多，也更容易。[参见此链接](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices)
    获取文档。
- en: Next we specify the image height, width, and number of channels for our input
    images. The actual high quality images are much larger then 96 pixels by 128 pixels;
    this **downsampling** of our images is done to reduce the computational load (larger
    images require more time for training). If you have the necessary horsepower (GPU)
    I don’t recommend downsampling.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们指定输入图像的高度、宽度和通道数。实际的高质量图像远大于 96 像素乘 128 像素；对图像进行**降采样**是为了减少计算负载（更大的图像需要更多的训练时间）。如果你有必要的计算能力（GPU），我不推荐进行降采样。
- en: We then use the .map() function of our Dataset objects to pre-process our images.
    This converts the file paths into images and does appropriate pre-processing.
    More on this in a moment.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们使用 Dataset 对象的 .map() 函数来预处理图像。这将文件路径转换为图像，并进行适当的预处理。稍后会详细介绍。
- en: Once we have pre-processed our raw training images and our ground-truth masks,
    we need a way to pair images with their masks. To accomplish this we use the .zip()
    function of the Dataset objects. This takes two lists of data, and joins the first
    element of each list and puts them into a tuple. It does the same for the second
    element, third, and so on. The end result is a single list full of tuples of the
    form (image, mask).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们预处理了原始训练图像和真实标注掩膜，我们需要一种方法将图像与其掩膜配对。为此，我们使用 Dataset 对象的 `.zip()` 函数。这将两个数据列表进行连接，将每个列表的第一个元素合并成一个元组。对第二个元素、第三个元素等进行相同操作。最终结果是一个包含
    (image, mask) 形式的元组的单一列表。
- en: We then use the .batch() function to create batches of 32 images from our one-thousand
    images. Batching is an important part of the machine learning pipeline, as it
    allows us to process multiple images at once, instead of one at a time. This speeds
    up training.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们使用 `.batch()` 函数从我们的一千张图像中创建 32 张图像的批次。批量处理是机器学习管道的重要部分，因为它允许我们一次处理多张图像，而不是逐张处理。这加快了训练速度。
- en: Finally we use the .prefetch() function. This is another step that helps to
    speed up training. Loading and preprocessing data can be a bottleneck in training
    pipelines. This can lead to idle GPU or CPU time, which no one wants. While your
    model is doing forward and back propagation, the .prefetch() function can ready
    up the next batch. The AUTOTUNE variable in TensorFlow dynamically computes how
    many batches to prefetch based on your system resources; this is generally recommended.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用 `.prefetch()` 函数。这是另一个帮助加速训练的步骤。加载和预处理数据可能成为训练管道中的瓶颈。这可能导致 GPU 或 CPU
    空闲时间，这是没有人希望看到的。当你的模型进行前向和反向传播时，`.prefetch()` 函数可以准备好下一批数据。TensorFlow 中的 AUTOTUNE
    变量会根据系统资源动态计算预取的批次数；这通常是推荐的做法。
- en: 'Let’s take a closer look at the pre-processing step:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解预处理步骤：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'These functions do the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数完成以下操作：
- en: 'First, we convert the file paths to a **tensor** of data type ‘string’ using
    tf.io.read_file(). A tensor is a special data structure in TensorFlow that is
    similar to multi-dimensional arrays in other math libraries, but with special
    properties and methods that are useful for deep learning. To quote the TensorFlow
    documentation: tf.io.read_file() “does not do any parsing, it just returns the
    contents as they are.” Essentially this means it returns a binary file (1s and
    0s) in the string type containing the information of the image.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们使用 tf.io.read_file() 将文件路径转换为数据类型为 'string' 的 **张量**。张量是 TensorFlow 中一种特殊的数据结构，类似于其他数学库中的多维数组，但具有对深度学习有用的特殊属性和方法。引用
    TensorFlow 文档：tf.io.read_file() “不进行任何解析，它仅返回原始内容。” 本质上，这意味着它返回一个包含图像信息的二进制文件（1
    和 0），以字符串类型表示。
- en: Second, we need to *decode* the binary data. To do this, we need to use the
    appropriate method in TensorFlow. Since the raw image data is in the .jpeg format,
    we use tf.image.decode_jpeg() method. Since the masks are in the GIF format, we
    can use tf.io.decode_gif(), or use the more general tf.image.decode_image() ,
    which can handle any file type. Which you choose is really unimportant. We set
    expand_animations=False because these are not really animations, they are just
    images.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，我们需要 *解码* 二进制数据。为此，我们需要使用 TensorFlow 中的适当方法。由于原始图像数据是 .jpeg 格式，我们使用 tf.image.decode_jpeg()
    方法。由于掩膜是 GIF 格式，我们可以使用 tf.io.decode_gif()，或者使用更通用的 tf.image.decode_image()，它可以处理任何文件类型。你选择哪个并不重要。我们将
    expand_animations 设置为 False，因为这些实际上不是动画，而只是图像。
- en: 'Then we use convert_image_dtype() to convert our image data into float32\.
    **This is only done for the images, not the mask, since the mask was already decoded
    into float32**. There are two common data types used in image processing: float32
    and uint8\. Float32 stands for a floating point number (decimal) that occupies
    32 bits in computer memory. They are signed (meaning the number can be negative)
    and can range in value from 0 to 2³² = 4294967296, although by convention in image
    processing we normalize these values to be between 0 and 1, where 1 is the maximum
    of a color. Uint8 stands for an unsigned (positive) integer that goes between
    0 and 255 and only occupies 8 bits in memory. For example, we can represent the
    color burnt orange as (Red: 204, Green: 85, Blue: 0) for uint8 or (Red: 0.8, Green:
    0.33, Blue: 0) for float32\. Float32 is usually the better choice, since it offers
    more precision and already comes normalized which helps improve training. However,
    uint8 saves memory, and this can be better depending on your memory limitations.
    Using float32 in convert_image_dtype automatically normalizes the values.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '然后我们使用 convert_image_dtype() 将我们的图像数据转换为 float32。**这仅适用于图像，不适用于掩膜，因为掩膜已经解码为
    float32**。图像处理常用的两种数据类型是 float32 和 uint8。Float32 代表一个浮点数（小数），在计算机内存中占用 32 位。它们是有符号的（意味着数字可以是负数），值的范围从
    0 到 2³² = 4294967296，尽管在图像处理的惯例中，我们将这些值归一化到 0 到 1 之间，其中 1 是颜色的最大值。Uint8 代表一个无符号（正数）整数，范围从
    0 到 255，只占用 8 位内存。例如，我们可以将颜色 burnt orange 表示为 uint8 格式的 (Red: 204, Green: 85,
    Blue: 0) 或 float32 格式的 (Red: 0.8, Green: 0.33, Blue: 0)。Float32 通常是更好的选择，因为它提供了更多的精度，并且已经归一化，这有助于提高训练效果。然而，uint8
    节省内存，根据你的内存限制，这可能是更好的选择。在 convert_image_dtype 中使用 float32 会自动归一化这些值。'
- en: In binary segmentation, we expect our masks to have shape (batch, hieght, width,
    channels), with channels = 1\. In other words, we want one class (car) being represented
    by the number 1, and the other class (background) being represented by the number
    0\. There is no reason for the number of channels to be 3, as in for RGB images.
    Unfortunately, after decoding it comes with three channels, with the class number
    repeated three times. To fix this, we use tf.math.reduce_max(mask, axis=-1, keepdims=True)
    to take the maximum of the values in the three channels and get rid of the rest.
    So a channel value of (1,1,1) gets reduced to just (1) and a channel value of
    (0,0,0) gets reduced to (0).
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二分类分割中，我们期望我们的掩膜具有形状 (batch, height, width, channels)，其中 channels = 1。换句话说，我们希望一个类别（汽车）由数字
    1 表示，另一个类别（背景）由数字 0 表示。没有理由将通道数设置为 3，就像 RGB 图像一样。不幸的是，解码后，它带有三个通道，其中类别编号重复三次。为了解决这个问题，我们使用
    tf.math.reduce_max(mask, axis=-1, keepdims=True) 来取三个通道中的最大值，并去掉其他值。因此，通道值 (1,1,1)
    会被减少为 (1)，通道值 (0,0,0) 会被减少为 (0)。
- en: Finally we resize the images/masks to our desired dimensions (small). Note that
    the images I showed earlier of the car with the ground truth mask look blurry;
    this downscaling was done on purpose to reduce the computational load and allow
    training to occur relatively quickly. Using method=‘nearest’ as a default is a
    good idea; otherwise the function will always return a float32, which is bad if
    you want it to be in uint8.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将图像/掩膜调整为我们所需的尺寸（较小）。请注意，我之前展示的汽车图像与真实掩膜看起来模糊；这次缩小是故意进行的，以减少计算负荷并使训练相对较快。默认使用
    method=‘nearest’ 是个好主意；否则函数将始终返回 float32，这对于你希望它是 uint8 格式的情况是不好的。
- en: '![](../Images/d9ea340e152cd50a2ed62d40fad7d400.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9ea340e152cd50a2ed62d40fad7d400.png)'
- en: The color burnt orange can be represented in float32 or uint8 format. Image
    by author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色 burnt orange 可以用 float32 或 uint8 格式表示。图片由作者提供。
- en: 'Once we have our datasets organized, we can now view our images:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们组织好数据集，我们现在可以查看我们的图像：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/4de9150e514f5730a3daf8f5bff40fd2.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4de9150e514f5730a3daf8f5bff40fd2.png)'
- en: Images of our cars paired with the accompanying masks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的汽车图像与附带的掩膜配对显示。
- en: 'Here we are using the .take() method to view the first batch of data in our
    batched_val_dataset. Since we are doing binary segmentation, we want our mask
    to only contain two values: 0 and 1\. Plotting the color bars on the mask confirms
    we have the right setup. Note that we added the argument cmap = ‘gray’ to the
    mask imshow() to let plt know we want these images presented in grayscale.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 .take() 方法来查看我们批处理的 batched_val_dataset 中的第一批数据。由于我们正在进行二分类分割，我们希望我们的掩膜只包含两个值：0
    和 1。绘制掩膜上的颜色条可以确认我们设置正确。请注意，我们在掩膜的 imshow() 中添加了参数 cmap = ‘gray’，以告知 plt 我们希望这些图像以灰度显示。
- en: Building a Simple U-Net Model
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个简单的 U-Net 模型
- en: 'In a letter dated February 5, 1675 to his rival Robert Hooke, Isaac Newton
    stated:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在1675年2月5日写给其对手罗伯特·胡克的信中，艾萨克·牛顿表示：
- en: '*“If I have seen further, it is by standing on the shoulders of giants.”*'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“如果我看得更远，那是因为我站在了巨人的肩膀上。”*'
- en: In this same vein, we will stand on the shoulders of previous machine learning
    researchers who have discovered what sorts of architectures work best for the
    task of image segmentation. It is not a bad idea to experiment with architectures
    of your own; however, the researchers who have come before us have gone down many
    dead ends to discover the models that work. These architectures aren’t necessarily
    the end all be all, as research is still ongoing and a better architecture may
    yet be found.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们将站在之前机器学习研究者的肩膀上，他们已经发现了哪些架构最适合图像分割任务。实验自己设计的架构并非坏主意；然而，之前的研究者们走过许多弯路才发现了有效的模型。这些架构并不一定是终极解决方案，因为研究仍在进行中，可能会发现更好的架构。
- en: '![](../Images/d01011a3b09163f8eaf0dd41cc39c0f8.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d01011a3b09163f8eaf0dd41cc39c0f8.png)'
- en: Visualization of the U-Net, described in [1]. Image by author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net的可视化，描述见[1]。图片由作者提供。
- en: 'One of the more well-known architectures is called the **U-Net**, so called
    because the downsampling and the upsampling portions of the network can be visualized
    as a U (see above). In a paper titled *U-Net: Convolutional Networks for Biomedical
    Image Segmentation* by Ronneberger, Fisher, and Brox [1], the authors describe
    how to create a **fully convolutional network** **(FCN)** that works effectively
    for image segmentation. Fully convolutional means there are no densely connected
    layers; all the layers are convolutional.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '一个较为知名的架构称为**U-Net**，因为网络的下采样部分和上采样部分可以被可视化为一个U形（见上文）。在由Ronneberger、Fisher和Brox撰写的名为*U-Net:
    Convolutional Networks for Biomedical Image Segmentation*的论文[1]中，作者描述了如何创建一个**全卷积网络**
    **(FCN)**，该网络在图像分割中表现有效。全卷积意味着没有密集连接层；所有层都是卷积层。'
- en: 'There are a few things to note:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要注意：
- en: The network consists of a series of repeating blocks of two convolutional layers,
    with padding = ‘same’ and stride = 1 so that the outputs of the convolutions are
    not downsized within the block.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络由一系列重复的两个卷积层块组成，使用padding = ‘same’和stride = 1，以确保卷积的输出在块内不会缩小。
- en: Each block is followed by a max pooling layer, which cuts down the width and
    height of the feature map in half.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个块后面跟着一个最大池化层，该层将特征图的宽度和高度减半。
- en: The next block then doubles the number of filters. And the pattern continues.
    This pattern of cutting the feature space down while increasing the number of
    filters should be familiar if you have studied CNNs. This completes what the authors
    call the “contracting path.”
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来的模块将滤波器的数量加倍。这个模式会继续。如果你研究过CNN，这种在减少特征空间的同时增加滤波器数量的模式应该很熟悉。这完成了作者所称的“收缩路径”。
- en: The “bottleneck” layer is at the bottom of the ‘U’. This layer captures highly
    abstracted features (lines, curves, windows, doors, etc.) but at a significantly
    reduced spatial resolution.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “瓶颈”层位于‘U’的底部。该层捕捉高度抽象的特征（如线条、曲线、窗户、门等），但在空间分辨率上大幅降低。
- en: Next begins what they call the “expanding path.” In short, this reverses the
    contractions, with each block consisting again of two convolutional layers. Each
    block is followed by an **upsampling layer**, which in TensorFlow we call the
    Conv2DTranspose layer. This takes a smaller feature map and doubles the height
    and width.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来是他们所谓的“扩展路径”。简而言之，这个过程是反向的，每个模块再次由两个卷积层组成。每个模块后面跟着一个**上采样层**，在TensorFlow中我们称之为Conv2DTranspose层。它将较小的特征图的高度和宽度加倍。
- en: The next block then cuts the number of filters in half. Repeat the process until
    you wind up with the same height and width as the images you started with. Finally,
    finish with a 1x1 conv layer to reduce the number of channels to 1\. We want to
    finish with one channel because this is binary segmentation, so we desire a single
    filter where the pixel values correspond to our two classes. We use a sigmoid
    activation to smush the pixel values between 0 and 1.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来的模块将滤波器的数量减半。重复这个过程，直到得到与起始图像相同的高度和宽度。最后，用一个1x1卷积层来将通道数量减少到1。我们希望最后得到一个通道，因为这是二值分割，所以我们需要一个滤波器，像素值对应于我们的两个类别。我们使用sigmoid激活函数将像素值压缩在0到1之间。
- en: There are also **skip connections** in the U-Net architecture, allowing the
    network to retain fine-grained spatial information even after downsampling and
    then upsampling. Normally there is a lot of information lost in this process.
    By passing the information from a contracting block and into the corresponding
    expanding block, we can preserve this spatial information. There is a nice symmetry
    to the architecture.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: U-Net架构中还有**跳跃连接**，允许网络在下采样和上采样后保留细粒度的空间信息。在这个过程中通常会丢失很多信息。通过将信息从收缩块传递到相应的扩展块，我们可以保留这些空间信息。这个架构有一个漂亮的对称性。
- en: We will begin by doing a simple version of the U-Net. This will be a FCN, but
    with no residual connections and no max pooling layers.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先做一个简单版本的U-Net。这将是一个FCN，但没有残差连接和最大池化层。
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here we have the same basic structure as the U-Net, with a contracting path
    and an expansion path. One interesting thing to note is that rather than use a
    max pooling layer to cut the feature space in half, we use a convolutional layer
    with strides=2\. According to Chollet [2], this cuts the feature space in half
    while preserving more spatial information than a max pooling layers. He states
    that whenever location information is important (as in image segmentation) it
    is a good idea to avoid destructive max pooling layers and stick to using strided
    convolutions instead (**this is curious, because the famous** **U-Net architecture
    does use max pooling**). Also observe that we are doing some data augmentation
    to help our model generalize to unseen examples.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有与U-Net相同的基本结构，包括一个收缩路径和一个扩展路径。一个有趣的观察是，与其使用最大池化层将特征空间切成两半，我们使用一个步幅为2的卷积层。根据Chollet
    [2]，这可以将特征空间切成两半，同时保留比最大池化层更多的空间信息。他指出，只要位置信息重要（如在图像分割中），最好避免破坏性的最大池化层，而改用步幅卷积（**这很有趣，因为著名的**
    **U-Net架构确实使用了最大池化**）。还要注意，我们正在进行一些数据增强，以帮助我们的模型泛化到未见的示例。
- en: 'Some important details: setting the kernel_intializer to ‘he_normal’ setting
    for ReLU activations makes a surprisingly large difference in terms of training
    stability. I initially underestimated the power of kernel initialization. Rather
    than initializing the weights randomly, he_normalization initializes the weights
    to have a mean of 0 and a standard deviation of the square root of (2 / # of input
    units to layer). In the case of CNNs the number of input units refers to the number
    of channels in the feature maps of the previous layer. He_normal initialization
    has been found to lead to faster convergence, mitigate vanishing gradients, and
    improve learning. See reference [3] for more details.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一些重要的细节：将kernel_initializer设置为‘he_normal’以用于ReLU激活会在训练稳定性方面产生意想不到的大差异。我最初低估了权重初始化的力量。与随机初始化权重不同，he_normalization将权重初始化为均值为0，标准差为（2
    / 层的输入单元数）的平方根。在CNN的情况下，输入单元的数量指的是前一层特征图的通道数。已发现he_normal初始化能导致更快的收敛，减轻梯度消失，并改善学习。有关更多细节，请参见参考文献[3]。
- en: Metrics and Loss Function
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指标和损失函数
- en: There are several common metrics and loss functions one can use for binary segmentation.
    Here, we will use the **dice coefficient** as a metric and the corresponding **dice
    loss** for training, as this is what the competition requires.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 二值分割可以使用几种常见的指标和损失函数。在这里，我们将使用**dice系数**作为指标，并使用相应的**dice损失**进行训练，因为这是比赛要求的。
- en: 'Let’s first take a look at the mathematics behind the dice coefficient:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看看dice系数背后的数学原理：
- en: '![](../Images/bd23ec4488d38032c23f8f61c35f95b0.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd23ec4488d38032c23f8f61c35f95b0.png)'
- en: The dice coefficient, in the general form.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一般形式的dice系数。
- en: The dice coefficient is defined as the intersection between two sets (*X* and
    *Y*), divided by the sum of each set, multiplied by 2\. The dice coefficient will
    lie between 0 (if the sets have no intersection) and 1 (if the sets overlap perfectly).
    Now we see why this makes a great metric for image segmentation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Dice系数被定义为两个集合（*X*和*Y*）的交集，除以每个集合的和，再乘以2。Dice系数的值范围在0（如果集合没有交集）到1（如果集合完全重叠）之间。现在我们可以理解为什么这成为图像分割的一个很好的度量标准。
- en: '![](../Images/cb83c98c2e92e3a20843999e79e16a57.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb83c98c2e92e3a20843999e79e16a57.png)'
- en: An example of two masks overlayed over each other. Orange used for clarity.
    Image by author.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 两个遮罩重叠的示例。橙色用于清晰度。图片来源于作者。
- en: 'The above equation is a general definition of the dice coefficient; when when
    you apply it to *vector* quantities (as opposed to sets), we use the more specific
    definition:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程是 dice 系数的通用定义；当应用于 *向量* 量时（与集合不同），我们使用更具体的定义：
- en: '![](../Images/b147a9a8680ca375b9bc738af7aeb79e.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b147a9a8680ca375b9bc738af7aeb79e.png)'
- en: The dice coefficient, in the vector form.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以向量形式表示的 dice 系数。
- en: Here, we are iterating over each element (pixel) in each mask. *x* represents
    the *i*th pixel in the predicted mask and *y* represents the corresponding pixel
    in the ground truth mask. On top we are doing the element-wise product, and on
    bottom we are summing over all elements in each mask independently. *N* represents
    the total number of pixels (which should be the same for both predicted and target
    masks.) Remember that in our masks, the numbers will all be either 0s or 1s, so
    a pixel with a value of 1 in the ground truth mask and a corresponding pixel in
    the predicted mask with a value of 0 will not contribute to the dice score, as
    expected (1 x 0 = 0).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对每个掩膜中的每个元素（像素）进行迭代。*x* 表示预测掩膜中的第 *i* 个像素，而 *y* 表示真实掩膜中的对应像素。顶部是逐元素乘积，底部是分别对每个掩膜中的所有元素求和。*N*
    表示像素的总数（预测掩膜和目标掩膜应相同）。记住，在我们的掩膜中，数字都为 0 或 1，因此真实掩膜中值为 1 的像素和预测掩膜中对应值为 0 的像素不会对
    dice 分数产生贡献，这也是预期的（1 x 0 = 0）。
- en: The **dice loss** will be simply defined as 1 — Dice Score. Since the dice score
    is between 0 and 1, the dice loss will also be between 0 and 1\. In fact the sum
    of the dice score and the dice loss must equal 1\. They are inversely related.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**dice 损失** 将简单地定义为 1 — Dice Score。由于 dice 分数在 0 和 1 之间，dice 损失也将在 0 和 1 之间。实际上，dice
    分数和 dice 损失的和必须等于 1。它们是反向相关的。'
- en: 'Let’s take a look at how this is implemented in code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在代码中实现这一点：
- en: '[PRE5]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here we are flattening two 4-D masks (batch, height, width, channels=1) into
    1-D vectors, and computing the dice scores for all images in the batch. Note that
    we adding a smoothing value to both the numerator and denominator to prevent having
    a 0/0 issue if the two masks do not overlap.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将两个 4-D 掩膜（批量，高度，宽度，通道=1）展平为 1-D 向量，并计算批次中所有图像的 dice 分数。请注意，我们向分子和分母都添加了平滑值，以防两个掩膜不重叠时出现
    0/0 的问题。
- en: Finally, we begin training. We are doing early stopping to prevent overfitting
    and saving the best model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们开始训练。我们进行早停以防止过拟合，并保存最佳模型。
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can determine the results of our training with the following code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码来确定我们训练的结果：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After 10 epochs, we arrive at a top validation dice score of 0.8788\. Not terrible,
    but not great. On a P100 GPU this took me about 20 minutes. Here is a sample mask
    for our review:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 经过 10 个周期后，我们达到了最高验证 dice 分数 0.8788。还不错，但不是很好。在 P100 GPU 上，这大约花了我 20 分钟。这里是我们审查的样本掩膜：
- en: '![](../Images/7cbd52526f40648c35425e6eabfb9170.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cbd52526f40648c35425e6eabfb9170.png)'
- en: Comparison of input image, true mask, and predicted mask. By the author.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像、真实掩膜和预测掩膜的比较。作者提供。
- en: 'Highlighting a few interesting points:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 突出几个有趣的点：
- en: Note that create_mask is the function that pushes pixel values to either 0 or
    1\. A pixel value of < 0.5 will will be cut to 0 and we will assign that pixel
    to the “background” category. A value ≥ 0.5 will be increased to 1 and we will
    call assign that pixel to the “car” category.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，`create_mask` 是将像素值推送到 0 或 1 的函数。像素值小于 0.5 的将被裁剪为 0，我们会将该像素分配到“背景”类别。值 ≥
    0.5 的将增加到 1，我们会将该像素分配到“汽车”类别。
- en: 'Why did the masks come out yellow and purple, instead of black and white? We
    used: tf.keras.preprocessing.image.array_to_img() to converts the output of the
    mask from a tensor to a PIL Image. We then passed the image to plt.imshow(). [From
    the documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html)
    we see that that the default colormap for single channel images is “viridis” (3-channel
    RGB images get output as is.) The viridis colormap transforms low values to a
    deep purple and a high values to yellow. This colormap can apparently help people
    with colorblindness get an accurate view of the color in an image. We could’ve
    fixed this by adding cmap=“grayscale” as an argument, but this would’ve messed
    up our input image. See more here [at this link](https://en.wikipedia.org/wiki/Color_blindness).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么掩码的颜色是黄色和紫色，而不是黑色和白色？我们使用了：tf.keras.preprocessing.image.array_to_img() 将掩码的输出从张量转换为
    PIL 图像。然后我们将图像传递给 plt.imshow()。[来自文档](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html)
    我们看到单通道图像的默认颜色图是“viridis”（3 通道 RGB 图像保持原样）。viridis 颜色图将低值转化为深紫色，高值转化为黄色。这个颜色图显然可以帮助色盲人士准确查看图像中的颜色。我们本可以通过添加
    cmap=“grayscale” 作为参数来解决这个问题，但这会搞砸我们的输入图像。更多信息见 [此链接](https://en.wikipedia.org/wiki/Color_blindness)。
- en: '![](../Images/82384d6f3ece15994a1d089ad40e0d75.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82384d6f3ece15994a1d089ad40e0d75.png)'
- en: The viridis colormap, from low values (purple) to high values (yellow). By the
    author.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: viridis 颜色图，从低值（紫色）到高值（黄色）。作者提供。
- en: Building the Complete U-Net
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建完整的 U-Net
- en: Now we turn to using the full U-Net architecture, with residual connections,
    max pooling layers, and including dropout layers for regularization. Note the
    contracting path, bottleneck layer, and expanding path. The dropout layers can
    be added in the contracting path, at the end of each block.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向使用完整的 U-Net 架构，包含残差连接、最大池化层，并包括用于正则化的丢弃层。注意收缩路径、瓶颈层和扩展路径。丢弃层可以在收缩路径中添加，在每个块的末尾。
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We then compile the U-Net. I am using 64 filters for the first contracting block.
    This is a hyperparameter that you would want to tune for optimal results.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们编译 U-Net。我在第一个收缩块中使用了 64 个滤波器。这是一个超参数，你需要调整以获得最佳结果。
- en: '[PRE9]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After 16 epochs, I get a validation dice score of 0.9416, much better then
    with the simple U-Net. This shouldn’t be too surprising; looking at the parameter
    count we have an order of magnitude increase from the simple U-Net to the complete
    U-Net. On a P100 GPU this took me about 32 minutes. We then take a peek at the
    predictions:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 经过 16 个训练周期，我得到了 0.9416 的验证骰子分数，比简单的 U-Net 要好得多。这不应该太令人惊讶；查看参数数量，我们从简单的 U-Net
    到完整的 U-Net 增加了一个数量级。在 P100 GPU 上，这大约花费了 32 分钟。然后我们查看一下预测结果：
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/49623a94ebb3eda2868cbb2909a1dd30.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49623a94ebb3eda2868cbb2909a1dd30.png)'
- en: Predicted mask for the complete U-Net. Much better! By the author.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 完整 U-Net 的预测掩码。要好得多！作者提供。
- en: These predictions are much better. One thing to note from looking at multiple
    predictions is that the antenna sticking out of the cars is tough for the network.
    Given that the images are very pixelated, I can’t blame the network for missing
    this.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预测结果要好得多。从多个预测中可以看到的一点是，车上的天线对网络来说很难处理。由于图像非常像素化，我不能责怪网络未能检测到这一点。
- en: 'To improve performance, one would look at tweaking hyperparameters including:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能，需要调整包括以下在内的超参数：
- en: Number of downsampling and upsampling blocks
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下采样和上采样块的数量
- en: Number of filters
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器数量
- en: Image resolution
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分辨率
- en: Size of training set
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集的大小
- en: Loss function (perhaps combining dice loss with binary cross-entropy loss)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数（也许将骰子损失与二进制交叉熵损失结合起来）
- en: 'Adjusting optimizer parameters. Training stability seems to be an issue for
    both models. [From the documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)
    for the Adam optimizer: “The default value of 1e-7 for epsilon might not be a
    good default in general.” Increasing epsilon by an order of magnitude or more
    may help with training stability.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整优化器参数。训练稳定性似乎是两个模型的问题。[来自文档](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)：“epsilon
    的默认值 1e-7 可能并不是一个好的默认值。” 增加 epsilon 一个数量级或更多可能有助于提高训练稳定性。
- en: We can already see the road to an excellent score on the Carvana challenge.
    Too bad it’s already over!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以看到在 Carvana 挑战中获得优秀分数的道路。真可惜比赛已经结束了！
- en: Summary
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'This article was a deep dive on the topic of image segmentation, specifically
    **binary segmentation**. If you take anything away, remember the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章深入探讨了图像分割的主题，特别是**二元分割**。如果你记住任何东西，请记住以下内容：
- en: The goal of image segmentation is to find a mapping from input pixel values
    in an image to output numbers that your model can use to assign classes to each
    pixel.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分割的目标是找到从图像中的输入像素值到模型可以用来为每个像素分配类别的输出数字的映射。
- en: One of the first steps is to organize your images into TensorFlow Dataset objects
    and take a look at your images and corresponding masks.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一步是将你的图像组织成TensorFlow数据集对象，并查看你的图像和相应的掩码。
- en: 'There is no need to re-invent the wheel when it comes to model architecture:
    we know from experience that a U-Net works well.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于模型架构，不需要重新发明轮子：我们知道U-Net效果良好。
- en: The dice score is a common metric that is used for monitoring the success of
    your model’s predictions. We can also get our loss function from this.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dice得分是一个常用的指标，用于监控模型预测的成功。我们也可以从中获得损失函数。
- en: Future work could go into converting the max pooling layers in the canonical
    U-Net architecture into strided convolutional layers.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的工作可能会将经典U-Net架构中的最大池化层转换为步幅卷积层。
- en: Best of luck on your image segmentation problems!
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你的图像分割问题好运！
- en: References
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] O. Ronneberger, P. Fischer, and T. Brox, [U-Net: Convolutional Networks
    for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) (2015), MICCAI
    2015 International Conference'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] O. Ronneberger, P. Fischer, 和 T. Brox, [U-Net：用于生物医学图像分割的卷积网络](https://arxiv.org/abs/1505.04597)
    (2015)，MICCAI 2015国际会议'
- en: '[2] F. Chollet, Deep Learning with Python (2021), Manning Publications Co.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] F. Chollet, 《用Python进行深度学习》（2021），Manning Publications Co.'
- en: '[3] K. He, X. Zhang, S. Ren, J. Sun, [Delving Deep into Rectifiers: Surpassing
    Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)
    (2015), International Conference on Computer Vision (ICCV)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] K. He, X. Zhang, S. Ren, J. Sun, [深入探讨整流器：在ImageNet分类中超越人类水平的表现](https://arxiv.org/abs/1502.01852)
    (2015)，国际计算机视觉大会（ICCV）'
