- en: 'Image Segmentation: An In-Depth Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/image-segmentation-an-in-depth-guide-5e56512eea2e?source=collection_archive---------2-----------------------#2023-10-06](https://towardsdatascience.com/image-segmentation-an-in-depth-guide-5e56512eea2e?source=collection_archive---------2-----------------------#2023-10-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How can you get a computer to distinguish between different types of objects
    in an image? A step-by-step guide.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ed.izaguirre?source=post_page-----5e56512eea2e--------------------------------)[![Ed
    Izaguirre](../Images/c9eded1f06c47571baa662107428483f.png)](https://medium.com/@ed.izaguirre?source=post_page-----5e56512eea2e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5e56512eea2e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5e56512eea2e--------------------------------)
    [Ed Izaguirre](https://medium.com/@ed.izaguirre?source=post_page-----5e56512eea2e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33a47cfa4187&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-segmentation-an-in-depth-guide-5e56512eea2e&user=Ed+Izaguirre&userId=33a47cfa4187&source=post_page-33a47cfa4187----5e56512eea2e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5e56512eea2e--------------------------------)
    ·21 min read·Oct 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e56512eea2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-segmentation-an-in-depth-guide-5e56512eea2e&user=Ed+Izaguirre&userId=33a47cfa4187&source=-----5e56512eea2e---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e56512eea2e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimage-segmentation-an-in-depth-guide-5e56512eea2e&source=-----5e56512eea2e---------------------bookmark_footer-----------)![](../Images/18ea8243d356c00038d27bbaa2202afd.png)'
  prefs: []
  type: TYPE_NORMAL
- en: An image of a cat in front of a white picket fence. From [DALL·E 3](https://openai.com/dall-e-3).
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of Contents**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction, Motivation](#4522)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Extracting Data](#6b76)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Visualizing the Images](#af63)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Building a Simple U-Net Model](#2142)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Metrics and the Loss Function](#c836)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Building the Complete U-Net Model](#978d)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Summary](#a780)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[References](#08e4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Relevant Links**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Working Kaggle notebook](https://www.kaggle.com/code/edizaguirre/carvana-image-segmentation)
    (**recommended;** register for the Kaggle competition, make a copy of the notebook,
    and use that free GPU)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Colab notebook](https://colab.research.google.com/drive/18sHAbbb4XNZlK0_rzH47DpIXukabiwe5?usp=sharing)
    (need to register for the Kaggle competition to get access to the dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Carvana Image Masking Challenge Kaggle Competition](https://www.kaggle.com/competitions/carvana-image-masking-challenge/overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction, Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Image segmentation** refers to the ability of computers (or more accurately
    models stored on computers) to take an image and assign each pixel in the image
    to a corresponding category. For example, it is possible to run the image of a
    cat in front of a white fence shown above through an image segmenter and have
    it spit out the segmented image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9385336d4aa4e3b361a97f77bff85580.png)'
  prefs: []
  type: TYPE_IMG
- en: The cat image, segmented into ‘cat’ pixels and ‘background’ pixels. Modified
    image from [DALL·E 3](https://openai.com/dall-e-3).
  prefs: []
  type: TYPE_NORMAL
- en: In this example, I segmented the image by hand. This is a tedious operation,
    and one that we would love to automate. In this guide I will walk you through
    the process of training an algorithm to conduct image segmentation. Many guides
    on the internet and in textbooks are helpful to a certain extent, but they all
    fail to go into the nitty gritty details of the implementation. Here I will leave
    as few stones unturned as possible, to help you save time when implementing image
    segmentation on your own datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us place our task in the broader context of **machine learning**.
    The definition of machine learning is self-evident: we are teaching machines to
    learn how to solve problems that we would love to automate. There are many problems
    humans would like to automate; in this article we focus on a subset of problems
    in **computer vision**. Computer vision seeks to teach a computer *how to see*.
    It is trivial to give a six-year-old child an image of a cat in front of a white
    picket fence and ask them to segment the image into ‘cat’ pixels and ‘background’
    pixels (after you explain what ‘segment’ means to the confused child, of course.)
    And yet for decades computers have struggled mightily with this problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Why do computers struggle to do what a six-year-old can do? We can empathize
    with the computer by thinking about how one learns to read via braille. Imagine
    you are handed an essay written in braille, and assume you have no knowledge of
    how to read it. How would you proceed? What would you need to decipher the braille
    into English?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23797d6f1750c9e5b7acd23806fbec45.png)'
  prefs: []
  type: TYPE_IMG
- en: A small passage written in braille. From [Unsplash](https://unsplash.com/).
  prefs: []
  type: TYPE_NORMAL
- en: What you require is a method of transforming this input into an output that
    is legible to you. In mathematics we call this a **mapping.** We say that we would
    like to learn a function *f(x)* that maps our input *x* that is illegible into
    an output *y* that is legible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7732d26a356f0a39a3135c97f833928b.png)'
  prefs: []
  type: TYPE_IMG
- en: With many months of practice and a good tutor, anyone can learn the necessary
    mapping from braille to English. By analogy, a computer processing an image is
    a bit like someone encountering braille for the first time; it appears like a
    bunch of nonsense. The computer needs to learn the necessary mapping *f(x)* to
    transform a bunch of numbers corresponding to pixels into something that it can
    use to segment the image. And unfortunately the computer model doesn’t have thousands
    of years of evolution, biology, and years of experience seeing the world; it is
    essentially ‘born’ when you start up your program. **This is what we hope to teach
    our model in computer vision.**
  prefs: []
  type: TYPE_NORMAL
- en: Why would we want to conduct image segmentation in the first place? One of the
    more obvious use cases is Zoom. Many people favor using virtual backgrounds when
    video conferencing to avoid having their co-workers see their dog doing cartwheels
    in the living room. *Image segmentation is crucial to this task*. Another powerful
    use case is medical imaging. When taking CT scans of patient’s organs, it could
    be helpful to have an algorithm automatically segment the organs in the images
    so that medical professionals can determine things like injury, the presence of
    tumors, etc. [Here is a great example](https://www.kaggle.com/competitions/rsna-2023-abdominal-trauma-detection)
    of a Kaggle competition focused on this task.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several flavors of image segmentation, ranging from simple to complex.
    In this article we will be dealing with the simplest kind of image segmentation:
    *binary segmentation*. This means that there will only be two different classes
    of objects e.g. ‘cat’ and ‘background’. No more, no less.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the code I present here has been slightly rearranged and edited for
    clarity. To run some working code, please see the code links at the top of the
    article. We will be using the Carvana Image Masking Challenge dataset from Kaggle.
    You will need to sign up for this challenge to get access to the dataset, and
    plug in your Kaggle API key into the Colab notebook to get it to work (if you
    don’t want to use the Kaggle notebook). [Please see this discussion post](https://www.kaggle.com/discussions/general/74235)
    for details on how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: One more thing; as much as I would like to dive into detail on every idea in
    this code, I will presume you have some working knowledge of convolutional neural
    networks, max pooling layers, densely connected layers, dropout layers, and residual
    connectors. Unfortunately discussing these concepts at length would require a
    new article, and is outside the scope of this one, where we focus on the nuts
    and bolts of implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The relevant data for this article will be housed in the following folders:'
  prefs: []
  type: TYPE_NORMAL
- en: '**train_hq.zip**: Folder containing high quality training images of cars'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**test_hq.zip**: Folder containing high quality test images of cars'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**train_masks.zip:** Folder containing masks for the training set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of image segmentation, a **mask** is the segmented image. We
    are trying to get our model to learn how to map an input image to an output segmentation
    mask. It is usually assumed that the true mask (a.k.a. ground truth) is hand-drawn
    by a human expert.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/535db816963d7b6d14f8c1a293d957d3.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of an image along with it’s corresponding true mask, hand-drawn by
    a human. From the Carvana Image Masking Challenge dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your first step will be to unzip the folders from your */kaggle/input* source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This code gets the file paths for all .zip files in your input, and extracts
    them into your */kaggle/output* directory. Notice that I purposely don’t extract
    the non-high quality photos; the Kaggle repository can only hold 20 GB worth of
    data, and this step is necessary to prevent going over this limit.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in most computer vision problems is to inspect your dataset.
    What exactly are we dealing with? We first need to assemble our images into organized
    datasets for viewing. This guide will be using TensorFlow; conversion to PyTorch
    shouldn’t be too difficult.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break this down:'
  prefs: []
  type: TYPE_NORMAL
- en: We first create a sorted list of all file paths to all images in the training
    set, test set, and ground truth masks. Note that these are *not* images yet; we
    are only looking at file paths to images up to this point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then only take the first 1000 file paths to images/masks in the Carvana dataset.
    This is done to reduce the computational load and speed up training. If you have
    access to multiple powerful GPUs (lucky you!) feel free to use all of the images
    for even better performance. We also create a train/validation split of 80/20\.
    The more data (images) you include, the greater this split should lean towards
    the training set. It is not uncommon to see splits of 98/1/1 for training/validation/test
    splits when dealing with very large datasets. The more data in the training set,
    the better your model will be in general.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then create TensorFlow (TF) **Dataset** objects using the tf.data.Dataset.from_tensor_slices()
    method. Using a Dataset object is a common method of handling training, validation,
    and testing sets, as opposed to keeping them as Numpy arrays. In my experience,
    pre-processing of data is much faster and easier when using Dataset objects. [See
    this link](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices)
    for the documentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next we specify the image height, width, and number of channels for our input
    images. The actual high quality images are much larger then 96 pixels by 128 pixels;
    this **downsampling** of our images is done to reduce the computational load (larger
    images require more time for training). If you have the necessary horsepower (GPU)
    I don’t recommend downsampling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then use the .map() function of our Dataset objects to pre-process our images.
    This converts the file paths into images and does appropriate pre-processing.
    More on this in a moment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have pre-processed our raw training images and our ground-truth masks,
    we need a way to pair images with their masks. To accomplish this we use the .zip()
    function of the Dataset objects. This takes two lists of data, and joins the first
    element of each list and puts them into a tuple. It does the same for the second
    element, third, and so on. The end result is a single list full of tuples of the
    form (image, mask).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then use the .batch() function to create batches of 32 images from our one-thousand
    images. Batching is an important part of the machine learning pipeline, as it
    allows us to process multiple images at once, instead of one at a time. This speeds
    up training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally we use the .prefetch() function. This is another step that helps to
    speed up training. Loading and preprocessing data can be a bottleneck in training
    pipelines. This can lead to idle GPU or CPU time, which no one wants. While your
    model is doing forward and back propagation, the .prefetch() function can ready
    up the next batch. The AUTOTUNE variable in TensorFlow dynamically computes how
    many batches to prefetch based on your system resources; this is generally recommended.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at the pre-processing step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'These functions do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we convert the file paths to a **tensor** of data type ‘string’ using
    tf.io.read_file(). A tensor is a special data structure in TensorFlow that is
    similar to multi-dimensional arrays in other math libraries, but with special
    properties and methods that are useful for deep learning. To quote the TensorFlow
    documentation: tf.io.read_file() “does not do any parsing, it just returns the
    contents as they are.” Essentially this means it returns a binary file (1s and
    0s) in the string type containing the information of the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, we need to *decode* the binary data. To do this, we need to use the
    appropriate method in TensorFlow. Since the raw image data is in the .jpeg format,
    we use tf.image.decode_jpeg() method. Since the masks are in the GIF format, we
    can use tf.io.decode_gif(), or use the more general tf.image.decode_image() ,
    which can handle any file type. Which you choose is really unimportant. We set
    expand_animations=False because these are not really animations, they are just
    images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then we use convert_image_dtype() to convert our image data into float32\.
    **This is only done for the images, not the mask, since the mask was already decoded
    into float32**. There are two common data types used in image processing: float32
    and uint8\. Float32 stands for a floating point number (decimal) that occupies
    32 bits in computer memory. They are signed (meaning the number can be negative)
    and can range in value from 0 to 2³² = 4294967296, although by convention in image
    processing we normalize these values to be between 0 and 1, where 1 is the maximum
    of a color. Uint8 stands for an unsigned (positive) integer that goes between
    0 and 255 and only occupies 8 bits in memory. For example, we can represent the
    color burnt orange as (Red: 204, Green: 85, Blue: 0) for uint8 or (Red: 0.8, Green:
    0.33, Blue: 0) for float32\. Float32 is usually the better choice, since it offers
    more precision and already comes normalized which helps improve training. However,
    uint8 saves memory, and this can be better depending on your memory limitations.
    Using float32 in convert_image_dtype automatically normalizes the values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In binary segmentation, we expect our masks to have shape (batch, hieght, width,
    channels), with channels = 1\. In other words, we want one class (car) being represented
    by the number 1, and the other class (background) being represented by the number
    0\. There is no reason for the number of channels to be 3, as in for RGB images.
    Unfortunately, after decoding it comes with three channels, with the class number
    repeated three times. To fix this, we use tf.math.reduce_max(mask, axis=-1, keepdims=True)
    to take the maximum of the values in the three channels and get rid of the rest.
    So a channel value of (1,1,1) gets reduced to just (1) and a channel value of
    (0,0,0) gets reduced to (0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally we resize the images/masks to our desired dimensions (small). Note that
    the images I showed earlier of the car with the ground truth mask look blurry;
    this downscaling was done on purpose to reduce the computational load and allow
    training to occur relatively quickly. Using method=‘nearest’ as a default is a
    good idea; otherwise the function will always return a float32, which is bad if
    you want it to be in uint8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/d9ea340e152cd50a2ed62d40fad7d400.png)'
  prefs: []
  type: TYPE_IMG
- en: The color burnt orange can be represented in float32 or uint8 format. Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our datasets organized, we can now view our images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4de9150e514f5730a3daf8f5bff40fd2.png)'
  prefs: []
  type: TYPE_IMG
- en: Images of our cars paired with the accompanying masks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we are using the .take() method to view the first batch of data in our
    batched_val_dataset. Since we are doing binary segmentation, we want our mask
    to only contain two values: 0 and 1\. Plotting the color bars on the mask confirms
    we have the right setup. Note that we added the argument cmap = ‘gray’ to the
    mask imshow() to let plt know we want these images presented in grayscale.'
  prefs: []
  type: TYPE_NORMAL
- en: Building a Simple U-Net Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a letter dated February 5, 1675 to his rival Robert Hooke, Isaac Newton
    stated:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“If I have seen further, it is by standing on the shoulders of giants.”*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this same vein, we will stand on the shoulders of previous machine learning
    researchers who have discovered what sorts of architectures work best for the
    task of image segmentation. It is not a bad idea to experiment with architectures
    of your own; however, the researchers who have come before us have gone down many
    dead ends to discover the models that work. These architectures aren’t necessarily
    the end all be all, as research is still ongoing and a better architecture may
    yet be found.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d01011a3b09163f8eaf0dd41cc39c0f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of the U-Net, described in [1]. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the more well-known architectures is called the **U-Net**, so called
    because the downsampling and the upsampling portions of the network can be visualized
    as a U (see above). In a paper titled *U-Net: Convolutional Networks for Biomedical
    Image Segmentation* by Ronneberger, Fisher, and Brox [1], the authors describe
    how to create a **fully convolutional network** **(FCN)** that works effectively
    for image segmentation. Fully convolutional means there are no densely connected
    layers; all the layers are convolutional.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few things to note:'
  prefs: []
  type: TYPE_NORMAL
- en: The network consists of a series of repeating blocks of two convolutional layers,
    with padding = ‘same’ and stride = 1 so that the outputs of the convolutions are
    not downsized within the block.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each block is followed by a max pooling layer, which cuts down the width and
    height of the feature map in half.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next block then doubles the number of filters. And the pattern continues.
    This pattern of cutting the feature space down while increasing the number of
    filters should be familiar if you have studied CNNs. This completes what the authors
    call the “contracting path.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The “bottleneck” layer is at the bottom of the ‘U’. This layer captures highly
    abstracted features (lines, curves, windows, doors, etc.) but at a significantly
    reduced spatial resolution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next begins what they call the “expanding path.” In short, this reverses the
    contractions, with each block consisting again of two convolutional layers. Each
    block is followed by an **upsampling layer**, which in TensorFlow we call the
    Conv2DTranspose layer. This takes a smaller feature map and doubles the height
    and width.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next block then cuts the number of filters in half. Repeat the process until
    you wind up with the same height and width as the images you started with. Finally,
    finish with a 1x1 conv layer to reduce the number of channels to 1\. We want to
    finish with one channel because this is binary segmentation, so we desire a single
    filter where the pixel values correspond to our two classes. We use a sigmoid
    activation to smush the pixel values between 0 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also **skip connections** in the U-Net architecture, allowing the
    network to retain fine-grained spatial information even after downsampling and
    then upsampling. Normally there is a lot of information lost in this process.
    By passing the information from a contracting block and into the corresponding
    expanding block, we can preserve this spatial information. There is a nice symmetry
    to the architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will begin by doing a simple version of the U-Net. This will be a FCN, but
    with no residual connections and no max pooling layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here we have the same basic structure as the U-Net, with a contracting path
    and an expansion path. One interesting thing to note is that rather than use a
    max pooling layer to cut the feature space in half, we use a convolutional layer
    with strides=2\. According to Chollet [2], this cuts the feature space in half
    while preserving more spatial information than a max pooling layers. He states
    that whenever location information is important (as in image segmentation) it
    is a good idea to avoid destructive max pooling layers and stick to using strided
    convolutions instead (**this is curious, because the famous** **U-Net architecture
    does use max pooling**). Also observe that we are doing some data augmentation
    to help our model generalize to unseen examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some important details: setting the kernel_intializer to ‘he_normal’ setting
    for ReLU activations makes a surprisingly large difference in terms of training
    stability. I initially underestimated the power of kernel initialization. Rather
    than initializing the weights randomly, he_normalization initializes the weights
    to have a mean of 0 and a standard deviation of the square root of (2 / # of input
    units to layer). In the case of CNNs the number of input units refers to the number
    of channels in the feature maps of the previous layer. He_normal initialization
    has been found to lead to faster convergence, mitigate vanishing gradients, and
    improve learning. See reference [3] for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics and Loss Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several common metrics and loss functions one can use for binary segmentation.
    Here, we will use the **dice coefficient** as a metric and the corresponding **dice
    loss** for training, as this is what the competition requires.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first take a look at the mathematics behind the dice coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd23ec4488d38032c23f8f61c35f95b0.png)'
  prefs: []
  type: TYPE_IMG
- en: The dice coefficient, in the general form.
  prefs: []
  type: TYPE_NORMAL
- en: The dice coefficient is defined as the intersection between two sets (*X* and
    *Y*), divided by the sum of each set, multiplied by 2\. The dice coefficient will
    lie between 0 (if the sets have no intersection) and 1 (if the sets overlap perfectly).
    Now we see why this makes a great metric for image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb83c98c2e92e3a20843999e79e16a57.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of two masks overlayed over each other. Orange used for clarity.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The above equation is a general definition of the dice coefficient; when when
    you apply it to *vector* quantities (as opposed to sets), we use the more specific
    definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b147a9a8680ca375b9bc738af7aeb79e.png)'
  prefs: []
  type: TYPE_IMG
- en: The dice coefficient, in the vector form.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are iterating over each element (pixel) in each mask. *x* represents
    the *i*th pixel in the predicted mask and *y* represents the corresponding pixel
    in the ground truth mask. On top we are doing the element-wise product, and on
    bottom we are summing over all elements in each mask independently. *N* represents
    the total number of pixels (which should be the same for both predicted and target
    masks.) Remember that in our masks, the numbers will all be either 0s or 1s, so
    a pixel with a value of 1 in the ground truth mask and a corresponding pixel in
    the predicted mask with a value of 0 will not contribute to the dice score, as
    expected (1 x 0 = 0).
  prefs: []
  type: TYPE_NORMAL
- en: The **dice loss** will be simply defined as 1 — Dice Score. Since the dice score
    is between 0 and 1, the dice loss will also be between 0 and 1\. In fact the sum
    of the dice score and the dice loss must equal 1\. They are inversely related.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at how this is implemented in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here we are flattening two 4-D masks (batch, height, width, channels=1) into
    1-D vectors, and computing the dice scores for all images in the batch. Note that
    we adding a smoothing value to both the numerator and denominator to prevent having
    a 0/0 issue if the two masks do not overlap.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we begin training. We are doing early stopping to prevent overfitting
    and saving the best model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can determine the results of our training with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After 10 epochs, we arrive at a top validation dice score of 0.8788\. Not terrible,
    but not great. On a P100 GPU this took me about 20 minutes. Here is a sample mask
    for our review:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7cbd52526f40648c35425e6eabfb9170.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of input image, true mask, and predicted mask. By the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Highlighting a few interesting points:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that create_mask is the function that pushes pixel values to either 0 or
    1\. A pixel value of < 0.5 will will be cut to 0 and we will assign that pixel
    to the “background” category. A value ≥ 0.5 will be increased to 1 and we will
    call assign that pixel to the “car” category.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Why did the masks come out yellow and purple, instead of black and white? We
    used: tf.keras.preprocessing.image.array_to_img() to converts the output of the
    mask from a tensor to a PIL Image. We then passed the image to plt.imshow(). [From
    the documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html)
    we see that that the default colormap for single channel images is “viridis” (3-channel
    RGB images get output as is.) The viridis colormap transforms low values to a
    deep purple and a high values to yellow. This colormap can apparently help people
    with colorblindness get an accurate view of the color in an image. We could’ve
    fixed this by adding cmap=“grayscale” as an argument, but this would’ve messed
    up our input image. See more here [at this link](https://en.wikipedia.org/wiki/Color_blindness).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/82384d6f3ece15994a1d089ad40e0d75.png)'
  prefs: []
  type: TYPE_IMG
- en: The viridis colormap, from low values (purple) to high values (yellow). By the
    author.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Complete U-Net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we turn to using the full U-Net architecture, with residual connections,
    max pooling layers, and including dropout layers for regularization. Note the
    contracting path, bottleneck layer, and expanding path. The dropout layers can
    be added in the contracting path, at the end of each block.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We then compile the U-Net. I am using 64 filters for the first contracting block.
    This is a hyperparameter that you would want to tune for optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After 16 epochs, I get a validation dice score of 0.9416, much better then
    with the simple U-Net. This shouldn’t be too surprising; looking at the parameter
    count we have an order of magnitude increase from the simple U-Net to the complete
    U-Net. On a P100 GPU this took me about 32 minutes. We then take a peek at the
    predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/49623a94ebb3eda2868cbb2909a1dd30.png)'
  prefs: []
  type: TYPE_IMG
- en: Predicted mask for the complete U-Net. Much better! By the author.
  prefs: []
  type: TYPE_NORMAL
- en: These predictions are much better. One thing to note from looking at multiple
    predictions is that the antenna sticking out of the cars is tough for the network.
    Given that the images are very pixelated, I can’t blame the network for missing
    this.
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve performance, one would look at tweaking hyperparameters including:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of downsampling and upsampling blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image resolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size of training set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss function (perhaps combining dice loss with binary cross-entropy loss)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adjusting optimizer parameters. Training stability seems to be an issue for
    both models. [From the documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)
    for the Adam optimizer: “The default value of 1e-7 for epsilon might not be a
    good default in general.” Increasing epsilon by an order of magnitude or more
    may help with training stability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can already see the road to an excellent score on the Carvana challenge.
    Too bad it’s already over!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This article was a deep dive on the topic of image segmentation, specifically
    **binary segmentation**. If you take anything away, remember the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of image segmentation is to find a mapping from input pixel values
    in an image to output numbers that your model can use to assign classes to each
    pixel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the first steps is to organize your images into TensorFlow Dataset objects
    and take a look at your images and corresponding masks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is no need to re-invent the wheel when it comes to model architecture:
    we know from experience that a U-Net works well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dice score is a common metric that is used for monitoring the success of
    your model’s predictions. We can also get our loss function from this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Future work could go into converting the max pooling layers in the canonical
    U-Net architecture into strided convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: Best of luck on your image segmentation problems!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] O. Ronneberger, P. Fischer, and T. Brox, [U-Net: Convolutional Networks
    for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) (2015), MICCAI
    2015 International Conference'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] F. Chollet, Deep Learning with Python (2021), Manning Publications Co.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] K. He, X. Zhang, S. Ren, J. Sun, [Delving Deep into Rectifiers: Surpassing
    Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)
    (2015), International Conference on Computer Vision (ICCV)'
  prefs: []
  type: TYPE_NORMAL
