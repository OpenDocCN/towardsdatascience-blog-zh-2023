- en: Modern Semantic Search for Images
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代图像语义搜索
- en: 原文：[https://towardsdatascience.com/modern-semantic-search-for-images-cb1a3242631d?source=collection_archive---------7-----------------------#2023-11-14](https://towardsdatascience.com/modern-semantic-search-for-images-cb1a3242631d?source=collection_archive---------7-----------------------#2023-11-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/modern-semantic-search-for-images-cb1a3242631d?source=collection_archive---------7-----------------------#2023-11-14](https://towardsdatascience.com/modern-semantic-search-for-images-cb1a3242631d?source=collection_archive---------7-----------------------#2023-11-14)
- en: A how-to article leveraging Python, Pinecone, Hugging Face, and the Open AI
    CLIP model to create a semantic search application for your cloud photos.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一篇利用 Python、Pinecone、Hugging Face 和 Open AI CLIP 模型来创建云照片语义搜索应用程序的操作指南。
- en: '[](https://medium.com/@joshpoduska?source=post_page-----cb1a3242631d--------------------------------)[![Josh
    Poduska](../Images/89ee323bac41d31083206a37df1dd0a3.png)](https://medium.com/@joshpoduska?source=post_page-----cb1a3242631d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb1a3242631d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb1a3242631d--------------------------------)
    [Josh Poduska](https://medium.com/@joshpoduska?source=post_page-----cb1a3242631d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@joshpoduska?source=post_page-----cb1a3242631d--------------------------------)[![Josh
    Poduska](../Images/89ee323bac41d31083206a37df1dd0a3.png)](https://medium.com/@joshpoduska?source=post_page-----cb1a3242631d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb1a3242631d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb1a3242631d--------------------------------)
    [Josh Poduska](https://medium.com/@joshpoduska?source=post_page-----cb1a3242631d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6dae10267e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodern-semantic-search-for-images-cb1a3242631d&user=Josh+Poduska&userId=b6dae10267e5&source=post_page-b6dae10267e5----cb1a3242631d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb1a3242631d--------------------------------)
    ·6 min read·Nov 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb1a3242631d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodern-semantic-search-for-images-cb1a3242631d&user=Josh+Poduska&userId=b6dae10267e5&source=-----cb1a3242631d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6dae10267e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodern-semantic-search-for-images-cb1a3242631d&user=Josh+Poduska&userId=b6dae10267e5&source=post_page-b6dae10267e5----cb1a3242631d---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb1a3242631d--------------------------------)
    ·6 min read·2023年11月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb1a3242631d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodern-semantic-search-for-images-cb1a3242631d&user=Josh+Poduska&userId=b6dae10267e5&source=-----cb1a3242631d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb1a3242631d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodern-semantic-search-for-images-cb1a3242631d&source=-----cb1a3242631d---------------------bookmark_footer-----------)![](../Images/1c4e7abefc2b87c21d19e23600a9530b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb1a3242631d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodern-semantic-search-for-images-cb1a3242631d&source=-----cb1a3242631d---------------------bookmark_footer-----------)![](../Images/1c4e7abefc2b87c21d19e23600a9530b.png)'
- en: Image by the author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'You want to find “that one picture” from several years ago. You remember a
    few details about the setting and want to search based on a specific phrase. Apple
    Photos doesn’t offer semantic search, and Google Photos is limited to a few predetermined
    item classifiers. Neither will do well with this kind of search. I’ll demonstrate
    the issue with two unusual queries of my Google Photos: “donut birthday cake”
    and “busted lip from a snowball fight”. Then I’ll share how to build your own
    semantic image search application.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你想找到“那张几年前的照片”。你记得一些场景细节，想根据特定的短语进行搜索。Apple Photos 不提供语义搜索，而 Google Photos 仅限于少数预定的项目分类器。两者在这类搜索中表现都不佳。我将用两个不寻常的
    Google Photos 查询来演示这个问题：“甜甜圈生日蛋糕”和“被雪仗打破的嘴唇”。然后我会分享如何构建自己的语义图像搜索应用程序。
- en: 'Demonstration: current limitations compared to modern semantic image search'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演示：当前局限性与现代语义图像搜索的比较
- en: 'Example #1'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '示例 #1'
- en: I like birthday cakes. I also like donuts. Last year, I had the brilliant idea
    to combine the two with a stack of donuts as my birthday cake. Let’s try to find
    it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢生日蛋糕。我也喜欢甜甜圈。去年，我有一个绝妙的主意，把两者结合起来，用一堆甜甜圈作为我的生日蛋糕。让我们尝试找到它。
- en: '**Google Photos query: “**donut birthday cake”'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**Google Photos 查询：“**甜甜圈生日蛋糕”'
- en: '**Results:** Six pictures of cakes with no donuts followed by the one I wanted.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果：** 六张没有甜甜圈的蛋糕图片，接着是一张我想要的图片。'
- en: '![](../Images/dd7019d79c347a6cfc0832aed02f9513.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd7019d79c347a6cfc0832aed02f9513.png)'
- en: Image by the author
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '**Semantic Search App query: “**donut birthday cake”'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**语义搜索应用查询：“**甜甜圈生日蛋糕”'
- en: '**Results:** Two images and a video that were exactly what I wanted.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果：** 两张和一个视频完全符合我想要的。'
- en: '![](../Images/7aed89626ffb2d725c711f6c4a81564e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7aed89626ffb2d725c711f6c4a81564e.png)'
- en: Image by the author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '**Example #2**'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**示例 #2**'
- en: I went to the snow with my teenage son and a big group of his friends. They
    climbed on top of an abandoned train tunnel. “Throw snowballs all at once, and
    I’ll get a slow-motion video of it!”, I yelled. It was not my brightest moment
    as I didn’t foresee the obvious conclusion that I would end up being target practice
    for twenty teenage boys with strong arms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我和我的青少年儿子及他的大群朋友一起去滑雪。他们爬上了一个废弃的火车隧道。 “一次性扔雪球，我会拍慢动作视频！”，我喊道。那并不是我最聪明的时刻，因为我没有预见到一个明显的结论，即我最终会成为二十个拥有强壮臂膀的青少年作为目标练习的对象。
- en: '**Google Photos query: “**busted lip from a snowball fight”'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**Google Photos 查询：“**雪仗中的破裂嘴唇”'
- en: '**Results:**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果：**'
- en: '![](../Images/dab76c3befff8ce7b26fe2ae9da7df38.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dab76c3befff8ce7b26fe2ae9da7df38.png)'
- en: Image created by the author
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作者创建的图片
- en: The current Google image classification model is limited to words it has been
    trained on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的 Google 图像分类模型仅限于它所训练过的词汇。
- en: '**Semantic Search App query: “**busted lip from a snowball fight”'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**语义搜索应用查询：“**雪仗中的破裂嘴唇”'
- en: '**Results:** The busted lip picture (not shown) and the video that preceded
    the busted lip were results one and two.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果：** 破裂的嘴唇照片（未展示）和破裂嘴唇之前的视频分别是结果一和二。'
- en: '![](../Images/1c4e7abefc2b87c21d19e23600a9530b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c4e7abefc2b87c21d19e23600a9530b.png)'
- en: Image by the author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: OpenAI CLIP model and application architecture
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI CLIP 模型和应用架构
- en: '[CLIP](https://openai.com/research/clip) allows the model to learn how to associate
    image pixels with text and gives it the flexibility to look for things like “donut
    cakes” and “busted lips” — things that you’d never think to include when training
    an image classifier. It stands for Constastive Language-Image Pretraining. It
    is an open-source, multi-modal, zero-shot model. It has been trained on millions
    of images with descriptive captions.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLIP](https://openai.com/research/clip) 使模型能够学习如何将图像像素与文本关联，并赋予它寻找“甜甜圈蛋糕”和“破裂的嘴唇”等内容的灵活性——这些内容在训练图像分类器时你可能从未考虑过。它代表了对比语言-图像预训练。它是一个开源的多模态零样本模型。它在数百万张带有描述性字幕的图像上进行了训练。'
- en: Given an image and text descriptions, the model can predict that image's most
    relevant text description, without optimizing for a particular task.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 给定图像和文本描述，模型可以预测该图像最相关的文本描述，而无需针对特定任务进行优化。
- en: '![](../Images/5b99366aac6000d76388a6ba9ff53b17.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b99366aac6000d76388a6ba9ff53b17.png)'
- en: '[Source: Nikos Karfitsas, Towards Data Science](/clip-the-most-influential-ai-model-from-openai-and-how-to-use-it-f8ee408958b1)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源: Nikos Karfitsas, Towards Data Science](/clip-the-most-influential-ai-model-from-openai-and-how-to-use-it-f8ee408958b1)'
- en: The CLIP architecture that you find in most online tutorials is good enough
    for a POC but is not enterprise-ready. In these tutorials, CLIP and the Hugging
    Face processors hold embeddings in memory to act as the vector store for running
    similarity scores and retrieval.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数在线教程中的 CLIP 架构对于概念验证来说足够好，但不适合企业级应用。在这些教程中，CLIP 和 Hugging Face 处理器将嵌入保持在内存中，以作为运行相似度评分和检索的向量存储。
- en: '![](../Images/be21708790b6b623a350f154e8a4e891.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be21708790b6b623a350f154e8a4e891.png)'
- en: Image created by the author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作者创建的图片
- en: A vector database like Pinecone is a key component to scaling an application
    like this. It provides simplified, robust, enterprise-ready features such as batch
    and stream processing of images, enterprise management of embeddings, low latency
    retrieval, and metadata filtering.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Pinecone 这样的向量数据库是扩展此类应用程序的关键组件。它提供了简化、稳健、企业级的功能，如图像的批处理和流处理、嵌入的企业管理、低延迟检索和元数据过滤。
- en: '![](../Images/9fa02718f8ecf9777750b0f46ad35495.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fa02718f8ecf9777750b0f46ad35495.png)'
- en: Image created by the author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者创建的图片
- en: Building the app
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建应用程序
- en: The code and supporting files for this application can be found on GitHub at
    [https://github.com/joshpoduska/llm-image-caption-semantic-search](https://github.com/joshpoduska/llm-image-caption-semantic-search).
    Use them to build a semantic search application for your cloud photos.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此应用程序的代码和支持文件可在 GitHub 上找到，链接为 [https://github.com/joshpoduska/llm-image-caption-semantic-search](https://github.com/joshpoduska/llm-image-caption-semantic-search)。使用它们构建一个用于云照片的语义搜索应用程序。
- en: The application runs locally on a laptop with sufficient memory. I tested it
    on a MacBook Pro.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序在配备足够内存的笔记本电脑上本地运行。我在 MacBook Pro 上进行了测试。
- en: Components needed to build the app
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建应用程序所需的组件。
- en: '**Pinecone** or similar vector database for embedding storage and semantic
    search (the free version of Pinecone is sufficient for this tutorial)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**松果** 或类似的向量数据库用于嵌入存储和语义搜索（本教程使用免费版松果已足够）。'
- en: '**Hugging Face** models and pipelines'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face** 模型和管道。'
- en: '**OpenAI CLIP** **model** for image and query text embedding creation (accessible
    from Hugging Face)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI CLIP** **模型**用于图像和查询文本嵌入的创建（从 Hugging Face 可访问）。'
- en: '**Google Photos API** to access your personal Google Photos'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Photos API** 用于访问您的个人 Google Photos。'
- en: Helpful information before you start
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在开始之前提供有用信息。
- en: GitHub user, polzerdo55862, has a [great notebook tutorial](https://github.com/polzerdo55862/google-photos-api/blob/main/Google_API.ipynb)
    on using the Google Photos API via Python
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub 用户，polzerdo55862，通过 Python 在使用 Google Photos API 上有一个 [优秀的笔记本教程](https://github.com/polzerdo55862/google-photos-api/blob/main/Google_API.ipynb)。
- en: The [Pinecone quick tour](https://github.com/pinecone-io/examples/blob/master/docs/quick-tour/hello-pinecone.ipynb)
    shows how to initialize, fill, and delete a Pinecone “index”
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[松果快速浏览](https://github.com/pinecone-io/examples/blob/master/docs/quick-tour/hello-pinecone.ipynb)展示了如何初始化、填充和删除松果“索引”。'
- en: Pinecone examples of [how to query an index](https://github.com/pinecone-io/examples/blob/master/docs/semantic-search.ipynb)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 松果示例展示了 [如何查询索引](https://github.com/pinecone-io/examples/blob/master/docs/semantic-search.ipynb)。
- en: HuggingFace example of [how to use CLIP in a stand-alone query and search pipeline](https://huggingface.co/openai/clip-vit-large-patch14)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingFace 示例展示了 [如何在独立查询和搜索管道中使用 CLIP](https://huggingface.co/openai/clip-vit-large-patch14)。
- en: Antti Havanko shared an example of [how to use CLIP to generate embeddings for
    use in a vector search engine](https://anttihavanko.medium.com/building-image-search-with-openai-clip-5a1deaa7a6e2)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Antti Havanko 分享了一个示例，展示了如何使用 CLIP 生成嵌入以在向量搜索引擎中使用，链接为 [如何使用 CLIP 生成嵌入](https://anttihavanko.medium.com/building-image-search-with-openai-clip-5a1deaa7a6e2)。
- en: Access your images
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问您的图像。
- en: The Google Photos API has several key data fields of note. See [the API reference](https://developers.google.com/photos/library/guides/access-media-items)
    for more details.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Google Photos API 具有几个关键的数据字段值得注意。更多细节请参阅 [API 参考文档](https://developers.google.com/photos/library/guides/access-media-items)。
- en: '**Id** is Immutable'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Id** 是不可变的。'
- en: '**baseUrl** allows you to access the bytes of the media items. They are valid
    for 60 minutes.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**baseUrl** 允许您访问媒体项的字节。它们有效期为 60 分钟。'
- en: A combination of the pandas, JSON, and requests libraries can be used straightforwardly
    to load a DataFrame of your image IDs, URLs, and dates.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以简单地使用 pandas、JSON 和 requests 库来加载包含图像 ID、URL 和日期的 DataFrame。
- en: Generate image embeddings
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成图像嵌入。
- en: With Hugging Face and the OpenAI CLIP model, this step is the simplest of the
    entire application.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Hugging Face 和 OpenAI CLIP 模型，这一步是整个应用程序中最简单的步骤。
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Creating metadata
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建元数据。
- en: Semantic search is often enhanced with metadata filters. In this application,
    I use the date of the photo to extract the year, month, and day. These are stored
    as a dictionary in a DataFrame field. Pinecone queries can use this dictionary
    to filter searches by metadata in the dictionary.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索通常通过元数据过滤得到增强。在此应用程序中，我使用照片的日期来提取年、月和日。这些信息存储为 DataFrame 字段中的字典。松果查询可以使用此字典通过字典中的元数据来过滤搜索。
- en: Here is the first row of my pandas DataFrame with the image fields, vectors,
    and metadata dictionary field.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我 pandas DataFrame 的第一行，包含图像字段、向量和元数据字典字段。
- en: '![](../Images/942e28262b92ccdbd38c53267175c137.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/942e28262b92ccdbd38c53267175c137.png)'
- en: Image by the author
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供。
- en: Load embeddings
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载嵌入。
- en: There are Pinecone optimizations for async and parallel loading. The base loading
    function is simple, as follows.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 松果有关异步和并行加载的优化。基本加载功能如下所示。
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Query embeddings
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询嵌入。
- en: To query the images with the CLIP model, we need to pass it the text of our
    semantic query. This is facilitated by loading the CLIP text embedding model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 CLIP 模型查询图像，我们需要传递语义查询文本。通过加载 CLIP 文本嵌入模型来简化此过程。
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we can create an embedding for our search phrase and compare that to the
    embeddings of the images stored in Pinecone.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以为我们的搜索短语创建一个嵌入，并将其与存储在Pinecone中的图像嵌入进行比较。
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Conclusion
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The CLIP model is amazing. It is a general knowledge, zero-shot model that has
    learned to associate images with text in a way that frees it from the constraints
    of training an image classifier on pre-defined classes. When we combine this with
    the power of an enterprise-grade vector database like Pinecone, we can create
    semantic image search applications with low latency and high fidelity. This is
    just one of the exciting applications of generative AI sprouting up daily.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP模型非常了不起。它是一个通用知识的零样本模型，已经学会了将图像与文本关联，从而摆脱了在预定义类别上训练图像分类器的限制。当我们将这一点与像Pinecone这样的企业级向量数据库的强大功能结合时，我们可以创建具有低延迟和高保真度的语义图像搜索应用。这只是生成性AI每天涌现出的令人兴奋的应用之一。
