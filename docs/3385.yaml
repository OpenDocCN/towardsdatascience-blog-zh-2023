- en: Modern Semantic Search for Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/modern-semantic-search-for-images-cb1a3242631d?source=collection_archive---------7-----------------------#2023-11-14](https://towardsdatascience.com/modern-semantic-search-for-images-cb1a3242631d?source=collection_archive---------7-----------------------#2023-11-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A how-to article leveraging Python, Pinecone, Hugging Face, and the Open AI
    CLIP model to create a semantic search application for your cloud photos.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@joshpoduska?source=post_page-----cb1a3242631d--------------------------------)[![Josh
    Poduska](../Images/89ee323bac41d31083206a37df1dd0a3.png)](https://medium.com/@joshpoduska?source=post_page-----cb1a3242631d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb1a3242631d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb1a3242631d--------------------------------)
    [Josh Poduska](https://medium.com/@joshpoduska?source=post_page-----cb1a3242631d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb6dae10267e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodern-semantic-search-for-images-cb1a3242631d&user=Josh+Poduska&userId=b6dae10267e5&source=post_page-b6dae10267e5----cb1a3242631d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb1a3242631d--------------------------------)
    ·6 min read·Nov 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb1a3242631d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodern-semantic-search-for-images-cb1a3242631d&user=Josh+Poduska&userId=b6dae10267e5&source=-----cb1a3242631d---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb1a3242631d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodern-semantic-search-for-images-cb1a3242631d&source=-----cb1a3242631d---------------------bookmark_footer-----------)![](../Images/1c4e7abefc2b87c21d19e23600a9530b.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'You want to find “that one picture” from several years ago. You remember a
    few details about the setting and want to search based on a specific phrase. Apple
    Photos doesn’t offer semantic search, and Google Photos is limited to a few predetermined
    item classifiers. Neither will do well with this kind of search. I’ll demonstrate
    the issue with two unusual queries of my Google Photos: “donut birthday cake”
    and “busted lip from a snowball fight”. Then I’ll share how to build your own
    semantic image search application.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Demonstration: current limitations compared to modern semantic image search'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Example #1'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I like birthday cakes. I also like donuts. Last year, I had the brilliant idea
    to combine the two with a stack of donuts as my birthday cake. Let’s try to find
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Photos query: “**donut birthday cake”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Results:** Six pictures of cakes with no donuts followed by the one I wanted.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd7019d79c347a6cfc0832aed02f9513.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic Search App query: “**donut birthday cake”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Results:** Two images and a video that were exactly what I wanted.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7aed89626ffb2d725c711f6c4a81564e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**Example #2**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I went to the snow with my teenage son and a big group of his friends. They
    climbed on top of an abandoned train tunnel. “Throw snowballs all at once, and
    I’ll get a slow-motion video of it!”, I yelled. It was not my brightest moment
    as I didn’t foresee the obvious conclusion that I would end up being target practice
    for twenty teenage boys with strong arms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Photos query: “**busted lip from a snowball fight”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Results:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dab76c3befff8ce7b26fe2ae9da7df38.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author
  prefs: []
  type: TYPE_NORMAL
- en: The current Google image classification model is limited to words it has been
    trained on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic Search App query: “**busted lip from a snowball fight”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Results:** The busted lip picture (not shown) and the video that preceded
    the busted lip were results one and two.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c4e7abefc2b87c21d19e23600a9530b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI CLIP model and application architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[CLIP](https://openai.com/research/clip) allows the model to learn how to associate
    image pixels with text and gives it the flexibility to look for things like “donut
    cakes” and “busted lips” — things that you’d never think to include when training
    an image classifier. It stands for Constastive Language-Image Pretraining. It
    is an open-source, multi-modal, zero-shot model. It has been trained on millions
    of images with descriptive captions.'
  prefs: []
  type: TYPE_NORMAL
- en: Given an image and text descriptions, the model can predict that image's most
    relevant text description, without optimizing for a particular task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b99366aac6000d76388a6ba9ff53b17.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source: Nikos Karfitsas, Towards Data Science](/clip-the-most-influential-ai-model-from-openai-and-how-to-use-it-f8ee408958b1)'
  prefs: []
  type: TYPE_NORMAL
- en: The CLIP architecture that you find in most online tutorials is good enough
    for a POC but is not enterprise-ready. In these tutorials, CLIP and the Hugging
    Face processors hold embeddings in memory to act as the vector store for running
    similarity scores and retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be21708790b6b623a350f154e8a4e891.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author
  prefs: []
  type: TYPE_NORMAL
- en: A vector database like Pinecone is a key component to scaling an application
    like this. It provides simplified, robust, enterprise-ready features such as batch
    and stream processing of images, enterprise management of embeddings, low latency
    retrieval, and metadata filtering.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9fa02718f8ecf9777750b0f46ad35495.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author
  prefs: []
  type: TYPE_NORMAL
- en: Building the app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code and supporting files for this application can be found on GitHub at
    [https://github.com/joshpoduska/llm-image-caption-semantic-search](https://github.com/joshpoduska/llm-image-caption-semantic-search).
    Use them to build a semantic search application for your cloud photos.
  prefs: []
  type: TYPE_NORMAL
- en: The application runs locally on a laptop with sufficient memory. I tested it
    on a MacBook Pro.
  prefs: []
  type: TYPE_NORMAL
- en: Components needed to build the app
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pinecone** or similar vector database for embedding storage and semantic
    search (the free version of Pinecone is sufficient for this tutorial)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hugging Face** models and pipelines'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI CLIP** **model** for image and query text embedding creation (accessible
    from Hugging Face)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Photos API** to access your personal Google Photos'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helpful information before you start
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GitHub user, polzerdo55862, has a [great notebook tutorial](https://github.com/polzerdo55862/google-photos-api/blob/main/Google_API.ipynb)
    on using the Google Photos API via Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [Pinecone quick tour](https://github.com/pinecone-io/examples/blob/master/docs/quick-tour/hello-pinecone.ipynb)
    shows how to initialize, fill, and delete a Pinecone “index”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinecone examples of [how to query an index](https://github.com/pinecone-io/examples/blob/master/docs/semantic-search.ipynb)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HuggingFace example of [how to use CLIP in a stand-alone query and search pipeline](https://huggingface.co/openai/clip-vit-large-patch14)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Antti Havanko shared an example of [how to use CLIP to generate embeddings for
    use in a vector search engine](https://anttihavanko.medium.com/building-image-search-with-openai-clip-5a1deaa7a6e2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access your images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Google Photos API has several key data fields of note. See [the API reference](https://developers.google.com/photos/library/guides/access-media-items)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: '**Id** is Immutable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**baseUrl** allows you to access the bytes of the media items. They are valid
    for 60 minutes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A combination of the pandas, JSON, and requests libraries can be used straightforwardly
    to load a DataFrame of your image IDs, URLs, and dates.
  prefs: []
  type: TYPE_NORMAL
- en: Generate image embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With Hugging Face and the OpenAI CLIP model, this step is the simplest of the
    entire application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Creating metadata
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semantic search is often enhanced with metadata filters. In this application,
    I use the date of the photo to extract the year, month, and day. These are stored
    as a dictionary in a DataFrame field. Pinecone queries can use this dictionary
    to filter searches by metadata in the dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the first row of my pandas DataFrame with the image fields, vectors,
    and metadata dictionary field.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/942e28262b92ccdbd38c53267175c137.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Load embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are Pinecone optimizations for async and parallel loading. The base loading
    function is simple, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Query embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To query the images with the CLIP model, we need to pass it the text of our
    semantic query. This is facilitated by loading the CLIP text embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now we can create an embedding for our search phrase and compare that to the
    embeddings of the images stored in Pinecone.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CLIP model is amazing. It is a general knowledge, zero-shot model that has
    learned to associate images with text in a way that frees it from the constraints
    of training an image classifier on pre-defined classes. When we combine this with
    the power of an enterprise-grade vector database like Pinecone, we can create
    semantic image search applications with low latency and high fidelity. This is
    just one of the exciting applications of generative AI sprouting up daily.
  prefs: []
  type: TYPE_NORMAL
