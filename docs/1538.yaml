- en: What are the fairness implications of encoding categorical protected attributes?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229?source=collection_archive---------5-----------------------#2023-05-06](https://towardsdatascience.com/what-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229?source=collection_archive---------5-----------------------#2023-05-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring the Impact of Encoding Protected Attributes on Fairness in ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@carmougan?source=post_page-----2cc1dd0f5229--------------------------------)[![Carlos
    Mougan](../Images/7a56269362fc48c7a6179474b106857a.png)](https://medium.com/@carmougan?source=post_page-----2cc1dd0f5229--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2cc1dd0f5229--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2cc1dd0f5229--------------------------------)
    [Carlos Mougan](https://medium.com/@carmougan?source=post_page-----2cc1dd0f5229--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd5344df58d03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229&user=Carlos+Mougan&userId=d5344df58d03&source=post_page-d5344df58d03----2cc1dd0f5229---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2cc1dd0f5229--------------------------------)
    ·7 min read·May 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2cc1dd0f5229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229&user=Carlos+Mougan&userId=d5344df58d03&source=-----2cc1dd0f5229---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2cc1dd0f5229&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-are-the-fairness-implications-of-encoding-categorical-protected-attributes-2cc1dd0f5229&source=-----2cc1dd0f5229---------------------bookmark_footer-----------)![](../Images/b3e4278cc0dbc42f6be7eb77d99a9e0c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Lady Justice at dawn from [Unsplash](https://www.istockphoto.com/es/foto/lady-justice-al-amanecer-gm1318486133-405603913)
  prefs: []
  type: TYPE_NORMAL
- en: We will explore the world of categorical attribute encoding and its implications
    for machine learning models in terms of accuracy and fairness. Categorical attributes,
    such as country of birth or ethnicity, play a crucial role in determining the
    presence of sensitive information in data. However, many machine learning algorithms
    struggle to directly process categorical attributes, necessitating the use of
    encoding methods to transform them into numerical features that the models can
    utilize. Furthermore, we study the implication of intersectional fairness engineering.
  prefs: []
  type: TYPE_NORMAL
- en: This blog contains a summary of our **AI, Ethics, and Society (AIES’23)** conference
    paper, **Fairness implications of encoding protected categorical attributes**,
    [[link](https://arxiv.org/abs/2201.11358)].
  prefs: []
  type: TYPE_NORMAL
- en: '***TLDR;*** *Encoding protected attributes improved model performance but also
    increases fairness violation. They can be reconciled by target encoding regularization.
    Engineering intersectional protected attribute (Gender-Race) increases performance
    but heavily increases fairness violations.*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sensitive attributes are central to fairness, as are their handling throughout
    the machine learning pipeline. Many machine learning algorithms require categorical
    attributes to be suitably encoded as numerical data before being fed to algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '***What are the implications of encoding categorical protected attributes?***'
  prefs: []
  type: TYPE_NORMAL
- en: '**Types of Induced Bias**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We investigate two types of biases that can arise from these encoding methods:
    irreducible bias and reducible bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Irreducible bias:** It refers to (direct) group discrimination arising from
    the categorization of groups into labels: more data about the compared groups
    do not reduce this type of bias. In the COMPAS dataset, criminal ethnicity was
    paramount when determining recidivism scores; the numerical encoding of large
    ethnicity groups such as *African-Americans* or *Caucasians* may lead to discrimination,
    which is an unfair effect coming from the irreducible bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reducible bias:** arises due to the variance when encoding groups with a
    small statistical representation, sometimes evenvery few instances of the group.
    Reducible bias can be found and introduced when encoding the ethnicity category
    *Arabic*, which is rarely represented in the data, provoking a large sampling
    variance that ends in an almost random and unrealistic encoding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoding methods**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**One hot encoding:** This is the most established encoding method for categorical
    features and is also the default method within the fairness literature.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bab53dcac4ac9357a9736db6654c298c.png)'
  prefs: []
  type: TYPE_IMG
- en: One Hot Encoding over illustrative example. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Target Encoding:** categorical features are replaced with the mean target
    value of each respective category. This technique handles high cardinality categorical
    data, and categories are ordered. The main drawback of target encoding appears
    when categories with few samples are replaced by values close to the desired target.
    This introduces bias to the model as it over-trusts the target encoded feature
    and makes it prone to overfitting and reducible bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, this type of encoding allows for regularization. In this paper,
    we study two types of regularization (in the blog, we only study Gaussian)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0d53d7dcb08e053827801e160c5d4c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Target Encoding over illustrative example. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For predictive performance, we use AUC and fairness metrics. Between the reference
    group (r) and the group, we want to compare (i). Z is the protected attribute,
    and \hat{Y} is the model prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Demographic Parity
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/641e5d5a310c41356bac49d93c2bbb2c.png)'
  prefs: []
  type: TYPE_IMG
- en: The difference between favorable outcomes received by the unprivileged group
    and privileged group
  prefs: []
  type: TYPE_NORMAL
- en: Equal Opportunity Fairness
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac4c65829d26fc3c5bd1c57fc0cf8f3a.png)'
  prefs: []
  type: TYPE_IMG
- en: Ensuring fair opportunity instead of raw outcomes. The value is the difference
    in the
  prefs: []
  type: TYPE_NORMAL
- en: True Positive Rate (TPR) between the protected group and
  prefs: []
  type: TYPE_NORMAL
- en: the reference group
  prefs: []
  type: TYPE_NORMAL
- en: Average Absolute Odds
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e06f1f4f8272733447eae1d2ca29488.png)'
  prefs: []
  type: TYPE_IMG
- en: The sum of the absolute differences between the True Positive Rates and the
    False Positive Rates of the unprivileged group plus the same ratio for the privileged
    group
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment Hypothesis**'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding the protected attribute
  prefs: []
  type: TYPE_NORMAL
- en: Improves performance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increases fairness violations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performance and fairness can be reconciled with target encoding regularization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/b618cc20c6b8c700b2e9dac280d95de9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure: Comparing OHE and target encoding regularization (Gaussian noise) for
    the Logistic Regression over COMPAS dataset. Red dots regard different regularization
    parameters: the darker the red the higher the regularization. The blue dot regards
    the one-hot encoding. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7f34b2884ac148042ee7ae252702788.png)'
  prefs: []
  type: TYPE_IMG
- en: Impact of the Gaussian noise regularization parameter $\lambda$ on performance
    and fairness metrics over the test set of the COMPAS dataset using a Logistic
    Regression with L1 penalty. In the left image, the AUC of all the protected groups
    is over the regularization hyperparameter. On the right, the equal opportunity
    fairness, demographic parity, and average absolute oods variation throughout the
    regularization hyperparameter. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In the experiment, we showed that the most used categorical encoding method
    in the fair machine learning literature, one-hot encoding, discriminates more
    regarding equal opportunity fairness than target encoding. However, target encoding
    shows promising results. Target encoding using Gaussian regularization shows improvements
    under the presence of both types of biases, with the risk of a noticeable loss
    of model performance in the case of over-parametrization.
  prefs: []
  type: TYPE_NORMAL
- en: Intersectional Fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the quest for fairness in machine learning, it is essential to recognize
    the complex interplay of attributes and their impact on discrimination. This section
    delves into the effects of encoding categorical attributes on intersectional fairness,
    focusing on insights gained from the COMPAS dataset. We put forth hypotheses concerning
    the potential degradation of fairness through attribute engineering, the propensity
    for categorical encoding to increase discrimination, and the efficacy of regularization
    techniques in mitigating intersectional biases.
  prefs: []
  type: TYPE_NORMAL
- en: To explore the effects of encodings on intersectional fairness, we analyze the
    concatenated “Ethnic” and “Marital Status” attributes in the COMPAS dataset. By
    selecting the “Caucasian Married” group as the reference, we compare the maximum
    fairness violation across all groups. To facilitate comprehension, we utilize
    the generalized linear model from the previous section and emphasize the Equal
    Opportunity Fairness metric, which aligns with the behavior of other fairness
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d1f54272e82a0c62bbef0e8edcdd335.png)'
  prefs: []
  type: TYPE_IMG
- en: Equal opportunity fairness implications of encoding categorical protected attributes
    and their regularization effects on the Compas Dataset. Horizontal lines are the
    baselines where the protected attribute is not included in the training data.
    Regularized target encoding does not harm fairness metrics but can improve this
    dataset's predictive performance. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The above figure visually demonstrates how attribute concatenation creates intersectional
    attributes and exacerbates fairness violations, providing empirical support for
    our first hypothesis. Remarkably, even when the protected attributes are not encoded
    (represented by horizontal lines), the maximum fairness violation increases substantially
    from 0.015 for “Ethnic” or 0.08 for “Marital Status” to 0.16 for the intersectional
    attribute. This finding substantiates Kimberle Crenshaw’s seminal work in 1958,
    which shed light on how different forms of oppression intersect and compound discrimination
    for marginalized groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, our second hypothesis is corroborated by observing that both encoding
    techniques result in higher equal opportunity violations compared to not encoding
    the protected attribute. This highlights the role of encoding in amplifying discrimination.
    However, there is a glimmer of hope: through the regularization of target encoding,
    fairness can be enhanced. This result aligns with our theoretical understanding,
    as attribute concatenation can worsen fairness by increasing both irreducible
    and reducible biases.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our research highlights the significant role of categorical attribute encoding
    in balancing model accuracy and fairness in machine learning. We have identified
    two types of biases, irreducible and reducible, that can arise from encoding categorical
    attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Through theoretical analysis and empirical experiments, we find that one-hot
    encoding exhibits more discrimination than target encoding. However, promising
    results are observed with regularized target encoding, which shows potential for
    improving fairness while maintaining acceptable model performance.
  prefs: []
  type: TYPE_NORMAL
- en: We emphasize the importance of considering the implications of encoding categorical
    protected attributes, as slight modifications in the encoding approach can lead
    to fairness improvements without significant sacrifices in predictive accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Between the lines**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In recent years we have seen algorithmic methods aiming to improve fairness
    in data-driven systems from many perspectives: data collection, pre-processing,
    in-processing, and post-processing steps. In this work, we have focused on how
    encoding categorical attributes (a common pre-processing step) can reconcile model
    quality and fairness.'
  prefs: []
  type: TYPE_NORMAL
- en: A common underpinning of much of the work in fair ML is the assumption that
    trade-offs between equity and accuracy may necessitate complex methods or difficult
    policy choices [[Rodolfa et al.](https://www.nature.com/articles/s42256-021-00396-x)]
  prefs: []
  type: TYPE_NORMAL
- en: Since target encoding with regularization is easy to perform and does not require
    significant changes to the machine learning models, it can be explored in the
    future as a suitable complementary for in-processing methods in fair machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: We encourage industry practitioners to consider the implications of encoding
    categorical protected attributes. With slight changes in the encoding approach,
    improvements in fairness can be achieved without significant detriment to predictive
    performance. However, it is essential to understand that using fair AI methods
    does not guarantee the fairness of complex socio-technical systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Limitations**: This work aims to show what are some of the implications of
    encoding categorical protected attributes at any moment. It should be understood
    as if, in any situation encoding categorical protected attributes won’t increase
    fairness metrics; we advocate considering the effects of encoding regularization
    along the fairness axis, too, not only on the predictive performance axis. Fair-AI
    methods do not necessarily guarantee the fairness of AI-based complex socio-technical
    systems.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Acknowledgments**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*This work has received funding from the European Union’s Horizon 2020 research
    and innovation program under Marie Sklodowska-Curie Actions (grant agreement number
    860630) for the project ‘’NoBIAS — Artificial Intelligence without Bias’’*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*This work reflects only the authors’ views, and the European Research Executive
    Agency (REA) is not responsible for any use that may be made of the information
    it contains.*'
  prefs: []
  type: TYPE_NORMAL
- en: Cite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
