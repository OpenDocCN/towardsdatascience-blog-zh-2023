- en: 'Anomaly Detection using Sigma Rules (Part 1): Leveraging Spark SQL Streaming'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Sigma 规则进行异常检测（第 1 部分）：利用 Spark SQL 流处理
- en: 原文：[https://towardsdatascience.com/anomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457?source=collection_archive---------9-----------------------#2023-01-24](https://towardsdatascience.com/anomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457?source=collection_archive---------9-----------------------#2023-01-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/anomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457?source=collection_archive---------9-----------------------#2023-01-24](https://towardsdatascience.com/anomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457?source=collection_archive---------9-----------------------#2023-01-24)
- en: Sigma rules are used to detect anomalies in cyber security logs. We use Spark
    structured streaming to evaluate Sigma rules at scale.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sigma 规则用于检测网络安全日志中的异常。我们使用 Spark 结构化流处理来大规模评估 Sigma 规则。
- en: '[](https://medium.com/@jean-claude.cote?source=post_page-----246900e95457--------------------------------)[![Jean-Claude
    Cote](../Images/aea2df9c7b95fc85cc336f64d64b0a76.png)](https://medium.com/@jean-claude.cote?source=post_page-----246900e95457--------------------------------)[](https://towardsdatascience.com/?source=post_page-----246900e95457--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----246900e95457--------------------------------)
    [Jean-Claude Cote](https://medium.com/@jean-claude.cote?source=post_page-----246900e95457--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jean-claude.cote?source=post_page-----246900e95457--------------------------------)[![Jean-Claude
    Cote](../Images/aea2df9c7b95fc85cc336f64d64b0a76.png)](https://medium.com/@jean-claude.cote?source=post_page-----246900e95457--------------------------------)[](https://towardsdatascience.com/?source=post_page-----246900e95457--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----246900e95457--------------------------------)
    [Jean-Claude Cote](https://medium.com/@jean-claude.cote?source=post_page-----246900e95457--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F444ed0089012&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457&user=Jean-Claude+Cote&userId=444ed0089012&source=post_page-444ed0089012----246900e95457---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----246900e95457--------------------------------)
    ·8 min read·Jan 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F246900e95457&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457&user=Jean-Claude+Cote&userId=444ed0089012&source=-----246900e95457---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F444ed0089012&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457&user=Jean-Claude+Cote&userId=444ed0089012&source=post_page-444ed0089012----246900e95457---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----246900e95457--------------------------------)
    ·8 min 阅读·2023年1月24日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F246900e95457&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457&user=Jean-Claude+Cote&userId=444ed0089012&source=-----246900e95457---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F246900e95457&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457&source=-----246900e95457---------------------bookmark_footer-----------)![](../Images/e01586182f002a7b3841a272a0a406cd.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F246900e95457&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fanomaly-detection-using-sigma-rules-part-1-leveraging-spark-sql-streaming-246900e95457&source=-----246900e95457---------------------bookmark_footer-----------)![](../Images/e01586182f002a7b3841a272a0a406cd.png)'
- en: Photo by Tom Carnegie on Unsplash, Supreme Court of Canada
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 摄影：Tom Carnegie，来源于 Unsplash，加拿大最高法院
- en: The Rise of Data Sketching
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据草图的兴起
- en: Data sketch is an umbrella term for data structures and algorithms that use
    theoretical mathematics, statistics and computer science to solve set cardinality,
    quantiles, frequency estimation, with mathematically proven error bounds.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据草图是一个总括性术语，涵盖了使用理论数学、统计学和计算机科学的各种数据结构和算法，以解决集合基数、分位数、频率估计等问题，并具有数学上证明的误差范围。
- en: 'Data sketches are orders-of magnitude faster than traditional approaches, they
    require less compute resources and sometimes are the only viable solution to big
    data problems. To find out more about data sketches checkout the [Apache Data
    Sketch project](https://datasketches.apache.org/docs/Background/SketchOrigins.html):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据草图比传统方法快几个数量级，它们需要更少的计算资源，有时是解决大数据问题的唯一可行解决方案。要了解更多关于数据草图的信息，请查看 [Apache Data
    Sketch 项目](https://datasketches.apache.org/docs/Background/SketchOrigins.html)。
- en: Sketches implement algorithms that can extract information from a
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 草图实现了可以从中提取信息的算法。
- en: stream of data in a single pass, which is also known as “one-touch” processing.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在单次传输的数据流中，这也被称为“单次触摸”处理。
- en: 'Spark leverages data sketches a lot, for example: [dimension reduction](https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html),
    [locality sensitive hashing](https://spark.apache.org/docs/latest/ml-features.html#locality-sensitive-hashing),
    [count min sketch](https://spark.apache.org/docs/latest/api/sql/#count_min_sketch).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 大量利用数据草图，例如：[维度缩减](https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html)、[局部敏感哈希](https://spark.apache.org/docs/latest/ml-features.html#locality-sensitive-hashing)、[计数最小草图](https://spark.apache.org/docs/latest/api/sql/#count_min_sketch)。
- en: In this series of articles we will walk you through the design of a highly performant
    fraud detection type system. Using real examples, we evaluate and contrast the
    performance of a traditional algorithm versus one based on data sketching.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一系列文章中，我们将带你深入了解高性能欺诈检测系统的设计。通过实际例子，我们评估并对比了传统算法与基于数据草图的算法的性能。
- en: What are Sigma Rules
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 Sigma 规则
- en: '[Sigma](https://github.com/SigmaHQ/sigma) is a generic signature format that
    allows you to make detections in log events. Rules are easy to write and applicable
    to any type of log. Best of all, Sigma rules are abstract and not tied to any
    particular SIEM, making Sigma rules shareable.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sigma](https://github.com/SigmaHQ/sigma) 是一种通用签名格式，允许你在日志事件中进行检测。规则易于编写，适用于任何类型的日志。最重要的是，Sigma
    规则是抽象的，不绑定于任何特定的 SIEM，使得 Sigma 规则可共享。'
- en: 'Once a cyber security researcher or analyst develops detection method, they
    can use Sigma to describe and share their technique with others. Here’s a quote
    from Sigma HQ:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络安全研究人员或分析师开发了检测方法，他们可以使用 Sigma 描述并与他人分享他们的技术。以下是来自 Sigma HQ 的一段话：
- en: Sigma is for log files what [Snort](https://www.snort.org/) is for network traffic
    and [YARA](https://github.com/VirusTotal/yara) is for files.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Sigma 对日志文件的作用就如同 [Snort](https://www.snort.org/) 对网络流量和 [YARA](https://github.com/VirusTotal/yara)
    对文件的作用。
- en: Let’s look at an [example](https://github.com/SigmaHQ/sigma/blob/master/rules/web/web_webshell_regeorg.yml)
    from Sigma HQ.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下来自 [Sigma HQ](https://github.com/SigmaHQ/sigma/blob/master/rules/web/web_webshell_regeorg.yml)
    的一个[示例](https://github.com/SigmaHQ/sigma/blob/master/rules/web/web_webshell_regeorg.yml)。
- en: '![](../Images/5ff877108907697beaaf7a34f11c1d3b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ff877108907697beaaf7a34f11c1d3b.png)'
- en: The heart of the rule is the detection section. When the condition evaluates
    to true it means we made a detection. A condition is composed of named expressions.
    For example, here a selection and a filter expression are declared. These expressions
    perform tests against attributes of the logs. In this case web logs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 规则的核心是检测部分。当条件评估为 true 时，意味着我们进行了检测。条件由命名表达式组成。例如，这里声明了选择和过滤表达式。这些表达式对日志的属性进行测试。在这种情况下是
    web 日志。
- en: Sigmac Generates SQL
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sigmac 生成 SQL
- en: The sigmac compiler is used to translate an abstract Sigma rule into a concrete
    form which can be evaluated by an actual SIEM or processing platform. Sigmac has
    many backends capable of translating a rule into QRadar, ElasticSearch, ArcSight,
    FortiSIEM and generic SQL.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmac 编译器用于将抽象的 Sigma 规则转换为具体的形式，以便实际的 SIEM 或处理平台进行评估。Sigmac 具有许多后端，能够将规则转换为
    QRadar、ElasticSearch、ArcSight、FortiSIEM 和通用 SQL。
- en: 'Using the SQL sigmac backend we can translate the rule above into:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SQL sigmac 后端，我们可以将上述规则转换为：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: These SQL statements are typically invoked by a scheduler on particular trigger
    interval; say 1 hour. Every hour the detection searches through the newest events.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 SQL 语句通常由调度器在特定触发间隔（例如 1 小时）下调用。每小时，检测系统会搜索最新的事件。
- en: '![](../Images/5929476215d8840014c7f5cbee9e00a9.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5929476215d8840014c7f5cbee9e00a9.png)'
- en: However, some Sigma rules apply temporal aggregations. For example [Enumeration
    via the Global Catalog](https://github.com/SigmaHQ/sigma/blob/7c36a33ea77e94cbb5ad58fa061a84ed74dd503a/rules/windows/builtin/security/win_global_catalog_enumeration.yml)
    counts occurrence of events in a window of time.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一些Sigma规则应用了时间聚合。例如，[通过全球目录进行枚举](https://github.com/SigmaHQ/sigma/blob/7c36a33ea77e94cbb5ad58fa061a84ed74dd503a/rules/windows/builtin/security/win_global_catalog_enumeration.yml)计算一段时间窗口内事件的发生次数。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the batch model above, these types of queries reprocess the same events
    over and over. Especially if the correlation window is large. Furthermore, if
    we try to reduce the detection latency by increasing the trigger rate say to every
    5 minutes, we introduce even more reprocessing of the same events.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述批处理模型，这些类型的查询会一遍又一遍地重新处理相同的事件。特别是当相关窗口很大时。此外，如果我们试图通过将触发频率提高到每`5分钟`来减少检测延迟，我们将引入更多对相同事件的重新处理。
- en: Ideally to reduce reprocessing the same events over and over we would like the
    anomaly detection to remember what was the last event it processed and what were
    the value of the counters so far. And that is exactly what Spark Structured Streaming
    framework provides. A streaming query triggers a micro-batch every minute (configurable).
    It reads in the new events, updates all counters and persists them (for disaster
    recovery).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，为了减少对相同事件的反复处理，我们希望异常检测能够记住上一个处理的事件是什么，以及迄今为止计数器的值。这正是Spark Structured
    Streaming框架所提供的功能。流式查询每分钟触发一次微批处理（可配置）。它读取新事件，更新所有计数器并将其持久化（用于灾难恢复）。
- en: '![](../Images/5364db4f9be8cdd1834da5eeb713c863.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5364db4f9be8cdd1834da5eeb713c863.png)'
- en: In this model, each event is evaluated once. Increasing the trigger rate does
    not incur the same cost as the stateless batch model. Also because events are
    evaluated only once, complex detections such as regex matching do not incur ballooning
    costs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模型中，每个事件只评估一次。提高触发频率不会像无状态批处理模型那样产生相同的成本。而且由于事件只评估一次，复杂的检测（如正则表达式匹配）不会产生膨胀的成本。
- en: Use Spark Streaming to run Detections
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark Streaming运行检测
- en: Spark Structured streaming can easily evaluate the SQL produced by the sigmac
    compiler. First we create a streaming dataframe by connecting to our favorite
    queuing mechanism (EventHubs, Kafka). In this example, we will `readStream` from
    an Iceberg table, where events are incrementally inserted into. Find out more
    about Iceberg’s streaming capabilities [here](https://iceberg.apache.org/docs/latest/spark-structured-streaming/).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Structured Streaming可以轻松评估sigmac编译器生成的SQL。首先，我们通过连接到我们最喜欢的队列机制（EventHubs，Kafka）来创建一个流数据框。在本例中，我们将从一个Iceberg表中`readStream`，该表中事件会被增量插入。了解有关Iceberg流能力的更多信息，[请点击这里](https://iceberg.apache.org/docs/latest/spark-structured-streaming/)。
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Notice that we aliased the streaming dataframe with the view name “events”.
    We do this in order to refer to this streaming dataframe in our SQL statement,
    i.e.: `select * from events`. All we have to do now is configure the sigmac compiler
    to produce SQL statements against an `events` table. For example a generated sql
    file might look like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将流数据框别名为视图名称“events”。这样做是为了在SQL语句中引用此流数据框，即：`select * from events`。我们现在要做的就是配置sigmac编译器，以便对`events`表生成SQL语句。例如，生成的sql文件可能如下所示：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In our analytic we load this generated SQL and ask Spark to create a `hitsDf`
    dataframe from it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中，我们加载生成的SQL并要求Spark从中创建一个`hitsDf`数据框。
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We start the streaming query by invoking `writeStream` and we configure the
    query to trigger a micro-batch every minute. This streaming query will run indefinitely
    writing detections to the sink of our choice. Here we simply write to the console
    sink but we could write to another Iceberg table. Or we could use `[forEachBatch](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch)`
    and execute some arbitrary python code, for example to push notifications into
    a REST endpoint. Or we can even do both.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过调用`writeStream`启动流查询，并配置查询以每分钟触发一次微批处理。此流查询将无限期运行，将检测结果写入我们选择的接收端。这里我们只是将结果写入控制台接收端，但我们也可以写入另一个Iceberg表。或者，我们可以使用`[forEachBatch](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch)`执行一些任意的python代码，例如，将通知推送到REST端点。或者我们甚至可以同时做这两件事。
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The Parent Process Challenge
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 父进程挑战
- en: So far we have seen how to detect anomalies in discrete events. However, Sigma
    rules can correlate an event with previous ones. A classic example of this is
    found in [Windows Security Logs (Event ID 4688)](https://www.ultimatewindowssecurity.com/securitylog/encyclopedia/event.aspx?eventID=4688).
    In this log source we find information about a process being created. A crucial
    piece of information in this log is the process that started this process. You
    can use these `Process ID` to determine what the program did while it ran etc.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use this Sigma rule as an example: [Rundll32 Execution Without DLL File](https://github.com/SigmaHQ/sigma/blob/master/rules/windows/process_creation/proc_creation_win_run_executable_invalid_extension.yml).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the raw telemetry an event only knows the parent `Process ID` . Yet the rule
    refers to the `ParentImage` and the `ParentCommandLine`. The rule basically assumes
    a join has already been performed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily Spark Structured Streaming supports [stream-stream joins](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins).
    In order to retrieve the ParentImage and ParentCommanLine, we perform a self-join
    of the process logs. We will join the `current` side with the `parent_of_interest`
    side. With a join condition like this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '`current.ParentProcessID = parent_of_interest.ProcessID`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Left side: Set flags for every detection rule'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use a convention `c` for current process and `r1` for rule 1.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: So `filter_empty` in [Rundll32 Execution Without DLL File](https://github.com/SigmaHQ/sigma/blob/master/rules/windows/process_creation/proc_creation_win_run_executable_invalid_extension.yml)
    (rule 1) is named `cr1_filter_empty`
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Right side: Filter messages on the parents_of_interest'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We do the same with conditions that apply to parent process. However, in this
    case we also filter the table. This means parent process that are filtered out
    necessarily had all their flags set to `FALSE`. By filtering we greatly reduce
    the amount of `parents_of_interest` keys to cache when performing a streaming
    join.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Join current with its parent
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are doing a left join with the parent side. Since the parent side is filtered
    it is possible we will not find the corresponding parent process id. When a parent
    is not found, the columns will have flags set to NULL. We use coalesce to assign
    these parent side flags a value of `FALSE`. `pr3_selection_atexec` is a parent
    flag, so we apply coalesce like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We also combine conditions that come from current and parent side. For example
    the `selection_atexec` condition is composed of a parent and child condition.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We thus combine them like this:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`r3_selection_atexec` is the final flag for `selection_atexec` in rule 3.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Finally we apply the Sigma rule condition
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For example rule1’s condition is:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We simply apply this condition and name the result `rule1`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here’s the condition in the complete statement.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note, the sigmac compiler does not produce this type of SQL. However, we plan
    on writing a custom sigma compiler to produce the above SQL statements.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，sigmac 编译器不会生成这种类型的 SQL。然而，我们计划编写一个自定义的 sigma 编译器以生成上述 SQL 语句。
- en: Running these SQL statements is no different then our original example.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这些 SQL 语句与我们最初的示例没有区别。
- en: Spark Structured Streaming can keep and persist state across micro-batches.
    In the documentation Spark refers to this as [Windowed Grouped Aggregation](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time).
    The same principal applies to stream-stream join. You configure Spark to cache
    aggregations or in this case the rows of the `parents_of_interests` inside a window.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Structured Streaming 可以在微批处理之间保持和持久化状态。在文档中，Spark 称之为[窗口分组聚合](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time)。相同的原则适用于流-流连接。你可以配置
    Spark 来缓存聚合，或者在这种情况下，缓存`parents_of_interests`的行在一个窗口中。
- en: However, how well does this scale? How many rows of `parents_of_interests` can
    we keep in Spark’s [state store](https://spark.apache.org/docs/3.0.2/api/scala/org/apache/spark/sql/streaming/GroupState.html)
    window?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方式的扩展性如何？我们可以在 Spark 的[状态存储](https://spark.apache.org/docs/3.0.2/api/scala/org/apache/spark/sql/streaming/GroupState.html)窗口中保留多少行`parents_of_interests`？
- en: In our next article we will answer these questions. In order not to miss it,
    follow us and subscribe to get these stories via email. Stay tuned!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们下一篇文章中，我们将回答这些问题。为了不遗漏，请关注我们并订阅邮件获取这些故事。敬请期待！
- en: All images unless otherwise noted are by the author
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图像均由作者提供。
