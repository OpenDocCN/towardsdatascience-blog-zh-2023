- en: Distributed data parallel and distributed model parallel in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/distributed-data-and-model-parallel-in-deep-learning-6dbb8d9c3540?source=collection_archive---------11-----------------------#2023-05-08](https://towardsdatascience.com/distributed-data-and-model-parallel-in-deep-learning-6dbb8d9c3540?source=collection_archive---------11-----------------------#2023-05-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how distributed data parallel and distributed model parallel works inside
    stochastic gradient descent to let you train gigantic models over huge datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jasonweiyi.medium.com/?source=post_page-----6dbb8d9c3540--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page-----6dbb8d9c3540--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6dbb8d9c3540--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6dbb8d9c3540--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page-----6dbb8d9c3540--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1b4bd5317a6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-data-and-model-parallel-in-deep-learning-6dbb8d9c3540&user=Wei+Yi&userId=1b4bd5317a6e&source=post_page-1b4bd5317a6e----6dbb8d9c3540---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6dbb8d9c3540--------------------------------)
    ·14 min read·May 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6dbb8d9c3540&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-data-and-model-parallel-in-deep-learning-6dbb8d9c3540&user=Wei+Yi&userId=1b4bd5317a6e&source=-----6dbb8d9c3540---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6dbb8d9c3540&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-data-and-model-parallel-in-deep-learning-6dbb8d9c3540&source=-----6dbb8d9c3540---------------------bookmark_footer-----------)![](../Images/ecced4c819e71328af444db89219dc4f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Olga Zhushman](https://unsplash.com/ja/@ori_photostory?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: You must have heard that recent successful models, such as ChatGPT, have trillions
    of parameters and trained with terabytes of data. In the meanwhile, you may have
    experienced that your deep learning model with tens of millions of parameters
    didn’t even fit in a GPU and it trained for days with only gigabytes of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wonder why other people could achieve so much in the same life time,
    and want to be them, please understand the two techniques that enable the training
    of large deep learning models with huge datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed data parallel** splits a mini-batch across GPUs. It lets you
    train faster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed model parallel** splits a model’s parameters, gradients and optimizer’s
    internal states across GPUs. It lets you load larger models in GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many API implementations of distributed data parallel and distributed
    model parallel, such as [DDP](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html),
    [FSDP](https://pytorch.org/docs/stable/fsdp.html) and [DeepSpeed](https://github.com/microsoft/DeepSpeed).
    They all have the common theme of splitting the training data or the model into
    parts…
  prefs: []
  type: TYPE_NORMAL
