- en: 'Paper Review: A Deep Dive into Imagen'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 论文综述：深度解析 Imagen
- en: 原文：[https://towardsdatascience.com/paper-review-a-deep-dive-into-imagen-4e5b4092af13?source=collection_archive---------13-----------------------#2023-02-01](https://towardsdatascience.com/paper-review-a-deep-dive-into-imagen-4e5b4092af13?source=collection_archive---------13-----------------------#2023-02-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/paper-review-a-deep-dive-into-imagen-4e5b4092af13?source=collection_archive---------13-----------------------#2023-02-01](https://towardsdatascience.com/paper-review-a-deep-dive-into-imagen-4e5b4092af13?source=collection_archive---------13-----------------------#2023-02-01)
- en: A critical analysis of Google’s impressive new text-to-image generation tool
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对谷歌令人印象深刻的新型文本到图像生成工具的批判性分析
- en: '[](https://j-w-mcgowan18.medium.com/?source=post_page-----4e5b4092af13--------------------------------)[![Jamie
    McGowan](../Images/1150476f58297eb7e45cd3942a7a072b.png)](https://j-w-mcgowan18.medium.com/?source=post_page-----4e5b4092af13--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e5b4092af13--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e5b4092af13--------------------------------)
    [Jamie McGowan](https://j-w-mcgowan18.medium.com/?source=post_page-----4e5b4092af13--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://j-w-mcgowan18.medium.com/?source=post_page-----4e5b4092af13--------------------------------)[![Jamie
    McGowan](../Images/1150476f58297eb7e45cd3942a7a072b.png)](https://j-w-mcgowan18.medium.com/?source=post_page-----4e5b4092af13--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4e5b4092af13--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4e5b4092af13--------------------------------)
    [Jamie McGowan](https://j-w-mcgowan18.medium.com/?source=post_page-----4e5b4092af13--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F685229ed4b15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaper-review-a-deep-dive-into-imagen-4e5b4092af13&user=Jamie+McGowan&userId=685229ed4b15&source=post_page-685229ed4b15----4e5b4092af13---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e5b4092af13--------------------------------)
    ·13 min read·Feb 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e5b4092af13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaper-review-a-deep-dive-into-imagen-4e5b4092af13&user=Jamie+McGowan&userId=685229ed4b15&source=-----4e5b4092af13---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F685229ed4b15&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaper-review-a-deep-dive-into-imagen-4e5b4092af13&user=Jamie+McGowan&userId=685229ed4b15&source=post_page-685229ed4b15----4e5b4092af13---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4e5b4092af13--------------------------------)
    · 13 分钟阅读 · 2023年2月1日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e5b4092af13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaper-review-a-deep-dive-into-imagen-4e5b4092af13&user=Jamie+McGowan&userId=685229ed4b15&source=-----4e5b4092af13---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e5b4092af13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaper-review-a-deep-dive-into-imagen-4e5b4092af13&source=-----4e5b4092af13---------------------bookmark_footer-----------)![](../Images/a4efdeb4a3d2eadd14364033d7f601fc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e5b4092af13&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpaper-review-a-deep-dive-into-imagen-4e5b4092af13&source=-----4e5b4092af13---------------------bookmark_footer-----------)![](../Images/a4efdeb4a3d2eadd14364033d7f601fc.png)'
- en: Photo by [Amanda Dalbjörn](https://unsplash.com/@amandadalbjorn?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Amanda Dalbjörn](https://unsplash.com/@amandadalbjorn?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Text-to-image synthesis is a research direction within the field of **multimodal
    learning** which has been the subject of many recent advancements [1–4]. This
    review will focus on the article, *‘Photorealistic Text-to-Image Diffusion Models
    with Deep Language Understanding’* [1].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到图像合成是**多模态学习**领域的一个研究方向，最近在这一领域取得了许多进展[1–4]。这篇综述将重点讨论文章*《具有深度语言理解的逼真文本到图像扩散模型》*[1]。
- en: Here the authors attempt to achieve state-of-the-art **photorealism** and provide
    insights into a deeper level of language understanding within text-to-image synthesis.
    The main output of this paper is a model named ‘Imagen’ which improves upon previous
    text-to-image synthesis models in the literature [2–4].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，作者试图实现最先进的**逼真度**，并提供对文本到图像合成中更深层次的语言理解的见解。本文的主要输出是一个名为‘Imagen’的模型，它改进了文献中以前的文本到图像合成模型[2–4]。
- en: You can see and find out more about Imagen [**here**](https://imagen.research.google/)!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[**这里**](https://imagen.research.google/)查看更多关于Imagen的信息！
- en: What is a diffusion model?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是扩散模型？
- en: As the title of the paper suggests, Imagen is a diffusion model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如论文标题所示，Imagen是一个扩散模型。
- en: Very briefly, diffusion models are an example of **generative AI** based upon
    taking an input ***x***⁰ and gradually adding Gaussian noise at each layer *t*
    until a pure noise representation is reached ***x****ᵀ*, where *T* is the final
    layer.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，扩散模型是基于**生成式 AI**的一个例子，其过程是从输入***x***⁰开始，在每一层*t*逐渐加入高斯噪声，直到达到纯噪声表示***x****ᵀ*，其中*T*是最终层。
- en: This is inspired by non-equilibrium **thermodynamics**, whereby states evolve
    by diffusion to be **homogeneous** given a long enough time frame.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这受非平衡**热力学**的启发，其中状态通过扩散演变为**均匀**，只要时间足够长。
- en: '![](../Images/c28bebf58ef6caebc4fae871ba146931.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c28bebf58ef6caebc4fae871ba146931.png)'
- en: Diagram displaying the forward and background diffusion processes. Image from
    [16].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 显示前向和背景扩散过程的图示。图片来自[16]。
- en: Diffusion models learn to reverse this process in an attempt to **generate**
    the original ***x***⁰ from ***x****ᵀ* (where in this case ***x***⁰ is an image).
    See the figure above for a visual aid in this.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型通过学习逆转这一过程，试图**生成**原始的***x***⁰（在这种情况下，***x***⁰是一张图像）。请参见上面的图示以获得视觉帮助。
- en: 'The goal of the model is to parameterise the **conditional probability** describing
    the reverse diffusion process at each step *t*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的目标是对描述每一步*t*反向扩散过程的**条件概率**进行参数化：
- en: '![](../Images/b996361cae90ae61f75d41f00f5dc8e3.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b996361cae90ae61f75d41f00f5dc8e3.png)'
- en: Equation describing the reverse diffusion process. Image created by author.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 描述反向扩散过程的方程。图片由作者创建。
- en: where the representation of ***x***ᵗ⁻¹ (the previous time step) is drawn from
    a **Gaussian** distribution characterised by the mean μ and covariance σ with
    model weights θ.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其中***x***ᵗ⁻¹（上一个时间步骤）的表示从**高斯**分布中绘制，特征是均值μ和协方差σ，模型权重为θ。
- en: Due to the diffusion process preserving the image at each step in the denoising
    process, this results in a more **intimate** connection between the data and prediction
    compared with other non-diffusion based text-to-image generators [4–7]. The upshot
    of this is, generally, a more **photorealistic** output from diffusion based models
    [1–3].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于扩散过程在每一步的去噪过程中保持了图像，这导致数据与预测之间的**亲密**连接，相比其他非扩散基于文本到图像生成器[4–7]。结果通常是，扩散模型[1–3]的输出更**逼真**。
- en: Once a base diffusion model is used to construct a 64 × 64 pixel image, Imagen
    then makes use of two further **super resolution** diffusion models to perform
    the upsampling 64 × 64 → 256 × 256 → 1024 × 1024\. The eventual result is therefore
    a high resolution 1024 × 1024 pixel image such as the one below!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用基础扩散模型构建一个64 × 64像素的图像，Imagen会利用另外两个**超分辨率**扩散模型进行上采样64 × 64 → 256 × 256
    → 1024 × 1024。最终结果因此是一个高分辨率的1024 × 1024像素图像，如下图所示！
- en: Note that this image is actually from DALL-E 2 [2] as Google has some restrictions
    on Imagen! The idea is the same but please make sure you check out the Imagen
    paper for the actual images.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这张图片实际上来自DALL-E 2 [2]，因为谷歌对Imagen有一些限制！概念是一样的，但请确保查看Imagen论文以获取实际的图片。
- en: '![](../Images/7ccda7e4fea77359cfad8f989ea29437.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ccda7e4fea77359cfad8f989ea29437.png)'
- en: An example output from DALL-E 2 with the text prompt “A teddy bear on a skateboard
    in times square”. Image from [2].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E 2的一个示例输出，文本提示为“时代广场上的滑板上的泰迪熊”。图片来自[2]。
- en: This review will provide a brief outline of previous work, I will then compile
    together the main contributions and results presented by the authors, and I will
    discuss these contributions and provide my personal **opinions** on the work.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述将简要概述以前的工作，然后汇总作者提出的主要贡献和结果，并讨论这些贡献，提供我个人对该工作的**意见**。
- en: Previous Work
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 之前的工作
- en: It has been possible to realise images from text for a number of years, however
    early work **struggled** to combine multiple textual concepts realistically into
    an image [5–7].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然实现文本生成图像已经有若干年，但早期的工作**困难**地将多个文本概念现实地结合到一幅图像中 [5–7]。
- en: Based on these shortcomings, OpenAI released DALL-E in [4] which is able to
    combine multiple seemingly **unrelated concepts** into a single image row by row
    — given a text prompt and the start (first row of pixels) of an image.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些不足，OpenAI 在 [4] 中发布了 DALL-E，它能够将多个看似**不相关的概念**逐行组合成一幅图像——给定文本提示和图像的起始（第一行像素）。
- en: Less than 12 months later, OpenAI then reformulated their approach to text-to-image
    synthesis with **diffusion models** via GLIDE [3]. The authors showed that GLIDE
    was preferred by human evaluators for **photorealism** and **caption similarity**
    in a variety of settings, thereby establishing the dominance of diffusion models
    in text-to-image generation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 不到12个月后，OpenAI 通过 GLIDE [3] 重新制定了他们的文本到图像合成方法。作者展示了 GLIDE 在各种设置中因**照片真实感**和**标题相似性**而被人类评估者偏好，从而确立了扩散模型在文本到图像生成中的主导地位。
- en: Finally, in [2], DALL-E 2 further improves upon GLIDE by generating images with
    an encoding based on the **image embedding** found from the textual prompt.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 [2] 中，DALL-E 2 通过基于从文本提示中找到的**图像嵌入**生成图像，从而进一步改进了 GLIDE。
- en: Note that other advancements were also made in this time frame, however I have
    focussed primarily on **three main contributions** which form the foundation for
    Imagen [1].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在这个时间段内还有其他进展，但我主要集中在**三大主要贡献**上，这些贡献构成了 Imagen [1] 的基础。
- en: Main Contributions
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要贡献
- en: '**Architecture**'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**架构**'
- en: Similar to GLIDE [3] and DALL-E 2 [2], Imagen is a diffusion model which is
    seemingly very close in its **architecture** to GLIDE (i.e. it takes a text embedding
    as input and generates images from noise). However, a key difference in Imagen
    is that the **text embeddings** are found from large *off-the-shelf* language
    models (LMs).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 GLIDE [3] 和 DALL-E 2 [2]，Imagen 是一个扩散模型，在其**架构**上似乎与 GLIDE 非常接近（即，它将文本嵌入作为输入，并从噪声中生成图像）。然而，Imagen
    的一个关键区别是，**文本嵌入**来自大型*现成的*语言模型（LMs）。
- en: One of the main findings of [1] is that incorporating **large frozen LM’s**
    which are trained on text-only data, proves extremely useful in obtaining text
    representations for text-to-image synthesis.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]的主要发现之一是，结合在仅文本数据上训练的**大型冻结语言模型**，在获取文本到图像合成的文本表示时非常有用。'
- en: Further to this, the authors explore the scaling of the text encoder and find
    that scaling the size of the LMs **improves results significantly** more than
    scaling the size of the diffusion model. The leftmost plot in Figure 4a in [1]
    summarises this result by displaying that the T5-XXL LM [8] achieves a **higher
    quality** images (↓ FID score) and better caption compatibility (↑ CLIP score).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者探讨了文本编码器的扩展，发现扩展语言模型的规模**显著提高**了结果，远超扩展扩散模型的规模。[1]中图4a的最左侧图表总结了这一结果，显示T5-XXL
    LM [8]生成了**更高质量**的图像（↓ FID分数）和更好的标题兼容性（↑ CLIP分数）。
- en: The authors also incorporate a new technique to avoid saturated pixels in image
    generation with classifier-free guidance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还引入了一种新技术，以避免在无分类器引导的图像生成中出现饱和像素。
- en: '*Classifier guidance* was introduced to improve the quality of generated images
    via a pre-trained model which pushes the output at test time to be more **faithful**
    to the text input [9].'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*分类器引导* 被引入以通过预训练模型提高生成图像的质量，该模型在测试时推动输出更**忠实**于文本输入 [9]。'
- en: '*Classifier-free guidance* [10] avoids this need for a pre-trained model by
    generating two samples (outputs) from the input noise, with and without text conditioning.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*无分类器引导* [10] 通过从输入噪声生成两个样本（输出），一个有文本条件，一个没有，避免了对预训练模型的需求。'
- en: By finding the **difference** between these two samples in the feature space,
    it is possible to find the effect of the text in the image generation. Scaling
    this textual effect, the image generation can be guided towards better **image-text
    alignment** (with a varying strength of guidance weight *w*).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在特征空间中找到这两个样本之间的**差异**，可以找出文本在图像生成中的效果。通过调整这种文本效应，图像生成可以引导至更好的**图像-文本对齐**（通过不同强度的引导权重
    *w*）。
- en: So far none of this is new, however one issue with this guidance is that when
    *w* is large, pixels can become saturated and **image fidelity** is damaged at
    the expense of better image-text alignment. Therefore the authors introduce *dynamic
    thresholding* whereby saturated pixels are pushed inwards from [-1, 1] by varying
    amounts, determined at each sampling step ***x***ᵗ (hence being dynamic). The
    authors claim significant enhancements in **photorealism** and **image-text alignment**
    for high guidance in image generation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这些都不算新鲜，然而这种指导的一个问题是，当*w*很大时，像素可能会饱和，**图像保真度**会因为更好的图像-文本对齐而受到损害。因此，作者引入了*动态阈值*，通过在每次采样步骤***x***ᵗ时确定的不同量将饱和像素从[-1,
    1]推向内部（因此是动态的）。作者声称，在图像生成中，高度指导下**逼真度**和**图像-文本对齐**有显著提升。
- en: Finally on the side of the model architecture, the authors propose a new variant
    of U-Net [11] which is **simpler** and more **efficient** than previous iterations.
    From what I can tell, the key modification is the removal of self-attention layers
    in the super-resolution models, from the U-Net models from [11–12].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在模型架构方面，作者提出了一种新的U-Net [11]变体，该变体比以前的版本**更简单**且**更高效**。根据我的了解，关键修改是去除了超分辨率模型中的自注意力层，这些模型来自于[11–12]的U-Net模型。
- en: '**DrawBench**'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**DrawBench**'
- en: Another important contribution to future research in text-to-image synthesis
    is the release of **DrawBench**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对未来文本到图像合成研究的另一个重要贡献是**DrawBench**的发布。
- en: DrawBench is a collection of ‘challenging’ evaluation benchmark text prompts
    that probe the models capacity to handle complex concepts such as compositionality,
    cardinality and spatial relations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: DrawBench是一个‘挑战性’评估基准文本提示的集合，探测模型处理复杂概念如组合性、基数和空间关系的能力。
- en: The idea behind this release is to provide an evaluation benchmark that includes
    some very **strange text prompts** to ensure that the image has never existed
    before. Therefore in theory this should push models to the limits of their imagination
    and **capabilities** to generate complex images.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本次发布的理念是提供一个包含一些非常**奇怪的文本提示**的评估基准，以确保图像之前从未存在过。因此，理论上，这应该将模型推向其想象力和**能力**的极限，以生成复杂图像。
- en: Quantitative Results
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定量结果
- en: '![](../Images/abf644d2a1fc4b2221648f4ee9900282.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abf644d2a1fc4b2221648f4ee9900282.png)'
- en: Photo by [Maxim Hopman](https://unsplash.com/@nampoh?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Maxim Hopman](https://unsplash.com/@nampoh?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: The quantitative results presented by the authors in [1] compare and contrast
    different models on COCO [15] and DrawBench text prompts.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在[1]中展示的定量结果对比了不同模型在COCO [15]和DrawBench文本提示上的表现。
- en: The authors find that human evaluation results on DrawBench show a strong preference
    for Imagen when analysing pairwise comparisons with DALL-E 2 [2], GLIDE [3], Latent
    Diffusion [14] and CLIP-guided VQ-GAN [13] models (Figure 3 in [1]). These results
    are provided as a measure of **caption alignment** and **fidelity**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作者发现，与DALL-E 2 [2]、GLIDE [3]、Latent Diffusion [14]和CLIP-guided VQ-GAN [13]模型进行成对比较时，DrawBench上的人工评估结果对Imagen表现出强烈的偏好（见[1]中的图3）。这些结果作为**标题对齐**和**保真度**的衡量标准。
- en: Meanwhile, the results on the COCO validation set seem to not show as much of
    a difference between different models — which is potentially why the authors do
    not dwell on these for too long.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，COCO验证集上的结果似乎在不同模型之间没有表现出太大差异——这可能是作者没有对此过多停留的原因。
- en: However, an interesting observation on the COCO dataset is that Imagen has a
    **limited capability** to generate photorealistic people — although the authors
    do not provide any qualitative example of how bad Imagen is at generating people.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对COCO数据集的一个有趣观察是，Imagen在生成逼真的人物方面有**有限的能力**——尽管作者没有提供任何关于Imagen生成人物效果差的定性例子。
- en: Discussion
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: 'In the introduction, the authors of [1] include the claim:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在引言中，[1]的作者提到以下声明：
- en: '[Imagen delivers] an unprecedented degree of photorealism and a deep level
    of language understanding in text-to-image synthesis.'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[Imagen交付]了前所未有的逼真度和深层次的语言理解能力，在文本到图像合成中。'
- en: Investigating the first half of this claim, the authors present several **qualitative
    comparisons** between Imagen and DALL-E 2 generated images. They also provide
    results from human evaluation experiments where people were asked to choose the
    most **photorealistic** image from a single text prompt or caption.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在调查这一声明的前半部分时，作者展示了Imagen与DALL-E 2生成图像之间的几种**定性比较**。他们还提供了人类评估实验的结果，询问人们从单个文本提示或标题中选择最**写实**的图像。
- en: Even before considering any results, immediately the authors have introduced
    a degree of **subjectivity** into their analysis that is inherent in human evaluation
    experiments. Therefore the results shown in [1] must be considered with care and
    a healthy level of **skepticism**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在考虑任何结果之前，作者立即在他们的分析中引入了人类评估实验中固有的**主观性**。因此，文献[1]中显示的结果必须谨慎考虑，并持有健康的**怀疑态度**。
- en: '![](../Images/3c5369c360b2143fd602d4c91b3c33d3.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c5369c360b2143fd602d4c91b3c33d3.png)'
- en: An example output from DALL-E 2 with the text prompt “A high quality photo of
    a dog playing in a green field next to a lake”. Image from [2].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个来自DALL-E 2的示例输出，文本提示为“狗在湖边的绿色田野里玩耍的高质量照片”。图像来自[2]。
- en: To provide some context to these results, the authors select some example comparisons
    shown to human raters and include these in the Appendix (definitely take a look
    at these — for motivation, I’ve added an example from DALL-E 2 above).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供这些结果的背景，作者选择了一些示例比较展示给人类评分者，并将其包含在附录中（一定要查看这些示例——作为动机，我在上面添加了一个来自DALL-E
    2的示例）。
- en: However, even with these examples, I find it difficult to make a clear judgement
    over which image should be preferred. Considering the copied examples shown in
    the figure above, personally I believe that some of DALL-E 2’s generated images
    are **more photorealistic** than Imagen’s, which demonstrates the issues of subjectivity
    when collecting results such as these.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使有这些例子，我发现很难对哪张图像更具优先性做出明确判断。考虑到上图中显示的复制例子，我个人认为一些DALL-E 2生成的图像比Imagen的图像**更具照片写实性**，这展示了在收集这类结果时主观性的问题。
- en: The authors choose to ask human raters *‘which image is more photorealistic?’*
    and whether each *‘caption accurately describes the image’* during the evaluation
    process. However the **discontinuous** nature of assessing these metrics is rather
    worrying to me.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 作者选择询问人类评分者*“哪张图像更具照片写实性？”*以及每个*“标题是否准确描述了图像？”*在评估过程中。然而，评估这些指标的**不连续**性质让我感到相当担忧。
- en: For example, if we have two cartoon images in a batch (which are presumably
    not very realistic) and a rater is asked to choose one. As far as the photorealism
    metric is concerned, the chosen image will have the same level of realism as a
    much **more realistic** image (i.e. not a cartoon) chosen from a separate batch.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有两张卡通图像（这些图像可能不太现实），并且评分者被要求选择其中一张。就照片写实性指标而言，所选择的图像将具有与从另一批次中选择的更**真实**的图像相同的现实水平（即不是卡通图像）。
- en: Clearly there is some interplay between the caption for a batch of images and
    the **level of photorealism** that can be achieved. Therefore it would be interesting
    to explore weighting certain text prompts based on difficulty, in an attempt to
    create a more **continuous** metric which can be **aggregated** more reliably.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，图像批次的标题与可以实现的**照片写实性**之间存在一定的相互作用。因此，探索根据难度加权某些文本提示，将会有趣，以尝试创建一个**连续的**度量标准，该标准可以更可靠地**汇总**。
- en: Similarly in the case of caption alignment, the raters choose between three
    **categorical** options whether the caption is aligned with the generated image
    (*yes*, *somewhat* and *no*). These experimental results attempt to back up the
    second half of the quote above (claiming *a deep level of language understanding*).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在标题对齐的情况下，评分者在三个**类别**选项中选择标题是否与生成的图像对齐（*是*、*有些*和*否*）。这些实验结果试图支持上述引言的后半部分（声称*深层次的语言理解*）。
- en: It is true that for caption alignment, one can claim there is a more **definitive**
    answer as to whether the relationships and concepts in the text prompts have been
    **captured** in the image generation (i.e. less subjectivity than for photorealism).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，对于标题对齐，可以说在文本提示中的关系和概念是否在图像生成中被**捕捉**，有一个更**明确**的答案（即比起照片写实性主观性更低）。
- en: However, I would argue once again that a more **continuous** metric should be
    used here, such as a 1–10 rating of alignment. Following from the discussion above,
    presumably the varying levels of difficulty across all captions would also manifest
    in **lower caption alignment**. Potentially asking raters to rate the difficulty
    of the caption or text prompt during evaluation would be interesting to explore
    and help **standardise** the dataset and metrics.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我再次认为应该使用更**连续**的度量标准，例如1-10的对齐评分。根据上述讨论，所有字幕的不同难度等级也会表现为**较低的字幕对齐度**。有趣的是，探讨让评估者在评价过程中评估字幕或文本提示的难度，并帮助**标准化**数据集和度量标准。
- en: '![](../Images/baf9f50b7a2532112fa7580bcf8f56d9.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/baf9f50b7a2532112fa7580bcf8f56d9.png)'
- en: Photo by [Mitchell Luo](https://unsplash.com/@mitchel3uo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Mitchell Luo](https://unsplash.com/@mitchel3uo?utm_source=medium&utm_medium=referral)于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: As this line of research develops and generated images become even more impressive
    and **creative**, this method of evaluation will naturally become less reliable
    (of course this is a good problem to have). Therefore it would have been great
    to see the authors discuss the potential for asking raters more specific questions
    to assess the levels of creativity, compositionality, cardinality and spatial
    relations captured by models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这一研究领域的发展和生成图像变得越来越令人印象深刻和**富有创意**，这种评估方法自然会变得不那么可靠（当然，这是一个好问题）。因此，希望看到作者讨论询问评估者更多具体问题的潜力，以评估模型所捕捉的创造力、构图性、数量关系和空间关系。
- en: In the event that two generated images are equally as impressive, asking the
    rater more **specific** questions could help distinguish the model performance
    at this very high level.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果生成的两张图像同样令人印象深刻，向评估者提出更多**具体**的问题可以帮助区分模型在这一非常高的水平上的表现。
- en: As an example, one of the applications for text-to-image generation is to aid
    in **generating** **illustrations.** Therefore there is surely some justification
    to assess the level of creativity and variation when interpreting a text prompt.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，文本到图像生成的一个应用是帮助**生成****插图**。因此，评估解释文本提示时的创造力和变化性是有充分理由的。
- en: In the examples shown earlier, DALL-E 2 [2] interprets *‘glasses’* in more ways
    than Imagen, hence one could argue that DALL-E 2 is the **more creative** model?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前展示的例子中，DALL-E 2 [2] 在解释*‘眼镜’*时比Imagen有更多的方式，因此可以认为DALL-E 2是**更具创意**的模型？
- en: When viewing the results in this way, a **major critique** of the paper would
    be that the chosen metrics play to Imagen’s strengths too much. The best indication
    (metric) of a well-performing model in different applications will presumably
    be different **depending** on the application (i.e. there is *no free lunch!*).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，论文的一个**主要批评**是所选择的度量标准过于侧重于Imagen的优势。不同应用中表现良好的模型的最佳指示（度量）将**依赖于**具体应用（即*没有免费的午餐!*）。
- en: Because of this, I would be interested to hear the authors thoughts on how to
    **rigorously evaluate** these models for more than just faithfulness and caption-alignment.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我很想听听作者对如何**严格评估**这些模型的不仅仅是忠实度和字幕对齐度的看法。
- en: '![](../Images/25df3c4e938441b1d059c157a55c9f01.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25df3c4e938441b1d059c157a55c9f01.png)'
- en: Photo by [Dragos Gontariu](https://unsplash.com/@dragos126?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Dragos Gontariu](https://unsplash.com/@dragos126?utm_source=medium&utm_medium=referral)于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The release of DrawBench is justified in [1] to be a necessary contribution
    to the text-to-image research field due to providing a *comprehensive* set of
    challenging text prompt scenarios.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[1]，DrawBench的发布被认为是对文本到图像研究领域的必要贡献，因为它提供了一套*全面*的挑战性文本提示场景。
- en: While I agree with most of this, based on the discussion surrounding this argument,
    I am yet to be convinced that this is a *comprehensive* benchmark. If one explores
    DrawBench a little **deeper**, only around 200 text prompts/captions across 11
    categories are included, which seems quite small at first glance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我同意大部分观点，但根据围绕这一论点的讨论，我尚未被说服这是一项*全面*的基准。如果深入探索DrawBench，会发现仅包括约200个文本提示/字幕，分布在11个类别中，这在初看时显得相当少。
- en: This concern is only deepened when **comparing** to the COCO dataset [15], which
    includes 330K images with 5 captions per image across a much wider variety of
    categories. Personally, I think it would be good for the authors to discuss their
    **reasoning** as to why they claim this is a *comprehensive* set.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这种担忧在与 COCO 数据集 [15] 比较时愈发加深，该数据集包括 330K 张图像，每张图像有 5 个标题，涵盖了更广泛的类别。个人认为，作者应该讨论他们声称这是一个*全面*集的**推理**。
- en: Further to this, with the rapid advancement in text-to-image synthesis, I would
    argue that DrawBench is a **moving** **target** in the field. Therefore it would
    be nice to see the possibility of adapting or adding to these captions being discussed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着文本到图像合成的快速进展，我认为 DrawBench 是该领域的**动态** **目标**。因此，希望能够看到讨论调整或添加这些标题的可能性。
- en: Also since DrawBench is presented with Imagen, there is scope for one to hold
    some **apprehension** as to whether there was any selectiveness in choosing the
    200 prompts in order to gain preferential results on Imagen.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 DrawBench 是与 Imagen 一起呈现的，因此有理由对选择 200 个提示以获得对 Imagen 有利的结果是否存在某种**担忧**。
- en: Once again, comparing the difference in results between Imagen and the baseline
    models when evaluated on COCO [15] and DrawBench, the results for COCO seem **much
    closer between models** than those for DrawBench (where Imagen is seemingly far
    above all baseline models).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 再次比较 Imagen 和基准模型在 COCO [15] 和 DrawBench 上评估的结果，COCO 的结果似乎**在模型之间更接近**，而 DrawBench
    的结果则显示 Imagen 似乎远高于所有基准模型。
- en: This could be due to DrawBench being a **naturally** harder set of prompts which
    Imagen is able to handle due to its pre-trained LM, or it could be that DrawBench
    is bias towards Imagen’s strengths? Indeed, the authors admit to some bias when
    constructing DrawBench by **not including** any people within the image generation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是因为 DrawBench 是一个**自然**难度较大的提示集，而 Imagen 能够处理这些提示是由于其预训练的 LM，或者 DrawBench
    可能对 Imagen 的优势存在偏见？确实，作者在构建 DrawBench 时承认存在一些偏见，**未包含**任何图像生成中的人物。
- en: Finally, it is easy to critique research when the model (or code) is not released,
    especially when there is the overwhelming potential for financial gain (which
    the authors do not mention).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当模型（或代码）未发布时，批评研究很容易，尤其是当存在巨大的财务获利潜力时（作者并未提及这一点）。
- en: However, I believe the social and ethical reasoning behind this is one of the
    **best** **contributions** from the paper, and one which highlights the need for
    some sort of **governance** when releasing powerful open-source AI software.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我认为其中的社会和伦理推理是该论文的**最佳** **贡献**之一，并且突显了在发布强大的开源 AI 软件时需要某种形式的**治理**。
- en: '![](../Images/ec3b55cce216cf52c3f1c0b907b58450.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec3b55cce216cf52c3f1c0b907b58450.png)'
- en: Photo by [Михаил Секацкий](https://unsplash.com/@sekatsky?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[Михаил Секацкий](https://unsplash.com/@sekatsky?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 上的照片'
- en: In a broader sense, generative models naturally hold a mirror up to society
    which may be **beneficial** for social research groups or even governments, if
    they are given access to **unfiltered** versions of models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从更广泛的意义上讲，生成模型自然地反映了社会，这对社会研究小组甚至政府可能是**有益的**，前提是他们能够访问**未经筛选**的模型版本。
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: To summarise, the authors have made **significant** contributions to the rapidly
    growing successes within text-to-image synthesis.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，作者在文本到图像合成领域的快速增长中做出了**重大**贡献。
- en: While not currently available to the public (for social and ethical reasons),
    the resulting model ‘Imagen’ incorporates novel techniques such as using *off-the-shelf*
    text encoders, dynamical thresholding and more efficient U-Net architectures for
    base and super resolution layers.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然出于社会和伦理原因目前不对公众开放，但最终的模型‘Imagen’结合了诸如使用*现成*文本编码器、动态阈值设定和更高效的 U-Net 架构用于基础和超分辨率层等新技术。
- en: Personally I enjoyed reading this paper and I believe the contributions made
    are exciting and interesting developments in the field.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人很享受阅读这篇论文，我认为所做的贡献是令人兴奋和有趣的领域发展。
- en: '![](../Images/b93de39c2db6c1efe68cb71c58de104d.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b93de39c2db6c1efe68cb71c58de104d.png)'
- en: Photo by [Arnold Francisca](https://unsplash.com/@clark_fransa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Arnold Francisca](https://unsplash.com/@clark_fransa?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: However while the results are impressive, when digging deeper, it is apparent
    to me that the authors tend to **oversell** Imagen and DrawBench. Therefore, it
    will be interesting to observe (maybe in a future publication or from a select
    contingent of researchers allowed access to Imagen) a more **extensive** evaluation
    of text-to-image generation models.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管结果令人印象深刻，但深入研究后我发现，作者往往**过度宣传** Imagen 和 DrawBench。因此，观察（也许在未来的出版物中，或从获得
    Imagen 访问权限的研究人员中）更**广泛**的文本到图像生成模型评估将会很有趣。
- en: References
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] — Chitwan Saharia, et. al. Photorealistic text-to-image'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] — Chitwan Saharia 等人，照片级文本到图像生成'
- en: diffusion models with deep language understanding, arXiv:2205.11487, (2022).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 深度语言理解的扩散模型，arXiv:2205.11487，（2022）。
- en: '[2] — Aditya Ramesh, et. al. Hierarchical text-conditional image generation
    with CLIP latents, arXiv:2204.06125, (2022).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] — Aditya Ramesh 等人，基于 CLIP 潜变量的层次化文本条件图像生成，arXiv:2204.06125，（2022）。'
- en: '[3] — Alex Nichol, et. al. Glide: Towards photorealistic image generation and
    editing with text-guided diffusion models, arXiv:2112.10741, (2021).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] — Alex Nichol 等人，Glide：朝向照片级图像生成与编辑的文本引导扩散模型，arXiv:2112.10741，（2021）。'
- en: '[4] — Aditya Ramesh, et. al. Zero-shot text-to-image generation, ICML, 8821
    — 8831, PMLR, (2021).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] — Aditya Ramesh 等人，零样本文本到图像生成，ICML，8821 — 8831，PMLR，（2021）。'
- en: '[5] — Han Zhang, et. al. Stackgan++: Realistic image synthesis with stacked
    generative adversarial networks, IEEE transactions on pattern analysis and machine
    intelligence, 41(8):1947–1962, (2018).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] — Han Zhang 等人，Stackgan++：利用堆叠生成对抗网络进行逼真图像合成，《IEEE 计算机视觉与模式识别学报》，41(8):1947–1962，（2018）。'
- en: '[6] — Tero Karras, et. al. Analyzing and improving the image quality of stylegan,
    In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    8110 — 8119, (2020).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] — Tero Karras 等人，分析与改善 stylegan 图像质量，发表于《IEEE/CVF 计算机视觉与模式识别会议论文集》，8110
    — 8119，（2020）。'
- en: '[7] Mark Chen, et. al. Generative pretraining from pixels, ICML, 1691 — 1703,
    PMLR, (2020).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] — Mark Chen 等人，基于像素的生成预训练，ICML，1691 — 1703，PMLR，（2020）。'
- en: '[8] — Colin Raffel, et. al. Exploring the limits of transfer learning with
    a unified text-to-text transformer, arXiv:1910.10683, (2019).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] — Colin Raffel 等人，探索统一文本到文本转换器的迁移学习极限，arXiv:1910.10683，（2019）。'
- en: '[9] — Prafulla Dhariwal and Alexander Nichol, Diffusion models beat GANs on
    image synthesis, NeurIPS, 34, (2021).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] — Prafulla Dhariwal 和 Alexander Nichol，扩散模型在图像合成中的表现优于 GANs，NeurIPS，34，（2021）。'
- en: '[10] — Jonathan Ho and Tim Salimans, Classifier-free diffusion guidance, In
    NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, (2021).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] — Jonathan Ho 和 Tim Salimans，无分类器扩散引导，发表于 NeurIPS 2021 深度生成模型与下游应用研讨会，（2021）。'
- en: '[11] — Alex Nichol and Prafulla Dhariwal, Improved denoising diffusion probabilistic
    models, ICML, 8162–8171, PMLR, (2021).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] — Alex Nichol 和 Prafulla Dhariwal，改进的去噪扩散概率模型，ICML，8162–8171，PMLR，（2021）。'
- en: '[12] — Chitwan Saharia, et. al. Palette: Image-to-image diffusion models, arXiv:2111.05826,
    (2021).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] — Chitwan Saharia 等人，Palette：图像到图像的扩散模型，arXiv:2111.05826，（2021）。'
- en: '[13] — Katherine Crowson, et. al. VQGAN-CLIP: Open domain image generation
    and editing with natural language guidance, arXiv:2204.08583, (2022).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] — Katherine Crowson 等人，VQGAN-CLIP：利用自然语言指导的开放域图像生成与编辑，arXiv:2204.08583，（2022）。'
- en: '[14] — Robin Rombach, et. al. High-resolution image'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] — Robin Rombach 等人，高分辨率图像'
- en: synthesis with latent diffusion models, arXiv:2112.10752, (2021).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 基于潜在扩散模型的合成，arXiv:2112.10752，（2021）。
- en: '[15] — Tsung-Yi Lin, et. al. Microsoft COCO: Common objects in context, In
    European conference on computer vision, 740 — 755, Springer, (2014).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] — Tsung-Yi Lin 等人，Microsoft COCO：上下文中的常见对象，发表于《欧洲计算机视觉会议论文集》，740 — 755，Springer，（2014）。'
- en: '[16] — Calvin Luo, Understanding Diffusion Models: A Unified Perspective, arXiv:2208.11970,
    (2022).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] — Calvin Luo，理解扩散模型：一个统一视角，arXiv:2208.11970，（2022）。'
