["```py\ndocker compose up\n```", "```py\n# import airflow dependencies\nfrom airflow import DAG\nfrom airflow.models import Variable\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsGenericHook\n\nfrom datetime import datetime\nimport requests\n```", "```py\nLINKS_ENEM = {\n    \"2010_1\":'https://download.inep.gov.br/educacao_basica/enem/provas/2010/dia1_caderno1_azul_com_gab.pdf',\n    \"2010_2\":'https://download.inep.gov.br/educacao_basica/enem/provas/2010/dia2_caderno7_azul_com_gab.pdf',\n    \"2010_3\":'https://download.inep.gov.br/educacao_basica/enem/provas/2010/AZUL_quarta-feira_GAB.pdf',\n    \"2010_4\":'https://download.inep.gov.br/educacao_basica/enem/provas/2010/AZUL_quinta-feira_GAB.pdf',\n\n    \"2011_1\":'https://download.inep.gov.br/educacao_basica/enem/provas/2011/01_AZUL_GAB.pdf',\n    \"2011_2\":'https://download.inep.gov.br/educacao_basica/enem/provas/2011/07_AZUL_GAB.pdf',\n    \"2011_3\":'https://download.inep.gov.br/educacao_basica/enem/ppl/2011/PPL_ENEM_2011_03_BRANCO.pdf',\n    # OMITTED TO MAKE THIS CODE BLOCK SMALLER\n    # ...\n}\n```", "```py\n# Connections & Variables\nAWS_CONN_ID = \"AWSConnection\"\nYEAR_VARIABLE = \"year\"\n```", "```py\nAWS_CONN_ID = \"AWSConnection\"\nYEAR_VARIABLE = \"year\"\n\ndef download_pdfs_from_year(\n        year_variable,\n        bucket\n    ):\n\n    # Create a S3 connection using the AWS Connection defined in the UI\n    conn = S3Hook(aws_conn_id=AWS_CONN_ID)\n    client = conn.get_conn()\n\n    year = Variable.get(year_variable)\n    year_keys = [key for key in LINKS_ENEM.keys() if year in key]\n\n    for key in year_keys:\n        print(f\"Downloading {key}\")\n        url = LINKS_ENEM[key]\n        r = requests.get(\n            url, \n            allow_redirects=True,\n            verify=False\n        )\n\n        client.put_object(\n            Body=r.content,\n            Key=f\"pdf_{key}.pdf\",\n            Bucket=bucket,\n        )\n\n    # increase the year\n    year = str(int(year)+1)\n    Variable.set(year_variable, year)\n```", "```py\n# Some airflow boilerplate and blah blah blah\ndefault_args = {\n    'owner': 'ENEM_PDF',\n    'depends_on_past': False,\n    'start_date': datetime(2021, 1, 1),\n}\n\ndag = DAG(\n    'process_enem_pdf_aws',\n    default_args=default_args,\n    description='Process ENEM PDFs using AWS',\n    tags=['enem'],\n    catchup=False,\n)\n```", "```py\nwith dag:\n    download_pdf_upload_s3 = PythonOperator(\n        task_id='download_pdf_upload_s3',\n        python_callable=download_pdfs_from_year,\n        op_kwargs={\n            'year_variable': YEAR_VARIABLE ,\n            'bucket': 'enem-bucket',\n        },\n    )\n```", "```py\nsource pdfextractor/bin/activate\npip3 install pypdf2 typing_extensions\n```", "```py\nimport boto3\nfrom PyPDF2 import PdfReader    \nimport io\nimport json\n\ndef lambda_handler(event, context):\n  # The code goes here blah blah blah\n  # Detailed latter\n  # ...\n```", "```py\n{\n  \"Records\": [\n    {\n      # blah blah blah blah blah\n      \"s3\": {\n        # blah blah blah blah blah\n        \"bucket\": {\n          \"name\": \"enem-bucket\",\n          \"ownerIdentity\": {\n            # blah blah blah\n          },\n          \"arn\": \"arn:aws:s3:::enem-bucket\"\n        },\n        \"object\": {\n          \"key\": \"pdf_2010_1.pdf\",\n          \"size\": 1024,\n        }\n      }\n      # blah blah blah\n    }\n  ]\n}\n```", "```py\nimport boto3\nfrom PyPDF2 import PdfReader\nimport io\nimport json\n\ndef lambda_handler(event, context):\n    object_key = event[\"Records\"][0][\"s3\"][\"object\"][\"key\"]\n    bucket = event[\"Records\"][0][\"s3\"][\"bucket\"][\"name\"]\n\n    object_uri = f\"s3://{bucket}/{object_key}\"\n\n    if not object_uri.endswith(\".pdf\"):\n        # Just to make sure that this function will not\n        # cause a recursive loop\n        return \"Object is not a PDF\"\n\n    # Create a S3 client\n    # Remember to configure the Lambda role used\n    # with read and write permissions to the bucket\n    client = boto3.client(\"s3\")\n\n    try:\n        pdf_file = client.get_object(Bucket=bucket, Key=object_key)\n        pdf_file = io.BytesIO(pdf_file[\"Body\"].read())\n    except Exception as e:\n        print(e)\n        print(f\"Error. Lambda was not able to get object from bucket {bucket}\")\n        raise e\n\n    try:\n        pdf = PdfReader(pdf_file)\n        text = \"\"\n        for page in pdf.pages:\n            text += page.extract_text()\n\n    except Exception as e:\n        print(e)\n        print(f\"Error. Lambda was not able to parse PDF {object_uri}\")\n        raise e\n\n    try:\n        # Save the results as JSON\n        text_object = {\n            \"content\": text,\n            \"original_uri\": object_uri\n        }\n\n        client.put_object(\n            Body=json.dumps(text_object).encode(\"utf-8\"),\n            Bucket=bucket,\n            Key=f\"content/{object_key[:-4]}.json\" ,\n        )\n    except Exception as e:\n        print(e)\n        print(f\"Error. Lambda was not able to put object in bucket {bucket}\")\n        raise e\n```", "```py\nfrom awsglue.transforms import *\nfrom pyspark.context import SparkContext\nimport pyspark.sql.functions as F\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\n# Reading the table content from the Data Catalog\ndyf = glueContext.create_dynamic_frame.from_catalog(\n    database=\"enem_pdf_project\", table_name=\"content\"\n)\ndyf.printSchema()\n\n# Just pyspark script below\ndf = dyf.toDF()\n\n# Create a new column with the year\ndf = df.withColumn(\n    \"year\", F.regexp_extract(F.col(\"original_uri\"), \".+pdf_([0-9]{4})\", 1)\n)\n\n# Split the text using the 'questão XX' regex\n# and explode the resultant list\n# resulting in one row per question\ndf = (\n    df.withColumn(\"text\", F.lower(F.col(\"content\")))\n    .withColumn(\n        \"text\",\n        F.regexp_replace(\n            F.col(\"text\"), \"(questão [0-9]+)\", \"<QUESTION_START_MARKER>$1\"\n        ),\n    )\n    .withColumn(\"text\", F.split(F.col(\"text\"), \"<QUESTION_START_MARKER>\"))\n    .withColumn(\"question\", F.explode(F.col(\"text\")))\n    .withColumn(\n        \"question_number\", F.regexp_extract(F.col(\"question\"), \"questão ([0-9]+)\", 1)\n    )\n    .drop(\"content\", \"text\")\n)\n\n# Save the result in CSV to S3\ndf.write.csv(\"s3://enem-bucket/processed/\", mode=\"overwrite\", header=True)\njob.commit()\n```", "```py\ndef trigger_process_enem_pdf_glue_job(\n    job_name\n):\n    session = AwsGenericHook(aws_conn_id=AWS_CONN_ID)\n\n    # Get a client in the same region as the Glue job\n    boto3_session = session.get_session(\n        region_name='us-east-1',\n    )\n\n    # Trigger the job using its name\n    client = boto3_session.client('glue')\n    client.start_job_run(\n        JobName=job_name,\n    )\n```", "```py\nwith dag:\n    download_pdf_upload_s3 = PythonOperator(\n        task_id='download_pdf_upload_s3',\n        python_callable=download_pdfs_from_year,\n        op_kwargs={\n            'year_variable': 'year',\n            'bucket': 'enem-bucket',\n        },\n    )\n\n    trigger_glue_job = PythonOperator(\n        task_id='trigger_glue_job',\n        python_callable=trigger_process_enem_pdf_glue_job,\n        op_kwargs={\n            'job_name': 'Spark_EnemExtractQuestionsJSON'\n        },\n    )\n\n    download_pdf_upload_s3 >> trigger_glue_job\n```"]