- en: 'Unravelling Complexity: A Novel Approach to Manifold Learning Using Noise Injection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unravelling-complexity-a-novel-approach-to-manifold-learning-using-oise-injection-41251565fded?source=collection_archive---------5-----------------------#2023-11-17](https://towardsdatascience.com/unravelling-complexity-a-novel-approach-to-manifold-learning-using-oise-injection-41251565fded?source=collection_archive---------5-----------------------#2023-11-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jceweaver?source=post_page-----41251565fded--------------------------------)[![Jimmy
    Weaver](../Images/2d487e7ee2f13bd3381aad718bafde69.png)](https://medium.com/@jceweaver?source=post_page-----41251565fded--------------------------------)[](https://towardsdatascience.com/?source=post_page-----41251565fded--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----41251565fded--------------------------------)
    [Jimmy Weaver](https://medium.com/@jceweaver?source=post_page-----41251565fded--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F73e4cc6810b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funravelling-complexity-a-novel-approach-to-manifold-learning-using-oise-injection-41251565fded&user=Jimmy+Weaver&userId=73e4cc6810b7&source=post_page-73e4cc6810b7----41251565fded---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----41251565fded--------------------------------)
    ·14 min read·Nov 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F41251565fded&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funravelling-complexity-a-novel-approach-to-manifold-learning-using-oise-injection-41251565fded&user=Jimmy+Weaver&userId=73e4cc6810b7&source=-----41251565fded---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F41251565fded&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funravelling-complexity-a-novel-approach-to-manifold-learning-using-oise-injection-41251565fded&source=-----41251565fded---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the world of data science, high-dimensional data presents both a challenge
    and opportunity. Whilst it provides a treasure trove of relationships and patterns
    that can be moulded and transformed, without careful cleaning and selection it
    can become overwhelming to analyse and draw conclusions from: “The curse of dimensionality”.
    Whilst instinctively you may lean to Principal Component Analysis to embed the
    data to a smaller subspace, you might be making your data problem even more challenging
    and a nonlinear embedding technique might be the more appropriate option. However,
    care needs to be made when selecting the right nonlinear technique as a wrong
    turn might lead to embeddings that are overfit or simply not fit for use. In this
    article I will take the opportunity to discuss a novel approach to understanding
    the manifold within high dimensional data, so that we as data scientists can make
    informed quantitative decisions on the underlining structure of our complex data.'
  prefs: []
  type: TYPE_NORMAL
- en: I will start by covering what manifold learning is and outline a high level
    but informative summary of four popular linear and non-linear embedding techniques.
    From this we will achieve a deeper understanding of the assumptions made in each
    case and the implications these have on effective embeddings. I will also cover
    some Python examples on how to apply my noise injection analytical approach to
    evaluating manifolds and the type of inferences that can be made. At the end of
    this article you will have a thorough understanding of different manifold learning
    techniques, and the steps you can take to truly understand the underlining structure
    within your data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a06a7cf6df5547bf38c8211c3329e43.png)'
  prefs: []
  type: TYPE_IMG
- en: Manifold learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before jumping into these manifold learning techniques, it is important to cover
    exactly what a manifold is? In our context the manifold is an approximate representation
    of the structure of our high-dimensional space which might have local and/or global
    relationships to other nearby data points. The caveat is that we really do not
    know up front the true structure within our N-dimensional space, and often are
    forced to make implicit assumptions of the relationship between the data points
    when embedding our data. Unlike manifold learning in mathematics (Riemannian geometry),
    where it is possible to find an explicit mapping from one space to another.
  prefs: []
  type: TYPE_NORMAL
- en: 'The success of a machine learning model, in terms of performance and data-driven
    insight, is fundamentally subject to the data we pass to it. Whilst passing more
    information can enable these algorithms to find more intricate relationships and
    patterns, it also leads to compounding problems that are often generalised under
    the phrase of the *curse of dimensionality*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfit models**: As the dimensionality of the data increases, subsequent
    machine learning models may fail to generalise the true relationship within the
    data as a result overfit to noise and outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpoint relationship dilation**: In large complex feature spaces it is
    not uncommon for some areas to become so sparse they are hard to model or so concentrated
    that key information is obscured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increase computational complexity**: Most machine learning algorithms do
    not scale well as the number of features increase, leading to increased computational
    time or memory demand when training the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To overcome this we must either reduce the number of features we consider or
    map the data to a lower dimensional space whilst preserving as much key information
    as possible. In the following section we summarise and explore different techniques
    (linear and nonlinear).
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Principal component analysis (PCA) is arguably the most infamous method to embed
    or reduce the dimensionality of a dataset, likely grounded in the explainability
    inferred from its statistical approach. There are plenty of other articles that
    can be found online that go deeper into this algorithm, but for the purpose of
    this article I have outlined the primary steps below.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key takeaway is that PCA attempts to preserve the relationships between
    all data points by assuming a linear manifold and mapping the data onto N orthogonal
    principal components that are a linear combination of the original features. It
    does this by first standardising the data, centring around the mean and scaling
    accordingly so that the variance is consistent across all variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9177947884556e755c9908d93ca0973e.png)'
  prefs: []
  type: TYPE_IMG
- en: where *Xⱼ* is the original feature space *X* for all features *j*, *μ* and *σ*
    are the mean and standard deviation of *Xⱼ* respectively. The algorithm then computes
    the covariance matrix, *S*, of the standardised data
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d68ac91dd23265669778aa3eca81baaa.png)'
  prefs: []
  type: TYPE_IMG
- en: expressing how each variable correlates with every other variable. PCA then
    performs eigen decomposition of the covariance matrix to determine the eigenvalues,
    *λᵢ*, and eigenvectors, *vᵢ*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd48fabfbbb7e74a998b06d84316f74e.png)'
  prefs: []
  type: TYPE_IMG
- en: A Matrix, *W*, is defined by these eigenvectors, ordered by decreasing eigenvalues.
    The final projection of the transformed data, *Y*, is simply the product of *Z*
    and *W*.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, PCA provides a way to uncover the internal structure of the data
    in a way that best preserves and explains the variance (i.e. maximising the information
    across the lowest number of dimensions). Each eigenvalue is proportional to the
    portion of the variance and so our matrix *W* enforces that the first projected
    principal component contains the most variance and each subsequent orthogonal
    component a fraction less.
  prefs: []
  type: TYPE_NORMAL
- en: Local Linear Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before jumping into more advanced nonlinear approaches, let’s start with what
    is probably the simplest to understand, Local Linear Embedding (LLE). In essence,
    LLE assumes that a given data point and its neighbours can be approximately represented
    and mapped to a linear tangential plane upon the manifold such that they are linear
    combinations of one another. Weights to map the neighbourhood clusters to the
    plane are tuned to minimise the error from the transformation (see below) and
    this process is repeated for each data point. Therefore, whilst there is local
    linearity in a neighbourhood of data points, nonlinearity will be captured globally.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a data scientist you need to define the number of nearest neighbours, *k*,
    which will require careful tuning. With this value to hand, the first optimisation
    problem to solve is the weights array *Wᵢⱼ* that maps each data point *Xᵢ* to
    the tangential plane as a linear combination of its neighbours:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/169a474e9428f82c275b97dc84d191e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'subject, for each *i*, to the constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74fa843c401dacf6e27369ca8028f8c7.png)'
  prefs: []
  type: TYPE_IMG
- en: that ensures local geometry is preserved with weights summing to 1\. From this
    our embedding, *Yᵢ*, is simply the product of these weights and the original space
    *Xᵢ* whilst ensuring that the following embedding cost function is minimised
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b17c013e9be5d8cea4ade79c3eee023a.png)'
  prefs: []
  type: TYPE_IMG
- en: The above ensures that the lower dimensional representation best preserves the
    local weights in the original space. Whilst this is quite an eloquent solution
    to capturing nonlinear relationships there is a risk of corrupt embeddings if
    our value for *k* is not tuned appropriately of if there are parts of our N-dimensional
    space that are sparse. Next we will explore other nonlinear embedding techniques
    that use a combination of local and global relationships to form the final embedding,
    which in most cases makes them more robust.
  prefs: []
  type: TYPE_NORMAL
- en: Spectral Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spectral Embedding (SE), also referred to as Laplacian Eigenmap embedding, forms
    a similarity graph that connects all data points together, which are then weighted
    based upon the spectral similarity between points. By doing so, SE not only preserves
    the local relationship (as LLE does) but the connected graph insures that the
    global relationships are also factored in. The reliance on the spectral properties
    observed in the graph Laplacian allows this algorithm to uncover complex non-linear
    structures that other techniques might fail to identify.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step of this algorithm is to construct the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9e7cbbe763177a52b6901f79c620beb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *Wᵢⱼ* is the width of the edge between nodes *i* and *j*, and *σ* is
    a customisable parameter to control the width of the neighbourhoods. From this
    the graph Laplacian matrix, *L*, is then defined as *L=D-W* where *W* is the adjacency
    matrix of the graph and *D* the diagonal degree matrix with the below entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/215fa65c561a70247f422c164782173a.png)'
  prefs: []
  type: TYPE_IMG
- en: By enforcing orthogonality and a centring constraint to the final embedding,
    the algorithm performs eigenvalue decomposition on the Laplacian matrix to identify
    the eigenvalues and eigenvectors to embed the data accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Isometric Feature Mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last nonlinear manifold technique that will be covered is Isometric Feature
    Mapping (ISOMAP), a powerful nonlinear embedding method that offers slightly more
    customisation than those mentioned above. The algorithmic approach assumes you
    can present the high dimensional space by a connected neighbourhood graph, whereby
    the distance between nodes are geodesic. The method then applies multidimensional
    scaling (MDS) to find a lower-dimensional representation of the data such that
    the pairwise distances between the nodes are preserved as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'When constructing the graph you can choose to impose limits on either/both
    the number of nearest neighbours to consider and the relative euclidean distance
    of a data point to its neighbours. These constraints need to be appropriately
    tuned, for example, not too large that shortcut edges are formed (missing key
    structural information) but also not too small that we fail to create a connected
    graph. If these neighbourhood conditions are satisfied then an edge is established
    between two nodes. With the graph, *G*, the algorithm then computes the geodesic
    distance, *Dᵢⱼ,* between all pairs of points using a shortest path algorithm,
    *f*, such as Dijkstra’s or Floyd’s:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c1d004a5bd40cff11a23d6dc02e3022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final step is to map the data to the subspace which involves applying MDS
    to *Dᵢⱼ*. If we recall earlier to our overview of PCA, we would evaluate the covariance
    matrix prior to eigenvalue decomposition. MDS differs slightly by calculating
    a Gram matrix, *B*, relative to centring matrix *H*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4392aa37b4ece63ac4da12d1d0769cd4.png)![](../Images/85b60dff442e1ae97f63d544ead3caa1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *e* is a vector of ones and *n* is the number of data points. The final
    embedding is derived from the *d* eigenvectors that correspond to the largest
    eigenvalues:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a80e6ba01c37e141ac6e788b69876bfd.png)'
  prefs: []
  type: TYPE_IMG
- en: Noise injection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up to now we have covered a selection of linear and non-linear methods for
    embedding data into a lower dimensional space by making certain assumptions about
    the underlining manifold, but how do we know which method is capturing useful
    information, especially when working with high-dimensional data that we cannot
    visualise. One approach to judging the performance of any embedding technique
    from a quantitative angle is to use, what I refer to as, noise injection. With
    this approach we apply varying amounts of noise to the original space and monitor
    the impact it has one our embeddings. The underlining principle is that as the
    amount of noise (distortion) is increased in the original space there will come
    a point where any manifold learning algorithms will fail to capture the true underlining
    structure. From observing how embeddings respond to varying amounts of noise it
    is easy to identify how well each technique is modelling the underlining structure
    in the dataset. Below you can find a step by step summary on how to do this analysis,
    with two python examples to bring the idea to life:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate surrogate datasets from the original dataset with the addition of gaussian
    noise, the variance of which we will scale for each subsequent surrogate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Embed these datasets using a selection of manifold learning techniques.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each technique compare the noise injected embeddings to the embedding of
    the original space (no synthetic additive noise) using procrustes analysis, a
    popular statistical method for comparing two shapes. This evaluation method will
    rotate, scale and translate one space upon another with the objective to minimise
    the sum of squared differences between each data point. This difference is a measure
    of similarity, which will be analysed to observe how each embedding method performs
    when subject to noise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final step is to plot the change in procrustes distance relative to the
    scale of synthetic additional noise, allowing us to draw conclusions on the performance
    of each technique.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s apply the above steps to the classic S-curve dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/039dec37d8f450c8b89775ce13b277a7.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7a093dd67a2d7d99e1281fa410c9168c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Of course, in real world applications we will not be privy to the underlining
    true structure as is the case with this dummy example, but for now let''s assume
    we do not know the true structure. What can we derive from the above graph? Fundamentally
    a good trend would be one that resembles a sigmoid curve, where initially we see
    resilience to a small amount of noise with very similar embedding to the original
    space, however there will come a critical point when this is not the case as the
    noise fractures the underlining structure. At this point we would expect a steep
    increase in procrustes distance with subsequent embeddings capturing noise with
    little to no meaningful information. Considering this, and the graph above, we
    can summarise the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PCA**: Whilst procrustes distance does increase with noise, the trend is
    quite linear and therefore likely to not be capturing sufficient information of
    the true structure. Without considering the other trends, this alone would be
    a strong indication that a nonlinear embedding is required.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LLE**: We see very little resilience to even marginal amounts of noise, which
    is likely due to a violation of the key assumption of local linearity. Increasing
    k, the number of nearest neighbours, might reduce the fragility of this embedding
    but it comes at a potential cost of losing detail (information) in the resultant
    subspace.'
  prefs: []
  type: TYPE_NORMAL
- en: '**ISOMAP**: Initially the performance of this embedding technique looks okay,
    but as more noise is introduced it becomes evident that there is a lack of information
    being captured (with a linear trend post the noise level: 0.25).'
  prefs: []
  type: TYPE_NORMAL
- en: '**SE**: Out of all of the methods explored SE performs the best, although it
    requires tuning to obtain an optimum fit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall you might conclude that evidently the structure of our data resides
    on a nonlinear manifold, and that nonlinearity appears to be captured reasonably
    well with SE and ISOMAP. Considering this, and the poor performance from LLE,
    we might infer significant curvature to the manifold in the original space. Let’s
    explore this with a different example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49c1fe78f3e1a97bf5d01619bf3dc87a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a188df39a6f206905f4f63a7f066a9df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the same functions outlined earlier we obtain the above procrustes distance
    trends for each embedding technique. Clearly the structure within this data is
    significantly nonlinear as we observe the PCA struggling to capture the underlining
    structure. The constant nonlinearity across the original space is also further
    emphasised by how poorly LLE performs, likely due to inconstant mapping of neighbourhoods
    to tangential planes. Again, SE and ISOMAP perform well, the latter slightly outperforming
    the former by trending to a procrustes distance of 1 quicker after the structure
    becomes fractured from noise. It would not be unreasonable to infer that SE is
    capturing some noise in all of the embeddings, which might be rectified with some
    tuning of the parameters. Tuning these algorithms can improve the generalisation
    and fit of the embedded data, here is an example of doing that for the above ISOMAP
    technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/008ef52856772a0cd8452f0f42f6a0cb.png)'
  prefs: []
  type: TYPE_IMG
- en: The above is a very generic example of tuning, and you will certainly want to
    explore other parameters, but the principle is that by simply adjusting k and
    observing the impact of noise we can start to see the algorithm generalise better.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article we have explored a range of manifold learning techniques and
    shown how we can use noise injection to better understand the underlining structure
    within high dimensional data. We achieved this by leveraging our understanding
    of how each algorithm works, the assumptions that underpin each, and analysing
    the influence of noise on embeddings. With this insight to hand we can make more
    informed decisions on how to pre-process or handle the data before passing it
    onto subsequent ML pipelines. This approach also enriches our understanding of
    the generalisation of the resultant embedding, and how it may respond to noise
    or potential future data drift.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you plan to deploy an embedding technique as part of your ML solution
    or want a way to expand upon your process of performing exploratory data analysis,
    the above approach will hold you in good stead to deepen your understanding of
    the hidden obscured structure in any high dimensional dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author.*'
  prefs: []
  type: TYPE_NORMAL
