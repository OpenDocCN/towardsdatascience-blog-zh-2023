- en: 'Unravelling Complexity: A Novel Approach to Manifold Learning Using Noise Injection'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unravelling-complexity-a-novel-approach-to-manifold-learning-using-oise-injection-41251565fded?source=collection_archive---------5-----------------------#2023-11-17](https://towardsdatascience.com/unravelling-complexity-a-novel-approach-to-manifold-learning-using-oise-injection-41251565fded?source=collection_archive---------5-----------------------#2023-11-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jceweaver?source=post_page-----41251565fded--------------------------------)[![Jimmy
    Weaver](../Images/2d487e7ee2f13bd3381aad718bafde69.png)](https://medium.com/@jceweaver?source=post_page-----41251565fded--------------------------------)[](https://towardsdatascience.com/?source=post_page-----41251565fded--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----41251565fded--------------------------------)
    [Jimmy Weaver](https://medium.com/@jceweaver?source=post_page-----41251565fded--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F73e4cc6810b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funravelling-complexity-a-novel-approach-to-manifold-learning-using-oise-injection-41251565fded&user=Jimmy+Weaver&userId=73e4cc6810b7&source=post_page-73e4cc6810b7----41251565fded---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----41251565fded--------------------------------)
    ·14 min read·Nov 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F41251565fded&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funravelling-complexity-a-novel-approach-to-manifold-learning-using-oise-injection-41251565fded&user=Jimmy+Weaver&userId=73e4cc6810b7&source=-----41251565fded---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F41251565fded&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funravelling-complexity-a-novel-approach-to-manifold-learning-using-oise-injection-41251565fded&source=-----41251565fded---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'In the world of data science, high-dimensional data presents both a challenge
    and opportunity. Whilst it provides a treasure trove of relationships and patterns
    that can be moulded and transformed, without careful cleaning and selection it
    can become overwhelming to analyse and draw conclusions from: “The curse of dimensionality”.
    Whilst instinctively you may lean to Principal Component Analysis to embed the
    data to a smaller subspace, you might be making your data problem even more challenging
    and a nonlinear embedding technique might be the more appropriate option. However,
    care needs to be made when selecting the right nonlinear technique as a wrong
    turn might lead to embeddings that are overfit or simply not fit for use. In this
    article I will take the opportunity to discuss a novel approach to understanding
    the manifold within high dimensional data, so that we as data scientists can make
    informed quantitative decisions on the underlining structure of our complex data.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学领域，高维数据既是挑战也是机遇。虽然它提供了大量的关系和模式，可以被塑造和转换，但如果不经过谨慎的清理和选择，它可能会变得令人不知所措，难以从中分析和得出结论：“维度的诅咒”。虽然直觉上你可能倾向于使用主成分分析将数据嵌入到一个较小的子空间，但这可能使你的数据问题变得更加复杂，非线性嵌入技术可能是更合适的选择。然而，在选择正确的非线性技术时需要注意，因为一次错误的转向可能导致过拟合或简单地不适合使用的嵌入。在本文中，我将借此机会讨论一种新颖的方法，来理解高维数据中的流形，使我们作为数据科学家能够基于我们复杂数据的潜在结构做出知情的定量决策。
- en: I will start by covering what manifold learning is and outline a high level
    but informative summary of four popular linear and non-linear embedding techniques.
    From this we will achieve a deeper understanding of the assumptions made in each
    case and the implications these have on effective embeddings. I will also cover
    some Python examples on how to apply my noise injection analytical approach to
    evaluating manifolds and the type of inferences that can be made. At the end of
    this article you will have a thorough understanding of different manifold learning
    techniques, and the steps you can take to truly understand the underlining structure
    within your data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我将首先介绍流形学习是什么，并概述四种流行的线性和非线性嵌入技术的高层但信息丰富的摘要。通过这些，我们将更深入地了解每种情况下所做的假设以及这些假设对有效嵌入的影响。我还将介绍一些
    Python 示例，演示如何应用我的噪声注入分析方法来评估流形及其可能推断的类型。在本文结尾，您将彻底理解不同的流形学习技术，以及您可以采取的步骤来真正理解数据内部结构。
- en: '![](../Images/8a06a7cf6df5547bf38c8211c3329e43.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a06a7cf6df5547bf38c8211c3329e43.png)'
- en: Manifold learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流形学习
- en: Before jumping into these manifold learning techniques, it is important to cover
    exactly what a manifold is? In our context the manifold is an approximate representation
    of the structure of our high-dimensional space which might have local and/or global
    relationships to other nearby data points. The caveat is that we really do not
    know up front the true structure within our N-dimensional space, and often are
    forced to make implicit assumptions of the relationship between the data points
    when embedding our data. Unlike manifold learning in mathematics (Riemannian geometry),
    where it is possible to find an explicit mapping from one space to another.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究这些流形学习技术之前，重要的是准确理解什么是流形？在我们的背景下，流形是对我们高维空间结构的近似表示，该空间可能与附近数据点具有局部和/或全局关系。然而，有一个重要的警告，即我们事先并不知道我们
    N 维空间内的真实结构，通常在嵌入数据时被迫对数据点之间的关系进行隐式假设。不同于数学中的流形学习（黎曼几何），在那里可以找到从一个空间到另一个空间的显式映射。
- en: 'The success of a machine learning model, in terms of performance and data-driven
    insight, is fundamentally subject to the data we pass to it. Whilst passing more
    information can enable these algorithms to find more intricate relationships and
    patterns, it also leads to compounding problems that are often generalised under
    the phrase of the *curse of dimensionality*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的成功，无论是在性能还是数据驱动洞察方面，都基本上取决于我们传递给它的数据。虽然传递更多信息可以使这些算法找到更复杂的关系和模式，但也会导致一系列问题，这些问题通常在“维度诅咒”一词下被泛化。
- en: '**Overfit models**: As the dimensionality of the data increases, subsequent
    machine learning models may fail to generalise the true relationship within the
    data as a result overfit to noise and outliers.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合模型**：随着数据维度的增加，后续的机器学习模型可能无法将数据中的真实关系泛化，结果是对噪声和异常值过拟合。'
- en: '**Interpoint relationship dilation**: In large complex feature spaces it is
    not uncommon for some areas to become so sparse they are hard to model or so concentrated
    that key information is obscured.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据点之间的关系扩展**：在大型复杂特征空间中，某些区域变得非常稀疏，很难建模，或者变得非常集中，关键信息被掩盖。'
- en: '**Increase computational complexity**: Most machine learning algorithms do
    not scale well as the number of features increase, leading to increased computational
    time or memory demand when training the model.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加计算复杂性**：大多数机器学习算法在特征数量增加时无法良好扩展，导致训练模型时增加计算时间或内存需求。'
- en: To overcome this we must either reduce the number of features we consider or
    map the data to a lower dimensional space whilst preserving as much key information
    as possible. In the following section we summarise and explore different techniques
    (linear and nonlinear).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要克服这一问题，我们必须要么减少我们考虑的特征数量，要么将数据映射到一个低维空间，同时尽可能地保留关键信息。在下一节中，我们总结并探讨不同的技术（线性和非线性）。
- en: Principal Component Analysis
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Principal component analysis (PCA) is arguably the most infamous method to embed
    or reduce the dimensionality of a dataset, likely grounded in the explainability
    inferred from its statistical approach. There are plenty of other articles that
    can be found online that go deeper into this algorithm, but for the purpose of
    this article I have outlined the primary steps below.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）可以说是将数据集嵌入或降低维度的最臭名昭著的方法，其解释性来源于其统计方法。在线上可以找到很多更深入探讨该算法的文章，但为了本文的目的，我以下列出了主要步骤。
- en: 'The key takeaway is that PCA attempts to preserve the relationships between
    all data points by assuming a linear manifold and mapping the data onto N orthogonal
    principal components that are a linear combination of the original features. It
    does this by first standardising the data, centring around the mean and scaling
    accordingly so that the variance is consistent across all variables:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的关键点在于，它通过假设一个线性流形并将数据映射到N个正交主成分上（这些主成分是原始特征的线性组合），试图保留所有数据点之间的关系。为此，首先对数据进行标准化，围绕均值中心化并相应缩放，以使所有变量的方差保持一致：
- en: '![](../Images/9177947884556e755c9908d93ca0973e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9177947884556e755c9908d93ca0973e.png)'
- en: where *Xⱼ* is the original feature space *X* for all features *j*, *μ* and *σ*
    are the mean and standard deviation of *Xⱼ* respectively. The algorithm then computes
    the covariance matrix, *S*, of the standardised data
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*Xⱼ*是所有特征*j*的原始特征空间*X*，*μ*和*σ*分别是*Xⱼ*的均值和标准差。算法然后计算标准化数据的协方差矩阵*S*。
- en: '![](../Images/d68ac91dd23265669778aa3eca81baaa.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/d68ac91dd23265669778aa3eca81baaa.png)'
- en: expressing how each variable correlates with every other variable. PCA then
    performs eigen decomposition of the covariance matrix to determine the eigenvalues,
    *λᵢ*, and eigenvectors, *vᵢ*
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 表达每个变量与其他每个变量的相关性。PCA然后执行协方差矩阵的特征值分解来确定特征值*λᵢ*和特征向量*vᵢ*。
- en: '![](../Images/fd48fabfbbb7e74a998b06d84316f74e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/fd48fabfbbb7e74a998b06d84316f74e.png)'
- en: A Matrix, *W*, is defined by these eigenvectors, ordered by decreasing eigenvalues.
    The final projection of the transformed data, *Y*, is simply the product of *Z*
    and *W*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由这些特征向量定义了一个矩阵*W*，按照特征值递减的顺序排列。转换数据的最终投影*Y*，就是*Z*和*W*的乘积。
- en: In summary, PCA provides a way to uncover the internal structure of the data
    in a way that best preserves and explains the variance (i.e. maximising the information
    across the lowest number of dimensions). Each eigenvalue is proportional to the
    portion of the variance and so our matrix *W* enforces that the first projected
    principal component contains the most variance and each subsequent orthogonal
    component a fraction less.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，PCA提供了一种发现数据内部结构的方法，以最大程度地保留和解释方差（即在最低维度上最大化信息）。每个特征值与方差的部分成比例，因此我们的矩阵*W*确保第一个投影的主成分包含最大的方差，每个后续的正交成分则包含稍少的一部分。
- en: Local Linear Embedding
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地线性嵌入
- en: Before jumping into more advanced nonlinear approaches, let’s start with what
    is probably the simplest to understand, Local Linear Embedding (LLE). In essence,
    LLE assumes that a given data point and its neighbours can be approximately represented
    and mapped to a linear tangential plane upon the manifold such that they are linear
    combinations of one another. Weights to map the neighbourhood clusters to the
    plane are tuned to minimise the error from the transformation (see below) and
    this process is repeated for each data point. Therefore, whilst there is local
    linearity in a neighbourhood of data points, nonlinearity will be captured globally.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究更高级的非线性方法之前，让我们从可能最简单易懂的方法开始，即局部线性嵌入（LLE）。本质上，LLE假设给定的数据点及其邻居可以近似地表示并映射到流形上的线性切平面，使它们彼此之间成为线性组合。用于映射邻域集群到平面的权重被调整以最小化转换误差（见下文），并且此过程对每个数据点重复进行。因此，尽管数据点邻域内存在局部线性关系，但全局上会捕捉到非线性。
- en: 'As a data scientist you need to define the number of nearest neighbours, *k*,
    which will require careful tuning. With this value to hand, the first optimisation
    problem to solve is the weights array *Wᵢⱼ* that maps each data point *Xᵢ* to
    the tangential plane as a linear combination of its neighbours:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名数据科学家，您需要定义最近邻数*k*，这需要仔细调整。有了这个值，第一个要解决的优化问题是将权重数组*Wᵢⱼ*定义为将每个数据点*Xᵢ*映射到切平面上的线性组合其邻居：
- en: '![](../Images/169a474e9428f82c275b97dc84d191e1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/169a474e9428f82c275b97dc84d191e1.png)'
- en: 'subject, for each *i*, to the constraint:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个*i*，受以下约束：
- en: '![](../Images/74fa843c401dacf6e27369ca8028f8c7.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74fa843c401dacf6e27369ca8028f8c7.png)'
- en: that ensures local geometry is preserved with weights summing to 1\. From this
    our embedding, *Yᵢ*, is simply the product of these weights and the original space
    *Xᵢ* whilst ensuring that the following embedding cost function is minimised
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 确保保留局部几何，权重总和为1。从而我们的嵌入*Yᵢ*就简单地是这些权重与原始空间*Xᵢ*的乘积，同时确保最小化以下嵌入成本函数
- en: '![](../Images/b17c013e9be5d8cea4ade79c3eee023a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b17c013e9be5d8cea4ade79c3eee023a.png)'
- en: The above ensures that the lower dimensional representation best preserves the
    local weights in the original space. Whilst this is quite an eloquent solution
    to capturing nonlinear relationships there is a risk of corrupt embeddings if
    our value for *k* is not tuned appropriately of if there are parts of our N-dimensional
    space that are sparse. Next we will explore other nonlinear embedding techniques
    that use a combination of local and global relationships to form the final embedding,
    which in most cases makes them more robust.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以上确保了较低维度表示在原始空间中最好地保留了局部权重。虽然这是捕捉非线性关系的一种优雅解决方案，但如果我们的*k*值未经适当调整或者我们的N维空间中存在稀疏部分，则可能会导致嵌入损坏。接下来，我们将探索其他使用局部和全局关系结合形成最终嵌入的非线性嵌入技术，在大多数情况下使它们更加稳健。
- en: Spectral Embedding
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谱嵌入
- en: Spectral Embedding (SE), also referred to as Laplacian Eigenmap embedding, forms
    a similarity graph that connects all data points together, which are then weighted
    based upon the spectral similarity between points. By doing so, SE not only preserves
    the local relationship (as LLE does) but the connected graph insures that the
    global relationships are also factored in. The reliance on the spectral properties
    observed in the graph Laplacian allows this algorithm to uncover complex non-linear
    structures that other techniques might fail to identify.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 谱嵌入(SE)，也称为拉普拉斯特征映射嵌入，形成一个相似性图，将所有数据点连接在一起，然后根据点之间的谱相似性进行加权。通过这样做，SE不仅保留了局部关系（如LLE所做的那样），而且连接的图确保全局关系也被考虑进去。依赖于图拉普拉斯中观察到的谱特性，使得这种算法能够发现其他技术可能无法识别的复杂非线性结构。
- en: 'The first step of this algorithm is to construct the graph:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法的第一步是构建图：
- en: '![](../Images/f9e7cbbe763177a52b6901f79c620beb.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9e7cbbe763177a52b6901f79c620beb.png)'
- en: 'where *Wᵢⱼ* is the width of the edge between nodes *i* and *j*, and *σ* is
    a customisable parameter to control the width of the neighbourhoods. From this
    the graph Laplacian matrix, *L*, is then defined as *L=D-W* where *W* is the adjacency
    matrix of the graph and *D* the diagonal degree matrix with the below entries:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*Wᵢⱼ*是节点*i*和*j*之间的边的宽度，*σ*是一个可定制的参数，用来控制邻域的宽度。从此可以得到图拉普拉斯矩阵*L*的定义为*L=D-W*，其中*W*是图的邻接矩阵，*D*是带以下条目的对角度矩阵：
- en: '![](../Images/215fa65c561a70247f422c164782173a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/215fa65c561a70247f422c164782173a.png)'
- en: By enforcing orthogonality and a centring constraint to the final embedding,
    the algorithm performs eigenvalue decomposition on the Laplacian matrix to identify
    the eigenvalues and eigenvectors to embed the data accordingly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对最终嵌入施加正交性和中心约束，该算法在拉普拉斯矩阵上执行特征值分解，以识别特征值和特征向量，从而相应地嵌入数据。
- en: Isometric Feature Mapping
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等距特征映射
- en: The last nonlinear manifold technique that will be covered is Isometric Feature
    Mapping (ISOMAP), a powerful nonlinear embedding method that offers slightly more
    customisation than those mentioned above. The algorithmic approach assumes you
    can present the high dimensional space by a connected neighbourhood graph, whereby
    the distance between nodes are geodesic. The method then applies multidimensional
    scaling (MDS) to find a lower-dimensional representation of the data such that
    the pairwise distances between the nodes are preserved as much as possible.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个将被介绍的非线性流形技术是等距特征映射（ISOMAP），这是一种强大的非线性嵌入方法，比前面提到的方法稍微具有更多定制性。算法假设可以通过连接的邻域图表示高维空间，其中节点之间的距离是测地距离。然后，该方法应用多维缩放（MDS）以找到数据的低维表示，使得节点之间的成对距离尽可能地保持不变。
- en: 'When constructing the graph you can choose to impose limits on either/both
    the number of nearest neighbours to consider and the relative euclidean distance
    of a data point to its neighbours. These constraints need to be appropriately
    tuned, for example, not too large that shortcut edges are formed (missing key
    structural information) but also not too small that we fail to create a connected
    graph. If these neighbourhood conditions are satisfied then an edge is established
    between two nodes. With the graph, *G*, the algorithm then computes the geodesic
    distance, *Dᵢⱼ,* between all pairs of points using a shortest path algorithm,
    *f*, such as Dijkstra’s or Floyd’s:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建图形时，您可以选择对要考虑的最近邻数量和数据点到其邻居的相对欧几里得距离之一或两者施加限制。这些约束需要适当调整，例如，不能太大以至于形成捷径边（缺少关键结构信息），但也不能太小以至于无法创建连接的图。如果满足这些邻域条件，则在两个节点之间建立一条边。通过图形*G*，算法然后使用最短路径算法*f*（如Dijkstra或Floyd）计算所有点对之间的测地距离*Dᵢⱼ*：
- en: '![](../Images/1c1d004a5bd40cff11a23d6dc02e3022.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c1d004a5bd40cff11a23d6dc02e3022.png)'
- en: 'The final step is to map the data to the subspace which involves applying MDS
    to *Dᵢⱼ*. If we recall earlier to our overview of PCA, we would evaluate the covariance
    matrix prior to eigenvalue decomposition. MDS differs slightly by calculating
    a Gram matrix, *B*, relative to centring matrix *H*:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将数据映射到子空间，这涉及将MDS应用于*Dᵢⱼ*。如果我们回顾PCA的概述，我们会在进行特征值分解之前评估协方差矩阵。MDS稍有不同，通过计算与中心矩阵*H*相关的Gram矩阵*B*：
- en: '![](../Images/4392aa37b4ece63ac4da12d1d0769cd4.png)![](../Images/85b60dff442e1ae97f63d544ead3caa1.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4392aa37b4ece63ac4da12d1d0769cd4.png)![](../Images/85b60dff442e1ae97f63d544ead3caa1.png)'
- en: 'where *e* is a vector of ones and *n* is the number of data points. The final
    embedding is derived from the *d* eigenvectors that correspond to the largest
    eigenvalues:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*e*是一个全为1的向量，*n*是数据点的数量。最终的嵌入来自于对应于最大特征值的*d*个特征向量：
- en: '![](../Images/a80e6ba01c37e141ac6e788b69876bfd.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a80e6ba01c37e141ac6e788b69876bfd.png)'
- en: Noise injection
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 噪声注入
- en: 'Up to now we have covered a selection of linear and non-linear methods for
    embedding data into a lower dimensional space by making certain assumptions about
    the underlining manifold, but how do we know which method is capturing useful
    information, especially when working with high-dimensional data that we cannot
    visualise. One approach to judging the performance of any embedding technique
    from a quantitative angle is to use, what I refer to as, noise injection. With
    this approach we apply varying amounts of noise to the original space and monitor
    the impact it has one our embeddings. The underlining principle is that as the
    amount of noise (distortion) is increased in the original space there will come
    a point where any manifold learning algorithms will fail to capture the true underlining
    structure. From observing how embeddings respond to varying amounts of noise it
    is easy to identify how well each technique is modelling the underlining structure
    in the dataset. Below you can find a step by step summary on how to do this analysis,
    with two python examples to bring the idea to life:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了将数据嵌入低维空间的线性和非线性方法，通过对潜在流形的某些假设。但是，我们如何知道哪种方法捕捉到了有用的信息，特别是当我们处理高维数据时无法可视化时。一种从定量角度评估任何嵌入技术性能的方法是使用我称之为噪声注入的方法。通过这种方法，我们将不同量的噪声应用于原始空间，并监控它对嵌入的影响。基本原理是，当原始空间中的噪声（失真）量增加时，会有一个点，任何流形学习算法都无法捕捉到真实的潜在结构。从观察嵌入如何响应不同量的噪声，可以很容易识别每种技术在建模数据集中的潜在结构时表现如何。下面是一个逐步总结，介绍如何进行这种分析，并提供两个Python示例来实现这个想法：
- en: Generate surrogate datasets from the original dataset with the addition of gaussian
    noise, the variance of which we will scale for each subsequent surrogate.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从原始数据集中生成带有高斯噪声的替代数据集，我们将为每个后续的替代数据集缩放噪声的方差。
- en: Embed these datasets using a selection of manifold learning techniques.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用各种流形学习技术嵌入这些数据集。
- en: For each technique compare the noise injected embeddings to the embedding of
    the original space (no synthetic additive noise) using procrustes analysis, a
    popular statistical method for comparing two shapes. This evaluation method will
    rotate, scale and translate one space upon another with the objective to minimise
    the sum of squared differences between each data point. This difference is a measure
    of similarity, which will be analysed to observe how each embedding method performs
    when subject to noise.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每种技术，使用Procrustes分析将注入噪声的嵌入与原始空间的嵌入（无合成附加噪声）进行比较，Procrustes分析是一种流行的比较两种形状的统计方法。该评估方法将一个空间旋转、缩放和转换到另一个空间，以最小化每个数据点之间的平方差之和。这个差异是相似性的度量，将被分析以观察每种嵌入方法在噪声影响下的表现。
- en: The final step is to plot the change in procrustes distance relative to the
    scale of synthetic additional noise, allowing us to draw conclusions on the performance
    of each technique.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终步骤是绘制Procrustes距离相对于合成附加噪声规模的变化，这使我们能够得出关于每种技术性能的结论。
- en: 'Let’s apply the above steps to the classic S-curve dataset:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将上述步骤应用于经典的S曲线数据集：
- en: '![](../Images/039dec37d8f450c8b89775ce13b277a7.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/039dec37d8f450c8b89775ce13b277a7.png)'
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/7a093dd67a2d7d99e1281fa410c9168c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a093dd67a2d7d99e1281fa410c9168c.png)'
- en: 'Of course, in real world applications we will not be privy to the underlining
    true structure as is the case with this dummy example, but for now let''s assume
    we do not know the true structure. What can we derive from the above graph? Fundamentally
    a good trend would be one that resembles a sigmoid curve, where initially we see
    resilience to a small amount of noise with very similar embedding to the original
    space, however there will come a critical point when this is not the case as the
    noise fractures the underlining structure. At this point we would expect a steep
    increase in procrustes distance with subsequent embeddings capturing noise with
    little to no meaningful information. Considering this, and the graph above, we
    can summarise the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在现实世界应用中，我们不会像这个虚拟示例那样知道潜在的真实结构，但现在我们假设我们不知道真实结构。从上面的图表中我们可以得出什么结论？从根本上说，一个好的趋势应该类似于S型曲线，其中最初我们会看到对少量噪声的抵抗，嵌入与原始空间非常相似，但会有一个临界点，当噪声破坏潜在结构时情况不再如此。此时，我们会预期Procrustes距离急剧增加，随后的嵌入将捕捉到噪声而几乎没有有意义的信息。考虑到这一点，以及上面的图表，我们可以总结如下：
- en: '**PCA**: Whilst procrustes distance does increase with noise, the trend is
    quite linear and therefore likely to not be capturing sufficient information of
    the true structure. Without considering the other trends, this alone would be
    a strong indication that a nonlinear embedding is required.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**PCA**: 尽管 Procrustes 距离随着噪声增加而增加，但趋势相当线性，因此可能无法捕捉真实结构的充分信息。在不考虑其他趋势的情况下，这一点本身就强烈表明需要非线性嵌入。'
- en: '**LLE**: We see very little resilience to even marginal amounts of noise, which
    is likely due to a violation of the key assumption of local linearity. Increasing
    k, the number of nearest neighbours, might reduce the fragility of this embedding
    but it comes at a potential cost of losing detail (information) in the resultant
    subspace.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLE**: 我们几乎看不到对即使是微小噪声的恢复能力，这可能是由于违反了局部线性假设。增加最近邻的数量（k）可能会减少这种嵌入的脆弱性，但这可能会以损失结果子空间中的细节（信息）为代价。'
- en: '**ISOMAP**: Initially the performance of this embedding technique looks okay,
    but as more noise is introduced it becomes evident that there is a lack of information
    being captured (with a linear trend post the noise level: 0.25).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**ISOMAP**: 最初这个嵌入技术的表现看起来还不错，但随着噪声的增加，显而易见地缺乏信息捕捉（噪声水平后的线性趋势：0.25）。'
- en: '**SE**: Out of all of the methods explored SE performs the best, although it
    requires tuning to obtain an optimum fit.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**SE**: 在所有探索的方法中，SE 的表现最好，尽管它需要调整以获得最佳拟合。'
- en: 'Overall you might conclude that evidently the structure of our data resides
    on a nonlinear manifold, and that nonlinearity appears to be captured reasonably
    well with SE and ISOMAP. Considering this, and the poor performance from LLE,
    we might infer significant curvature to the manifold in the original space. Let’s
    explore this with a different example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，你可能会得出结论，显然我们的数据结构位于一个非线性流形上，并且 SE 和 ISOMAP 对这种非线性捕捉得相当好。考虑到这一点，以及 LLE
    的糟糕表现，我们可以推测原始空间中流形的显著曲率。让我们通过一个不同的示例来探讨这一点：
- en: '![](../Images/49c1fe78f3e1a97bf5d01619bf3dc87a.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49c1fe78f3e1a97bf5d01619bf3dc87a.png)'
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/a188df39a6f206905f4f63a7f066a9df.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a188df39a6f206905f4f63a7f066a9df.png)'
- en: 'Using the same functions outlined earlier we obtain the above procrustes distance
    trends for each embedding technique. Clearly the structure within this data is
    significantly nonlinear as we observe the PCA struggling to capture the underlining
    structure. The constant nonlinearity across the original space is also further
    emphasised by how poorly LLE performs, likely due to inconstant mapping of neighbourhoods
    to tangential planes. Again, SE and ISOMAP perform well, the latter slightly outperforming
    the former by trending to a procrustes distance of 1 quicker after the structure
    becomes fractured from noise. It would not be unreasonable to infer that SE is
    capturing some noise in all of the embeddings, which might be rectified with some
    tuning of the parameters. Tuning these algorithms can improve the generalisation
    and fit of the embedded data, here is an example of doing that for the above ISOMAP
    technique:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前概述的相同函数，我们得到上述每种嵌入技术的 Procrustes 距离趋势。显然，这些数据中的结构是显著非线性的，因为我们观察到 PCA 很难捕捉底层结构。原始空间中持续的非线性也通过
    LLE 的糟糕表现进一步强调，可能是由于邻域到切平面的映射不一致。同样，SE 和 ISOMAP 的表现良好，后者在结构被噪声打破后，趋于 1 的 Procrustes
    距离的速度略快于前者。推测 SE 在所有嵌入中都捕捉到了一些噪声，这可能通过一些参数调整来纠正。调整这些算法可以改善嵌入数据的泛化和拟合，这里是一个对上述
    ISOMAP 技术进行调整的示例：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/008ef52856772a0cd8452f0f42f6a0cb.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/008ef52856772a0cd8452f0f42f6a0cb.png)'
- en: The above is a very generic example of tuning, and you will certainly want to
    explore other parameters, but the principle is that by simply adjusting k and
    observing the impact of noise we can start to see the algorithm generalise better.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上述是一个非常通用的调整示例，你肯定会想要探索其他参数，但原则是通过简单地调整 k 并观察噪声的影响，我们可以开始看到算法的泛化效果更好。
- en: Conclusion
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article we have explored a range of manifold learning techniques and
    shown how we can use noise injection to better understand the underlining structure
    within high dimensional data. We achieved this by leveraging our understanding
    of how each algorithm works, the assumptions that underpin each, and analysing
    the influence of noise on embeddings. With this insight to hand we can make more
    informed decisions on how to pre-process or handle the data before passing it
    onto subsequent ML pipelines. This approach also enriches our understanding of
    the generalisation of the resultant embedding, and how it may respond to noise
    or potential future data drift.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了一系列流形学习技术，并展示了如何通过噪声注入来更好地理解高维数据中的潜在结构。我们通过利用对每种算法如何工作的理解、每种算法所依据的假设，以及分析噪声对嵌入的影响来实现这一目标。有了这些见解，我们可以在将数据传递到后续的机器学习管道之前，更加明智地决定如何预处理或处理数据。这种方法也丰富了我们对结果嵌入的泛化理解，以及它可能如何响应噪声或潜在的未来数据漂移。
- en: Whether you plan to deploy an embedding technique as part of your ML solution
    or want a way to expand upon your process of performing exploratory data analysis,
    the above approach will hold you in good stead to deepen your understanding of
    the hidden obscured structure in any high dimensional dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你计划将嵌入技术作为机器学习解决方案的一部分，还是希望扩展你的探索性数据分析过程，上述方法都将帮助你更深入地理解任何高维数据集中的隐藏结构。
- en: '*Unless otherwise noted, all images are by the author.*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图像均由作者提供。*'
