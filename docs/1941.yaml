- en: Hey GPU, Whatâ€™s Up with My Matrix?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6?source=collection_archive---------16-----------------------#2023-06-13](https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6?source=collection_archive---------16-----------------------#2023-06-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A gentle guide to understanding how GPUs perform matrix multiplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Thushan
    Ganegedara](../Images/3fabfa37132f7d3a9e7679c3b8d7e061.png)](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    [Thushan Ganegedara](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0b045d5681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&user=Thushan+Ganegedara&userId=6f0b045d5681&source=post_page-6f0b045d5681----cb7f6d7ae7d6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    Â·8 min readÂ·Jun 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb7f6d7ae7d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&user=Thushan+Ganegedara&userId=6f0b045d5681&source=-----cb7f6d7ae7d6---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb7f6d7ae7d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&source=-----cb7f6d7ae7d6---------------------bookmark_footer-----------)![](../Images/a911c1d35115175bbfe3894ea74e8d00.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Thomas Foster](https://unsplash.com/@thomasfos?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/vWgoeEYdtIY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication; the holy grail of deep neural networks and modern language
    understanding behemoths. As MLEs or data scientists, our fingers are too quick
    to type `tf.matmul` or `torch.matmul` and we never look back. But donâ€™t tell me
    youâ€™ve never had the millisecond infatuation to know what might be happening to
    that matrix when it enters the GPU! If you did, youâ€™re in the right place. Join
    me in a journey through the fascinating intricacies within a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Iâ€™ll explain to you how these compute powerhouses crunch up the numbers. Youâ€™ll
    learn three little-known impressive things GPUs do, when they come face-to-face
    with matrices. By the end of this blog post, youâ€™ll have a good understanding
    of how matrix multiplication works inside GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'GEMM: A true gem ðŸ’Ž for a GPU'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GEMM or generalized matrix multiplication is the kernel thatâ€™s executed when
    GPUs perform matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: '`C = a (A.B) + b C`'
  prefs: []
  type: TYPE_NORMAL
- en: Here, `a` and `b` are scalars, `A` is an `MxK` matrix, `B` is an `KxN` matrix,
    and thus `C` is an `MxN` matrix. Itâ€™s easy as that! You might wonder why that
    trailing addition exists. Turns out this is a pretty common pattern for neural
    networksâ€¦
  prefs: []
  type: TYPE_NORMAL
