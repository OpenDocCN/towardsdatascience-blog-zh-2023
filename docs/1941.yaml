- en: Hey GPU, What’s Up with My Matrix?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嘿 GPU，我的矩阵怎么了？
- en: 原文：[https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6?source=collection_archive---------16-----------------------#2023-06-13](https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6?source=collection_archive---------16-----------------------#2023-06-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6?source=collection_archive---------16-----------------------#2023-06-13](https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6?source=collection_archive---------16-----------------------#2023-06-13)
- en: A gentle guide to understanding how GPUs perform matrix multiplication
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一份温和的指南，帮助你理解 GPU 如何执行矩阵乘法
- en: '[](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Thushan
    Ganegedara](../Images/3fabfa37132f7d3a9e7679c3b8d7e061.png)](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    [Thushan Ganegedara](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Thushan
    Ganegedara](../Images/3fabfa37132f7d3a9e7679c3b8d7e061.png)](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    [Thushan Ganegedara](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0b045d5681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&user=Thushan+Ganegedara&userId=6f0b045d5681&source=post_page-6f0b045d5681----cb7f6d7ae7d6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    ·8 min read·Jun 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb7f6d7ae7d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&user=Thushan+Ganegedara&userId=6f0b045d5681&source=-----cb7f6d7ae7d6---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0b045d5681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&user=Thushan+Ganegedara&userId=6f0b045d5681&source=post_page-6f0b045d5681----cb7f6d7ae7d6---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    ·8分钟阅读·2023年6月13日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb7f6d7ae7d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&user=Thushan+Ganegedara&userId=6f0b045d5681&source=-----cb7f6d7ae7d6---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb7f6d7ae7d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&source=-----cb7f6d7ae7d6---------------------bookmark_footer-----------)![](../Images/a911c1d35115175bbfe3894ea74e8d00.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb7f6d7ae7d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&source=-----cb7f6d7ae7d6---------------------bookmark_footer-----------)![](../Images/a911c1d35115175bbfe3894ea74e8d00.png)'
- en: Photo by [Thomas Foster](https://unsplash.com/@thomasfos?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/vWgoeEYdtIY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Thomas Foster](https://unsplash.com/@thomasfos?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/vWgoeEYdtIY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Matrix multiplication; the holy grail of deep neural networks and modern language
    understanding behemoths. As MLEs or data scientists, our fingers are too quick
    to type `tf.matmul` or `torch.matmul` and we never look back. But don’t tell me
    you’ve never had the millisecond infatuation to know what might be happening to
    that matrix when it enters the GPU! If you did, you’re in the right place. Join
    me in a journey through the fascinating intricacies within a GPU.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法；深度神经网络和现代语言理解巨头的圣杯。作为机器学习工程师或数据科学家，我们的手指总是迅速地敲打 `tf.matmul` 或 `torch.matmul`，而且从不回头。但别告诉我你从未有过对矩阵进入
    GPU 后发生了什么的短暂迷恋！如果你有，那你来对地方了。跟随我一起探索 GPU 内部迷人的复杂性。
- en: I’ll explain to you how these compute powerhouses crunch up the numbers. You’ll
    learn three little-known impressive things GPUs do, when they come face-to-face
    with matrices. By the end of this blog post, you’ll have a good understanding
    of how matrix multiplication works inside GPUs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我会向你解释这些计算强者如何处理数据。你将了解到 GPU 在面对矩阵时所做的三件鲜为人知的令人印象深刻的事情。到这篇博客文章结束时，你将对 GPU 内部的矩阵乘法有一个很好的理解。
- en: 'GEMM: A true gem 💎 for a GPU'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'GEMM: 对于 GPU 来说，真正的瑰宝💎'
- en: GEMM or generalized matrix multiplication is the kernel that’s executed when
    GPUs perform matrix multiplication.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GEMM 或广义矩阵乘法是当 GPU 执行矩阵乘法时执行的内核。
- en: '`C = a (A.B) + b C`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`C = a (A.B) + b C`'
- en: Here, `a` and `b` are scalars, `A` is an `MxK` matrix, `B` is an `KxN` matrix,
    and thus `C` is an `MxN` matrix. It’s easy as that! You might wonder why that
    trailing addition exists. Turns out this is a pretty common pattern for neural
    networks…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`a` 和 `b` 是标量，`A` 是一个 `MxK` 矩阵，`B` 是一个 `KxN` 矩阵，因此 `C` 是一个 `MxN` 矩阵。就是这么简单！你可能会想知道为什么会有那项附加的加法。事实证明，这是一种神经网络中相当常见的模式……
