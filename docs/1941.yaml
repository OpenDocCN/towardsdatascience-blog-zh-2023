- en: Hey GPU, Whatâ€™s Up with My Matrix?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6?source=collection_archive---------16-----------------------#2023-06-13](https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6?source=collection_archive---------16-----------------------#2023-06-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A gentle guide to understanding how GPUs perform matrix multiplication
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Thushan
    Ganegedara](../Images/3fabfa37132f7d3a9e7679c3b8d7e061.png)](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    [Thushan Ganegedara](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0b045d5681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&user=Thushan+Ganegedara&userId=6f0b045d5681&source=post_page-6f0b045d5681----cb7f6d7ae7d6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    Â·8 min readÂ·Jun 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb7f6d7ae7d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&user=Thushan+Ganegedara&userId=6f0b045d5681&source=-----cb7f6d7ae7d6---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb7f6d7ae7d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&source=-----cb7f6d7ae7d6---------------------bookmark_footer-----------)![](../Images/a911c1d35115175bbfe3894ea74e8d00.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Thomas Foster](https://unsplash.com/@thomasfos?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/vWgoeEYdtIY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication; the holy grail of deep neural networks and modern language
    understanding behemoths. As MLEs or data scientists, our fingers are too quick
    to type `tf.matmul` or `torch.matmul` and we never look back. But donâ€™t tell me
    youâ€™ve never had the millisecond infatuation to know what might be happening to
    that matrix when it enters the GPU! If you did, youâ€™re in the right place. Join
    me in a journey through the fascinating intricacies within a GPU.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Iâ€™ll explain to you how these compute powerhouses crunch up the numbers. Youâ€™ll
    learn three little-known impressive things GPUs do, when they come face-to-face
    with matrices. By the end of this blog post, youâ€™ll have a good understanding
    of how matrix multiplication works inside GPUs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¼šå‘ä½ è§£é‡Šè¿™äº›è®¡ç®—å¼ºè€…å¦‚ä½•å¤„ç†æ•°æ®ã€‚ä½ å°†äº†è§£åˆ° GPU åœ¨é¢å¯¹çŸ©é˜µæ—¶æ‰€åšçš„ä¸‰ä»¶é²œä¸ºäººçŸ¥çš„ä»¤äººå°è±¡æ·±åˆ»çš„äº‹æƒ…ã€‚åˆ°è¿™ç¯‡åšå®¢æ–‡ç« ç»“æŸæ—¶ï¼Œä½ å°†å¯¹ GPU å†…éƒ¨çš„çŸ©é˜µä¹˜æ³•æœ‰ä¸€ä¸ªå¾ˆå¥½çš„ç†è§£ã€‚
- en: 'GEMM: A true gem ğŸ’ for a GPU'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'GEMM: å¯¹äº GPU æ¥è¯´ï¼ŒçœŸæ­£çš„ç‘°å®ğŸ’'
- en: GEMM or generalized matrix multiplication is the kernel thatâ€™s executed when
    GPUs perform matrix multiplication.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GEMM æˆ–å¹¿ä¹‰çŸ©é˜µä¹˜æ³•æ˜¯å½“ GPU æ‰§è¡ŒçŸ©é˜µä¹˜æ³•æ—¶æ‰§è¡Œçš„å†…æ ¸ã€‚
- en: '`C = a (A.B) + b C`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`C = a (A.B) + b C`'
- en: Here, `a` and `b` are scalars, `A` is an `MxK` matrix, `B` is an `KxN` matrix,
    and thus `C` is an `MxN` matrix. Itâ€™s easy as that! You might wonder why that
    trailing addition exists. Turns out this is a pretty common pattern for neural
    networksâ€¦
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ`a` å’Œ `b` æ˜¯æ ‡é‡ï¼Œ`A` æ˜¯ä¸€ä¸ª `MxK` çŸ©é˜µï¼Œ`B` æ˜¯ä¸€ä¸ª `KxN` çŸ©é˜µï¼Œå› æ­¤ `C` æ˜¯ä¸€ä¸ª `MxN` çŸ©é˜µã€‚å°±æ˜¯è¿™ä¹ˆç®€å•ï¼ä½ å¯èƒ½ä¼šæƒ³çŸ¥é“ä¸ºä»€ä¹ˆä¼šæœ‰é‚£é¡¹é™„åŠ çš„åŠ æ³•ã€‚äº‹å®è¯æ˜ï¼Œè¿™æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œä¸­ç›¸å½“å¸¸è§çš„æ¨¡å¼â€¦â€¦
