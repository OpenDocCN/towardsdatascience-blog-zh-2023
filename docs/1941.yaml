- en: Hey GPU, What’s Up with My Matrix?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6?source=collection_archive---------16-----------------------#2023-06-13](https://towardsdatascience.com/hey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6?source=collection_archive---------16-----------------------#2023-06-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A gentle guide to understanding how GPUs perform matrix multiplication
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Thushan
    Ganegedara](../Images/3fabfa37132f7d3a9e7679c3b8d7e061.png)](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    [Thushan Ganegedara](https://thushv89.medium.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0b045d5681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&user=Thushan+Ganegedara&userId=6f0b045d5681&source=post_page-6f0b045d5681----cb7f6d7ae7d6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb7f6d7ae7d6--------------------------------)
    ·8 min read·Jun 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcb7f6d7ae7d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&user=Thushan+Ganegedara&userId=6f0b045d5681&source=-----cb7f6d7ae7d6---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcb7f6d7ae7d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhey-gpu-whats-up-with-my-matrix-cb7f6d7ae7d6&source=-----cb7f6d7ae7d6---------------------bookmark_footer-----------)![](../Images/a911c1d35115175bbfe3894ea74e8d00.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Thomas Foster](https://unsplash.com/@thomasfos?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/vWgoeEYdtIY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication; the holy grail of deep neural networks and modern language
    understanding behemoths. As MLEs or data scientists, our fingers are too quick
    to type `tf.matmul` or `torch.matmul` and we never look back. But don’t tell me
    you’ve never had the millisecond infatuation to know what might be happening to
    that matrix when it enters the GPU! If you did, you’re in the right place. Join
    me in a journey through the fascinating intricacies within a GPU.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: I’ll explain to you how these compute powerhouses crunch up the numbers. You’ll
    learn three little-known impressive things GPUs do, when they come face-to-face
    with matrices. By the end of this blog post, you’ll have a good understanding
    of how matrix multiplication works inside GPUs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我会向你解释这些计算强者如何处理数据。你将了解到 GPU 在面对矩阵时所做的三件鲜为人知的令人印象深刻的事情。到这篇博客文章结束时，你将对 GPU 内部的矩阵乘法有一个很好的理解。
- en: 'GEMM: A true gem 💎 for a GPU'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'GEMM: 对于 GPU 来说，真正的瑰宝💎'
- en: GEMM or generalized matrix multiplication is the kernel that’s executed when
    GPUs perform matrix multiplication.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GEMM 或广义矩阵乘法是当 GPU 执行矩阵乘法时执行的内核。
- en: '`C = a (A.B) + b C`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`C = a (A.B) + b C`'
- en: Here, `a` and `b` are scalars, `A` is an `MxK` matrix, `B` is an `KxN` matrix,
    and thus `C` is an `MxN` matrix. It’s easy as that! You might wonder why that
    trailing addition exists. Turns out this is a pretty common pattern for neural
    networks…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`a` 和 `b` 是标量，`A` 是一个 `MxK` 矩阵，`B` 是一个 `KxN` 矩阵，因此 `C` 是一个 `MxN` 矩阵。就是这么简单！你可能会想知道为什么会有那项附加的加法。事实证明，这是一种神经网络中相当常见的模式……
