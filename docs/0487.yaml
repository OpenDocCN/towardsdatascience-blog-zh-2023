- en: 'Optimization, Newton’s Method, & Profit Maximization: Part 2— Constrained Optimization
    Theory'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770?source=collection_archive---------9-----------------------#2023-02-02](https://towardsdatascience.com/optimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770?source=collection_archive---------9-----------------------#2023-02-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/d3d6794ce774945474e8adef3314e157.png)'
  prefs: []
  type: TYPE_IMG
- en: All Images by Author
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to extend Newton’s Method to and solve constrained optimization problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jakepenzak?source=post_page-----dc18613c5770--------------------------------)[![Jacob
    Pieniazek](../Images/2d9c6295d39fcaaec4e62f11c359cb29.png)](https://medium.com/@jakepenzak?source=post_page-----dc18613c5770--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dc18613c5770--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dc18613c5770--------------------------------)
    [Jacob Pieniazek](https://medium.com/@jakepenzak?source=post_page-----dc18613c5770--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0948d99b1c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=post_page-6f0948d99b1c----dc18613c5770---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dc18613c5770--------------------------------)
    ·13 min read·Feb 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdc18613c5770&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770&user=Jacob+Pieniazek&userId=6f0948d99b1c&source=-----dc18613c5770---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc18613c5770&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foptimization-newtons-method-profit-maximization-part-2-constrained-optimization-theory-dc18613c5770&source=-----dc18613c5770---------------------bookmark_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: This article is the **2nd** in a 3 part series. In the [1st part](https://medium.com/towards-data-science/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565),
    we studied basic optimization theory. Now, in pt. 2, we will extend this theory
    to constrained optimization problems. Lastly, in pt. 3, we will apply the optimization
    theory covered, as well as econometric and economic theory, to solve a profit
    maximization problem.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Consider the following problem: You want to determine how much money to invest
    in specific financial instruments to maximize your return on investment. However,
    the problem of simply maximizing your return on investment is too broad and simple
    of an optimization question to ask. By virtue of the simplicity, the solution
    is to just invest *all* of your money in the financial instrument that shows promise
    for highest return. Clearly this is not a good investment strategy; so, how can
    we improve this? *By putting constraints on the investment decisions, our choice
    variables.* For example, we can specify constraints that, to name a couple, 1)
    limit the amount of financial risk we are willing to entertain (see [modern portfolio
    theory](https://en.wikipedia.org/wiki/Modern_portfolio_theory)) or 2) specify
    the amount of our portfolio to be allocated towards each category of financial
    instruments (equity, bonds, derivatives, etc.) — the possibilities are endless.
    Notice how this problem becomes significantly more tractable as we add constraints.
    Despite this simple example, it helps to capture a fundamental motivation of constrained
    optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: The essence of constrained optimization is to provide unconstrained optimization
    problems a sense of tractability and applicability to complex real world problems.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Constrained optimization is defined as “the process of optimizing an objective
    function with respect to some variables in the presence of constraints on those
    variables.”[1] The process of adding constraints on the variables transforms an
    unconstrained and, perhaps, intractable optimization problem into one which can
    help model and solve a real world problem. However, the addition of constraints
    can turn a simple optimization problem into a problem that is no longer trivial.
    In this post, we will dive into some of the techniques that we can add to our
    toolbox to extend the unconstrained optimization theory, learned in [part 1](https://medium.com/towards-data-science/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565)
    of this series, to now solve constrained optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: In [part 1](https://medium.com/towards-data-science/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565),
    we covered basic optimization theory — including 1) setting up and solving a simple
    single variable optimization problem analytically, 2) iterative optimization schemes
    — namely, gradient descent & Newton’s Method, and 3) implementing Newton’s method
    by hand and in python for a multi-dimensional optimization problem. This article
    is designed to be accessible for those who are already familiar with the content
    covered in [part 1](https://medium.com/towards-data-science/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Constrained Optimization Basics (& [Part 1](https://medium.com/towards-data-science/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565)
    Recap)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A mathematical optimization problem can be formulated abstractly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91d0602859b3778af388eb855d77cd2a.png)'
  prefs: []
  type: TYPE_IMG
- en: (1)
  prefs: []
  type: TYPE_NORMAL
- en: where we choose real values of the vector **x** that minimize the *objective
    function* *f*(**x**) (or maximize -*f*(**x**)) subject to the *inequality constraints*
    *g*(**x**) and *equality constraints* *h*(**x**). In [part 1](https://medium.com/towards-data-science/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565),
    we discussed how to solve these problems in the absence of *g*(**x**) and *h*(**x**)
    and now we will introduce these back into our optimization problem. First, let’s
    succinctly recap how to implement Newton’s method for unconstrained problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that we can approximate the first order necessary condition of a minimum
    using a Taylor Series expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f671020420ba3b2e88ede14734e5cb88.png)'
  prefs: []
  type: TYPE_IMG
- en: (2)
  prefs: []
  type: TYPE_NORMAL
- en: 'where **H**(**x**)and **∇***f*(**x**) denote the Hessian and gradient of*f*(**x**),
    respectively. Each iterative addition of delta, **Δ,** is an expected better approximation
    of the optimal values **x***. Thus, each iterative step using the NM can be represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aeea601b1cde8c66b5fd09c5d76f1710.png)'
  prefs: []
  type: TYPE_IMG
- en: (3) Newton Method Iterative Scheme
  prefs: []
  type: TYPE_NORMAL
- en: 'We do this scheme until we reach convergence across one or more of the following
    criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/855991698a9b3c03d179ffcba7c2ef8c.png)'
  prefs: []
  type: TYPE_IMG
- en: (4) Convergence Criteria for Iterative Optimization Schemes
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting this into python code, we make use of [SymPy](https://www.sympy.org/en/index.html)
    — a python library for symbolic mathematics — and create generalizable functions
    to compute the gradient, compute the Hessian, and implement Newton’s method for
    an *n*-dimensional function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And to solve an unconstrained optimization problem, we can run the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e02ab685850059438bb587571f23aa35.png)'
  prefs: []
  type: TYPE_IMG
- en: If all of the material reviewed above feels extremely foreign, then I recommend
    taking a look at [part 1](https://medium.com/towards-data-science/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565)
    which will dive into everything above in more depth and bring you up to speed!
    Without further ado, let’s dive into implementing constraints in our optimization
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: All of the following constrained optimization techniques can and should
    be incorporated w/ gradient descent algorithms when applicable!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Solving Constrained Optimization Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed above there are two possible constraints on an objective function
    — equality and inequality constraints. Note that there are varying methodologies
    out there for dealing with each type of constraint with varying pros and cons.
    See [2] for a further discussion of different methodologies. Nevertheless, we
    will home our focus in on two methodologies, one for equality and one for inequality
    constraints, that I believe are robust in their performance, easy to grasp for
    newcomers, and easily integrated together into one cohesive problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Equality Constraints — The Lagrangian**'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will address optimization problems with equality constraints in our
    optimization problem. That is, optimization problems that take the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c3591911df6b1fdfbc9e110bd125a96.png)'
  prefs: []
  type: TYPE_IMG
- en: (5)
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we are working with the Rosenbrock’s Parabolic Valley, as in [part
    1](https://medium.com/towards-data-science/optimization-newtons-method-profit-maximization-part-1-basic-optimization-theory-ff7c5f966565),
    but now with the equality constraint that x² - y = 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8432624d0786198013c82b316631ac28.png)'
  prefs: []
  type: TYPE_IMG
- en: (6) Rosenbrock’s Parabolic Valley w/ Equality Constraint **(Problem 1)**
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that, for simplicity and consistency, the equality constraints should
    be written such that they are equal to zero. Now our optimization problem looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f53819cf3d990f8b3b66a9a4f099f9f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Rosenbrock’s Parabolic Valley (purple-yellow color map) & Equality Constraint
    Curve (Black)
  prefs: []
  type: TYPE_NORMAL
- en: where the ***feasible region*** of the optimal values lie somewhere ***along***
    the ***intersection*** of the equality constraint curve and our objective function
    above.
  prefs: []
  type: TYPE_NORMAL
- en: 'Joseph-Louis Lagrange developed a method for incorporating an equality constraint
    directly into the objective function — creatingthe ***Lagrangian function***—
    so that traditional approaches using first and second derivates can still be applied.[2][3]
    Formally, the Lagrangian function takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9006bcb26fb988f6e540e010e4406b0e.png)'
  prefs: []
  type: TYPE_IMG
- en: (7) Formal Definition of Lagrangian Function
  prefs: []
  type: TYPE_NORMAL
- en: where *f*(**x**) and *h*(**x**) are the objective function and equality constraints,
    respectively.**Λ** are the ***Lagrange multipliers*** that correspond to each
    equality constraint *j.* The Lagrange multipliers are treated as new choice variables
    in the Lagrangian function. It just so happens that the *necessary conditions*
    for **x*** to be a minimum of the equality constrained problem is that **x***
    corresponds to the stationarity points of the Lagrangian (**x***, **Λ***). That
    is,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e5891acaa9ae389cfef97f4643c8c26.png)'
  prefs: []
  type: TYPE_IMG
- en: (8) Lagrangian First Order Conditions
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example above — the equality constrained Rosenbrock’s Parabolic Valley
    (Eq. 1)— we can write our Lagrangian function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22009a1b32d8186a21a319735d60e285.png)'
  prefs: []
  type: TYPE_IMG
- en: (9)
  prefs: []
  type: TYPE_NORMAL
- en: We can then solve this Lagrangian using Newton’s method, but now including the
    Lagrange multipliers as additional choice variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ff125c962b23c9f672350f9325ec12c.png)'
  prefs: []
  type: TYPE_IMG
- en: One can easily verify that the solution satisfies our equality constraint. And
    there you have it! That wasn’t too bad, right? This method can be extended to
    add any number of equality constraints — just add another Lagrange multiplier.
    Let’s move on now to the incorporation of inequality constraints.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inequality Constraints — The Logarithmic Barrier Function**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will address optimization problems with inequality constraints in our
    optimization problem. That is, optimization problems that take the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58c27b771f787bcae0f5d488a401ed27.png)'
  prefs: []
  type: TYPE_IMG
- en: (10)
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose, again, we are working with Rosenbrock’s Parabolic Valley but now with
    the inequality constraints x ≤ 0 and y ≥ 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d59ea38998b5b8c688ae979913d5d93d.png)'
  prefs: []
  type: TYPE_IMG
- en: (11) Rosenbrock’s Parabolic Valley w/ Inequality Constraint **(Problem 2)**
  prefs: []
  type: TYPE_NORMAL
- en: 'Now our optimization problem looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9114ed61ed48e7bb6a35c7a4bce20b19.png)'
  prefs: []
  type: TYPE_IMG
- en: Rosenbrock’s Parabolic Valley (purple-yellow color map) & Inequality Constraint
    Planes (Black)
  prefs: []
  type: TYPE_NORMAL
- en: where the ***feasible region*** of the optimal values lies in the quadrant bounded
    by the constraints that is marked by the red star.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because these constraints do not have a strict equality, our ability to directly
    include them into the objective function is not as straightforward. However, we
    can get creative — what we can do is augment our objective function to include
    a “barrier” in the objective function that penalizes values of the solution that
    approach the bounds of the inequality constraints. These class of methods are
    known as “interior-point methods” or “barrier methods.”[4][5] Like the Lagrangian
    function, we can transform our original constrained optimization problem into
    an unconstrained optimization problem by incorporating barrier functions (the
    logarithmic barrier function in our case) that can be solved using traditional
    methods— thereby creating the ***barrier function***. Formally, the logarithmic
    barrier function is characterized by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00ca107eb8bf9a5ca094939980679c6e.png)'
  prefs: []
  type: TYPE_IMG
- en: (12) Formal Definition of Logarithmic Barrier Function
  prefs: []
  type: TYPE_NORMAL
- en: where *ρ* is a small positive scalar — known as the barrier parameter. As *ρ*
    → 0, the solution of the barrier function *B*(**x**,*ρ*) should converge to the
    solution of our original constrained optimization function. Note, the *c*(**x**)
    states that depending on how we formulate our inequality constraints (greater
    than or less than zero) will dictate whether we use the negative or positive of
    that constraint. We know that y=log(x) is undefined for x ≤ 0, thus we need to
    formulate our constraint to always be ≥ 0.
  prefs: []
  type: TYPE_NORMAL
- en: How exactly does the barrier method work, you may ask? To begin with, when using
    the barrier method, we must choose starting values that are in the feasible region.
    As the optimal values approach the “barrier” outlined by the constraint, this
    method relies on the fact that the logarithmic function approaches negative infinity
    as the value approaches zero, thereby penalizing the objective function value.
    As *ρ* → 0, the penalization decreases (see figure directly below) and we converge
    to the solution. However, it is necessary to start with a sufficiently large *ρ*
    so that the penalization is large enough to prevent “jumping” out of the barriers.
    Therefore, the algorithm has one extra loop than Newton’s method alone — namely,
    we choose a starting value *ρ,* optimize the barrier function using Newton’s method,
    then update *ρ* by slowly decreasing it (*ρ* → 0), and repeat until convergence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/242fcc7a4203dfeda32e1586b7de38e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Logarithmic Barrier for Different Values of *ρ*
  prefs: []
  type: TYPE_NORMAL
- en: 'Revisiting our example above — the inequality constrained Rosenbrock’s Parabolic
    Valley (Eq. 2)— we can write our barrier function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e4ea9a6ecc31e36040a0a9085014df6.png)'
  prefs: []
  type: TYPE_IMG
- en: (13)
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that log(a) + log(b) = log(ab) and our one constraint x ≤ 0 → -x ≥ 0\.
    We must then update our code to accommodate the barrier method algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now solve the Barrier function with the code above (Note: Make sure
    starting values are in the feasible range of inequality constraints & you may
    have to increase the starting value of rho if you jump out of inequality constraints):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1047398ab08e2f8ad3015f9fa87a4ace.png)![](../Images/d2b0cc746107ec260184b19a31b4199c.png)'
  prefs: []
  type: TYPE_IMG
- en: Clearly the solution satisfies the inequality constraints specified. And there
    you have it. We have now tackled inequality constraints in our optimization problems.
    To wrap up, let’s put everything together and move on to tackling constrained
    optimization problems with mixed constraints — which is simply the combination
    of what we have done above.
  prefs: []
  type: TYPE_NORMAL
- en: '**Putting it All Together**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now solve our optimization problem by combining both the equality and
    inequality constraints from above. That is, we want to solve an optimization of
    the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91d0602859b3778af388eb855d77cd2a.png)'
  prefs: []
  type: TYPE_IMG
- en: (14)
  prefs: []
  type: TYPE_NORMAL
- en: 'All we have to do is combine the Lagrangian and the Barrier functions into
    one function. Thus, we can create a generalizable function, call it *O*, for dealing
    with optimization problems that have both equality and inequality constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46185c03e54a45fd89007a0ab112edb1.png)'
  prefs: []
  type: TYPE_IMG
- en: (15) Generalizable function for constrained optimization problems
  prefs: []
  type: TYPE_NORMAL
- en: 'where, as before, **Λ** is the vector of Lagrange multipliers and*ρ* is the
    barrier parameter.Thus, combining our constrained (Eq. 6) and unconstrained problems
    from above (Eq. 11), we can formulate our mixed constrained optimization problem
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bae4b09261694380a079cc64b665c83f.png)'
  prefs: []
  type: TYPE_IMG
- en: (16)
  prefs: []
  type: TYPE_NORMAL
- en: In python,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'With the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50feae3f14b28f95d2f2da062f6e5c6d.png)![](../Images/5965f76b2276c8b3ecffa41085fc5f14.png)'
  prefs: []
  type: TYPE_IMG
- en: We can verify that this solution does in fact obey the constraints. Specifically,
    x ≤ 0, y ≥ 3, & x² - y = 2\. Satisfying, no?
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Phew. Take a deep breath — you earned it. Hopefully at this point you should
    have a much better understanding of the techniques to incorporate constraints
    into your optimization problems. We are still just brushing the surface of the
    different tools and techniques utilized in mathematical optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Stay tuned for **part 3 of this series**, the final part, where we will apply
    the optimization material learned thus far alongside econometric & economic theory
    to solve a profit maximization problem. It is my goal that part 3 will bring home
    everything we have covered and show a practical use case. As usual, I hope you
    have enjoyed reading this much as much I have enjoyed writing it!
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [https://en.wikipedia.org/wiki/Constrained_optimization](https://en.wikipedia.org/wiki/Constrained_optimization)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Snyman, J. A., & Wilke, D. N. (2019). *Practical mathematical optimization:
    Basic optimization theory and gradient-based algorithms* (2nd ed.). Springer.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://en.wikipedia.org/wiki/Lagrange_multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [https://en.wikipedia.org/wiki/Interior-point_method](https://en.wikipedia.org/wiki/Interior-point_method)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [https://en.wikipedia.org/wiki/Barrier_function](https://en.wikipedia.org/wiki/Barrier_function)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Access all the code via this GitHub Repo:* [https://github.com/jakepenzak/Blog-Posts](https://github.com/jakepenzak/Blog-Posts)'
  prefs: []
  type: TYPE_NORMAL
- en: '*I appreciate you reading my post! My posts on Medium seek to explore real-world
    and theoretical applications utilizing* ***econometric*** *and* ***statistical/machine
    learning*** *techniques. Additionally, I seek to provide posts on the theoretical
    underpinnings of various methodologies via theory and simulations. Most importantly,
    I write to learn! I hope to make complex topics slightly more accessible to all.
    If you enjoyed this post, please consider* [***following me on Medium***](https://medium.com/@jakepenzak)*!*'
  prefs: []
  type: TYPE_NORMAL
