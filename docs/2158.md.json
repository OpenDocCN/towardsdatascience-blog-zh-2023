["```py\n\"\"\"Answer the question below given the context. \n<document 1>\n<document 2>\n...\n<document n>\n\nQuestion: <user question>\nAnswer:\n\"\"\"\n```", "```py\npip install langchain==0.0.191\npip install transformers\n```", "```py\nimport os\n\nfolder_name = \"sample_code\"\nos.system(f\"git clone https://github.com/hwchase17/langchain {folder_name}\")\n```", "```py\nfrom langchain.docstore.document import Document\n\ndocuments = []\nfor root, dirs, files in os.walk(folder_name):\n    for file in files:\n        try:\n            with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as o:\n                code = o.readlines()\n                d = Document(page_content=\"\\n\".join(code), metadata={\"source\": os.path.join(root, file)})\n                documents.append(d)\n        except UnicodeDecodeError:\n            # some files are not utf-8 encoded; let's ignore them for now.\n            pass\n```", "```py\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\n\nhfemb = HuggingFaceEmbeddings(model_name=\"krlvi/sentence-t5-base-nlpl-code-x-glue\")\npersist_directory = \"db\"\ndb = Chroma.from_documents(documents, hfemb, persist_directory=persist_directory)\ndb.persist()\n```", "```py\ndb = Chroma(persist_directory=persist_directory, embedding_function=hfemb)\n```", "```py\nretriever = db.as_retriever()\n```", "```py\nfrom langchain import HuggingFacePipeline\nimport transformers\n\nmodel_id = \"mosaicml/mpt-7b-instruct\"\nconfig = transformers.AutoConfig.from_pretrained(model_id,trust_remote_code=True)\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(model_id, config=config, trust_remote_code=True)\npipe = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100)\nllm = HuggingFacePipeline(pipeline=pipe)\n```", "```py\nconfig.init_device=\"cuda:0\"\nmodel.to(device='cuda:0')\npipe = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100, device=0)\n```", "```py\nfrom langchain.chains import ConversationalRetrievalChain\n\nqa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, return_source_documents=True)\n```", "```py\nresult = qa_chain({\"question\":\"What is the return type of the create_index function in the KNNRetriever?\", \"chat_history\":[]})\nprint(f\"Answer: {result['answer']}\")\nprint(f\"Sources: {[x.metadata['source'] for x in result['source_documents']]}\")\n```", "```py\nAnswer:  The return type of the create_index function in the KNNRetriever is np.ndarray.\nSources: ['sample_code/langchain/retrievers/knn.py', 'sample_code/langchain/vectorstores/elastic_vector_search.py', 'sample_code/langchain/vectorstores/elastic_vector_search.py', 'sample_code/langchain/vectorstores/opensearch_vector_search.py']\n```"]