- en: Summarize Podcast Transcripts and Long Texts Better with NLP and AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 NLP 和 AI 更好地总结播客文字记录和长文本
- en: 原文：[https://towardsdatascience.com/summarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb?source=collection_archive---------0-----------------------#2023-05-03](https://towardsdatascience.com/summarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb?source=collection_archive---------0-----------------------#2023-05-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/summarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb?source=collection_archive---------0-----------------------#2023-05-03](https://towardsdatascience.com/summarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb?source=collection_archive---------0-----------------------#2023-05-03)
- en: Why the existing summarization approach is flawed, and a walkthrough of how
    to do better
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么现有的总结方法存在缺陷，以及如何改进的详细步骤
- en: '[](https://iztham.medium.com/?source=post_page-----e04c89d3b2cb--------------------------------)[![Isaac
    Tham](../Images/57f44f34adc534dbf09791a8cd54e7f3.png)](https://iztham.medium.com/?source=post_page-----e04c89d3b2cb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e04c89d3b2cb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e04c89d3b2cb--------------------------------)
    [Isaac Tham](https://iztham.medium.com/?source=post_page-----e04c89d3b2cb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://iztham.medium.com/?source=post_page-----e04c89d3b2cb--------------------------------)[![Isaac
    Tham](../Images/57f44f34adc534dbf09791a8cd54e7f3.png)](https://iztham.medium.com/?source=post_page-----e04c89d3b2cb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e04c89d3b2cb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e04c89d3b2cb--------------------------------)
    [Isaac Tham](https://iztham.medium.com/?source=post_page-----e04c89d3b2cb--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8177b59b4815&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsummarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb&user=Isaac+Tham&userId=8177b59b4815&source=post_page-8177b59b4815----e04c89d3b2cb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e04c89d3b2cb--------------------------------)
    ·11 min read·May 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe04c89d3b2cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsummarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb&user=Isaac+Tham&userId=8177b59b4815&source=-----e04c89d3b2cb---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8177b59b4815&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsummarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb&user=Isaac+Tham&userId=8177b59b4815&source=post_page-8177b59b4815----e04c89d3b2cb---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e04c89d3b2cb--------------------------------)
    · 11 分钟阅读 · 2023年5月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe04c89d3b2cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsummarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb&user=Isaac+Tham&userId=8177b59b4815&source=-----e04c89d3b2cb---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe04c89d3b2cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsummarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb&source=-----e04c89d3b2cb---------------------bookmark_footer-----------)![](../Images/27bac9eb1dfda2b1e95e33debfc9c121.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe04c89d3b2cb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsummarize-podcast-transcripts-and-long-texts-better-with-nlp-and-ai-e04c89d3b2cb&source=-----e04c89d3b2cb---------------------bookmark_footer-----------)![](../Images/27bac9eb1dfda2b1e95e33debfc9c121.png)'
- en: Image from Unsplash.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 Unsplash。
- en: LLMs like GPT-4 have taken the world by storm, and one of the tasks that generative
    text models are particularly good at is summarization of long texts such as books
    or podcast transcripts. However, the conventional method of getting LLMs to summarize
    long texts is actually fundamentally flawed. In this post, I will tell you about
    the problems with existing summarization methods, and present a better summarization
    method that actually takes into account the structure of the text! Even better,
    this method will also give us the text’s main topics — killing two birds with
    one stone!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: I will walk you through how you can easily implement this in Python, with just
    several tweaks of the existing method. This is the method that we use at [Podsmart](http://podsmartai.com),
    our newly-launched AI-powered podcast summarizer app that helps busy intellectuals
    save hours of listening.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '**Problems with existing solutions**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The canonical method to summarize long texts is by **recursive summarization**,
    in which the long text is split equally into shorter chunks which can fit inside
    the LLM’s context window. Each chunk is summarized, and the summaries are concatenated
    together to and then passed through GPT-3 to be further summarized. This process
    is repeated until one obtains a final summary of desired length.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: However, the major downside is that existing implementations e.g. LangChain’s
    [summarize chain using map_reduce](https://python.langchain.com/en/latest/modules/chains/index_examples/summarize.html),
    split the text into chunks with no regard for the logical and structural flow
    of the text.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the article is 1000 words long, a chunk size of 200 would mean
    that we would get 5 chunks. What if the author has several main points, the first
    of which takes up the first 250 words? The last 50 words would be placed into
    the second chunk with text from the author’s next point, and passing this chunk
    through GPT-3’s summarizer would lead to potentially important information from
    the first point being omitted. Also, some key points may be longer than others,
    and there is no way of knowing this a priori.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Another method is the ‘refine’ method, which passes every chunk of text, along
    with a summary from previous chunks, through the LLM, which progressively refines
    the summary as it sees more of the text (see the prompt [here](https://github.com/hwchase17/langchain/blob/master/langchain/chains/summarize/refine_prompts.py)).
    However, the sequential nature of the process means that it cannot be parallelized
    and takes linear time, far longer than a recursive method which takes logarithmic
    time. Additionally, intuition suggests that the meaning from the initial parts
    will be overrepresented in the final summary. For podcast transcripts where the
    first minutes are advertisements completely irrelevant to the rest of the podcast,
    this is a stumbling block. Hence, this method is hence not widely used.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Even if more advanced language models come out with longer context windows,
    it will still be woefully inadequate for many summarization use cases (entire
    books), and some chunking and recursive summarization is inevitably necessary.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 即使出现了更先进的语言模型，具有更长的上下文窗口，它仍然会在许多总结用例（整个书籍）中显得极其不足，因此不可避免地需要一些分块和递归总结。
- en: In essence, if the summarization process doesn’t recognize the text’s hierarchy
    of meaning and isn’t compatible with it, it’s not likely that the resulting summary
    will be good enough to accurately convey the author’s intended meaning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，如果总结过程未能识别文本的意义层次结构，并且与之不兼容，那么生成的总结很可能不足以准确传达作者的意图。
- en: '**A Better Way Forward**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**更好的前进方式**'
- en: A better solution is to tackle the summarization and topic modelling process
    together in the same algorithm. Here, we split the summary outputs from one step
    of the recursive summarization into chunks to be fed into the next step. We can
    achieve this through by clustering chunks semantically into topics and passing
    topics into the next iteration of the summarization. Let’s walk you through how
    we can implement this in Python!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的解决方案是将总结和主题建模过程在同一个算法中一起处理。在这里，我们将递归总结的一个步骤的总结输出拆分为块，然后将这些块输入到下一步。我们可以通过将块在语义上进行主题聚类，并将主题传递到下一次总结迭代中来实现这一点。让我们来看看如何在
    Python 中实现这一点吧！
- en: '**Requirements**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**要求**'
- en: 'Python packages:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Python 包：
- en: scipy — for cosine distance metric
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scipy — 用于余弦距离度量
- en: networkx — for the Louvain community detection algorithm
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: networkx — 用于 Louvain 社区检测算法
- en: langchain — package with utility functions allowing you to call LLMs like OpenAI’s
    GPT-3
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: langchain — 一个实用功能包，允许你调用像 OpenAI 的 GPT-3 这样的 LLMs。
- en: '**Data and Preprocessing**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据和预处理**'
- en: 'The GitHub repository with the Jupyter notebook and data can be found here:
    [https://github.com/thamsuppp/llm_summary_medium](https://github.com/thamsuppp/llm_summary_medium)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 包含 Jupyter notebook 和数据的 GitHub 仓库可以在这里找到：[https://github.com/thamsuppp/llm_summary_medium](https://github.com/thamsuppp/llm_summary_medium)
- en: The text we are summarizing today is the 2023 State of the Union speech by US
    President Joe Biden. The text file is in the GitHub repository, and [here](https://www.whitehouse.gov/briefing-room/speeches-remarks/2023/02/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery/#:~:text=We%20are%20the%20only%20country,ever%20created%20in%20four%20years.)
    is the original source. The speech, as are all US government publications, are
    in [public domain](https://www.copyrightlaws.com/copyright-laws-in-u-s-government-works/).
    Do note that it is important to make sure that that you are allowed to use the
    source text — Towards Data Science has published [some helpful tips](/writers-faq-462571b65b35#1b06)
    about checking for dataset copyrights and licenses.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天总结的文本是 2023 年美国总统乔·拜登的国情咨文演讲。文本文件在 GitHub 仓库中，[这里](https://www.whitehouse.gov/briefing-room/speeches-remarks/2023/02/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery/#:~:text=We%20are%20the%20only%20country,ever%20created%20in%20four%20years.)
    是原始来源。演讲，像所有美国政府出版物一样，属于[公有领域](https://www.copyrightlaws.com/copyright-laws-in-u-s-government-works/)。请注意，确保你被允许使用源文本很重要——《Towards
    Data Science》发布了[一些有用的提示](/writers-faq-462571b65b35#1b06)，关于如何检查数据集的版权和许可证。
- en: We split the raw text it into sentences, restricting sentences to have a minimum
    length of 20 words and maximum length of 80.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将原始文本拆分为句子，限制句子的最小长度为20个单词，最大长度为80个单词。
- en: '**Creating Chunks**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建块**'
- en: Instead of creating chunks large enough to fit into a context window, I propose
    that the chunk size should be the number of sentences it generally takes to express
    a discrete idea. This is because we will later embed this chunk of text, essentially
    distilling its semantic meaning into a vector. I currently use 5 sentences (but
    you can experiment with other numbers). I tend to have a 1-sentence overlap between
    chunks, just to ensure continuity so that each chunk has some contextual information
    about the previous chunk. For the given text file, there are 65 chunks, with an
    average chunk length is 148 words, with a range from 46–197 words.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与其创建足够大以适应上下文窗口的块，我建议块的大小应为表达一个离散思想通常需要的句子数量。这是因为我们随后将嵌入这个文本块，本质上将其语义意义提炼成一个向量。我目前使用5个句子（但你可以尝试其他数量）。我倾向于在块之间有1句重叠，以确保连续性，使每个块都能包含一些关于前一个块的上下文信息。对于给定的文本文件，共有65个块，平均块长148个单词，范围从46到197个单词。
- en: '**Getting Titles and Summaries for Each Chunk**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取每个块的标题和摘要**'
- en: Now, this is where I start deviating from LangChain’s summarize chain.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**Getting two for the price of 1 LLM call: title + summary**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: I wanted to get both an informative title as well as a summary of each chunk
    (the importance of the title will become clearer later). So I created a custom
    prompt, adapting Langchain’s [default summarize chain prompt](https://github.com/hwchase17/langchain/blob/master/langchain/chains/summarize/map_reduce_prompt.py).
    As you can see in `map_prompt_template` - `text` is a parameter that will be inserted
    into the prompt - this will be the original text of each chunk. I create a LLM,
    which is currently GPT-3, and create an LLMChain, which combines an LLM with the
    prompt template. Then, `map_llm_chain.apply()` calls GPT-3 with the prompt template
    with the text inputs inserted in, returning titles and summaries for each chunk,
    which I parse into a dictionary output. Note that all chunks can be processed
    in parallel as they are independent of each other, hence leading to immense speed
    benefits.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: You can use ChatGPT for 10x cheaper price and similar performance, however when
    I tried it, only the GPT-3 LLM runs the query in parallel, whereas using ChatGPT
    runs it one-by-one, which was painfully slow as I normally pass in ~100 chunks
    at the same time. Running ChatGPT in parallel requires an async implementation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Embedding Chunks and Clustering into Topics**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: After obtaining the summaries for each chunk, I will embed them using OpenAI’s
    embeddings into 1536-dimension vectors. The conventional recursive summarization
    method does not require embedding as they split texts arbitrarily by even length.
    For us, we aim to improve on that by grouping semantically-similar chunks together
    into topics.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Grouping texts into topics is a well-studied problem in NLP, with many traditional
    methods such as [Latent Dirichlet Allocation](/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)
    which predates the age of deep learning. I remember using LDA in 2017 to cluster
    newspaper articles for my college’s newspaper — it was very slow to estimate,
    and only used word frequency which does not capture semantic meaning.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can leverage OpenAI’s embeddings-as-a-service API to obtain embeddings
    that capture the semantic meaning of sentences in one second. There are many other
    possible embedding models that can be used here e.g. HuggingFace’s `sentence-transformers`
    , which reportedly has better performance than OpenAI’s embeddings, but that involves
    downloading the model and running it on your own server.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: After obtaining embedding vectors from the chunks, we group similar vectors
    together.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: I create a chunk similarity matrix, where the `(i,j)th` entry denotes the cosine
    similarity between the embedding vectors of the ith and jth chunk, i.e. the semantic
    similarity between the chunks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00f32224a5ea1b3512ef8d67c774bd57.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: The chunk similarity matrix for the State of the Union speech. You can see certain
    groups of chunks are similar to collectively similar to each other — this is what
    the topic detection algorithm later on will uncover. Image by author.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 国情咨文演讲的片段相似度矩阵。你可以看到某些片段组在语义上彼此相似——这正是话题检测算法随后会揭示的内容。图片由作者提供。
- en: 'We can view this as a similarity graph between nodes which are chunks, with
    edge weight being the similarity between two chunks. We use the [Louvain community
    detection algorithm](/louvain-algorithm-93fde589f58c) to detect topics from the
    chunks. This is because communities are defined in graph analysis as having dense
    intra-community connections, and sparse inter-community connections, which is
    what we want: chunks within a topic to be very semantically-similar to each other,
    while each chunk being less semantically-similar to chunks in other detected topics.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其视为节点之间的相似度图，其中节点是片段，边的权重是两个片段之间的相似度。我们使用 [Louvain 社区检测算法](/louvain-algorithm-93fde589f58c)
    从片段中检测话题。这是因为在图分析中，社区被定义为具有密集的内部社区连接和稀疏的社区间连接，这正是我们想要的：话题内的片段彼此非常语义相似，而每个片段与其他检测到的话题中的片段的语义相似度较低。
- en: The Louvain community detection algorithm has a hyperparameter called resolution
    — small resolutions lead to smaller clusters. Additionally, I add a hyperparameter
    `proximity_bonus` - which bumps up the similarity score of chunks if their position
    in the original text is closer to each other. You can interpret this as treating
    the temporal structure of the text as a prior (i.e. chunks closer to each other
    are more likely to be semantically similar). I put this in to discouraging the
    detected topics from having chunks from all over the text which is less plausible.
    The function also tries to minimize the variance in cluster sizes, preventing
    situations when one cluster has 1 chunk while another has 13 chunks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Louvain 社区检测算法有一个超参数叫做分辨率——较小的分辨率会导致较小的簇。此外，我增加了一个超参数 `proximity_bonus` —— 如果原始文本中片段的位置彼此接近，它会提高片段的相似度分数。你可以将此解释为将文本的时间结构视为先验（即，彼此接近的片段更可能在语义上相似）。我加入这个参数是为了避免检测到的话题中包含来自文本各处的片段，这种情况不太可能。该函数还试图最小化簇大小的方差，防止出现一个簇有1个片段而另一个簇有13个片段的情况。
- en: For the State of the Union speech, the output are 10 clusters, which are nicely
    continuous.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于国情咨文演讲，输出是10个簇，这些簇之间的连贯性很好。
- en: '![](../Images/9487630608c51b3d01d1b5ec957f06ab.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9487630608c51b3d01d1b5ec957f06ab.png)'
- en: Detected topic clusters for the State of the Union speech. Image by author.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 检测到的国情咨文演讲的话题簇。图片由作者提供。
- en: '![](../Images/bbf397fe7ecb2acec32aee0c908b6283.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbf397fe7ecb2acec32aee0c908b6283.png)'
- en: Detected topic clusters for a Bloomberg Surveillance podcast transcript. The
    purple and orange topics picked up advertisements. Image by author.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 检测到的 Bloomberg Surveillance 播客转录的话题簇。紫色和橙色的话题识别了广告。图片由作者提供。
- en: The second image is the topic clustering for another podcast episode. As you
    can see, the beginning and end is detected to the same topic, which is common
    for podcasts with ads at the start and end of the episode. Some topics, like the
    purple one, are also discontinuous — it’s nice that our method allows for this,
    as a text can cycle back to an earlier-mentioned topic, and this another possibility
    that the conventional text-splitting fails to account for.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第二张图片是另一个播客节目中的话题聚类。正如你所见，开始和结束部分被检测为相同的话题，这在开头和结尾有广告的播客中很常见。一些话题，比如紫色的，也是不连续的——我们的算法允许这种情况很不错，因为文本可能回到之前提到的话题，而这也是传统文本拆分未考虑的另一种可能性。
- en: '**Topic Titles and Summaries**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**话题标题和总结**'
- en: Now, we have topics that are semantically coherent that we can pass into the
    next step of the recursive summarization. For this example, this will be the last
    step, but for much longer texts like books, you can imagine repeating the process
    several times until there are ~10 topics left whose topic summaries can fit into
    the context window.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们得到的是语义一致的话题，可以进入递归总结的下一步。对于这个例子，这将是最后一步，但对于更长的文本如书籍，你可以想象重复这个过程几次，直到剩下大约10个话题，其话题总结可以适应上下文窗口。
- en: The next step involves three different parts.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步涉及三个不同的部分。
- en: '**Topic Titles**: For each topic, we have generated a list of titles for the
    chunks in that topic. We pass all topics’ list of titles into GPT-3 and ask it
    to aggregate the titles to arrive at one title for each topic. We do this concurrently
    for all topics to prevent the topics’ titles from being too similar with one another.
    Previously, when I generated topic titles individually, GPT-3 does not have context
    of the other topic titles, hence there were cases where 4 out of 7 titles were
    ‘Federal Reserve’s Monetary Policy’. This is why we wanted to generate chunk titles
    — trying to fit all chunk summaries into the context window here may not be possible
    for very long texts.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题标题**：对于每个主题，我们生成了该主题中块的标题列表。我们将所有主题的标题列表传递给 GPT-3，并要求其聚合这些标题以得出每个主题的一个标题。我们对所有主题同时进行此操作，以防主题标题之间过于相似。以前，当我单独生成主题标题时，GPT-3
    没有其他主题标题的上下文，因此出现了 7 个标题中有 4 个是‘联邦储备的货币政策’的情况。这就是我们希望生成块标题的原因——试图将所有块摘要放入上下文窗口在非常长的文本中可能不可行。'
- en: As you can see below, the titles look good! Descriptive, yet unique from each
    other.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，标题看起来很好！描述性强，但彼此独特。
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Topic Summary:** Nothing new here, this involves combining the chunk summaries
    of each topic together and asking GPT-3 to summarize them into a topic summary.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题摘要：** 没有新意，这涉及将每个主题的块摘要结合在一起，并要求 GPT-3 将其总结为主题摘要。'
- en: '**Final Summary:** To arrive at the overall summary of the text, we once again
    concatenate the topic summaries together and prompt GPT-3 to summarize them.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**最终摘要：** 为了得到文本的总体摘要，我们再次将主题摘要连接在一起，并提示 GPT-3 进行总结。'
- en: '![](../Images/eed826bec998107a4e309a67481f4b3f.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eed826bec998107a4e309a67481f4b3f.png)'
- en: The final summary of the State of the Union address. Image by author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 国情咨文的最终摘要。图像由作者提供。
- en: '**To summarize**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**'
- en: What are the benefits of our method?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法有哪些好处？
- en: The text is split hierarchically into topics, chunks, and sentences. As we progress
    down the hierarchy, we get progressively detailed and specific summaries, from
    the final summary, to each topic’s summary, to each chunk’s summary.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 文本被分层拆分为主题、块和句子。随着层级的深入，我们得到越来越详细和具体的摘要，从最终摘要，到每个主题的摘要，再到每个块的摘要。
- en: As I mentioned above, the summary hence accurately captures the semantic structure
    of the text — where there is an overarching theme which is split into several
    main topics, each of which comprises several key ideas (chunks), ensuring that
    the essential information is retained through the various layers of summarization.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我上面提到的，摘要准确地捕捉了文本的语义结构——其中有一个总体主题，分成几个主要主题，每个主题包含若干关键思想（块），确保在各层摘要中保留了关键信息。
- en: This also offers greater flexibility than merely an overall summary. Different
    people are more interested in different parts of the text and would hence choose
    the appropriate level of detail they want for each part of the text.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这比仅仅提供总体摘要更具灵活性。不同的人对文本的不同部分更感兴趣，因此他们会选择适合自己需求的详细程度。
- en: Of course, this requires pairing the generated summaries with an intuitive and
    coherent interface that visualizes this hierarchical nature of the text. An example
    of such a visualization is on Podsmart— [click here](https://www.podsmartai.com/examples/26e50d5d)
    for an interactive summary of the speech.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这需要将生成的摘要与直观且连贯的界面配对，该界面可视化文本的层级特性。此类可视化的一个示例见 Podsmart—— [点击这里](https://www.podsmartai.com/examples/26e50d5d)
    查看演讲的互动摘要。
- en: '![](../Images/985553239a363da0687b3d8e45a8120e.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/985553239a363da0687b3d8e45a8120e.png)'
- en: A visualization of the extracted topics and the timeline of the transcript.
    Image by author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的主题及其时间线的可视化。图像由作者提供。
- en: Note that this does not drastically increase the LLM costs — we are still passing
    just as much input as the conventional method into the LLM, yet we get a much
    richer summarization.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这并不会显著增加 LLM 的费用——我们仍然将与传统方法相同的输入传递给 LLM，但我们获得了更丰富的摘要。
- en: '**TLDR** — here are the secret sauces to produce superior summaries of your
    texts'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**TLDR** —— 这里是生成优质文本摘要的秘密法宝'
- en: Semantically-coherent topics — by doing semantic embeddings on small chunks
    of the text and splitting the text by semantic similarity
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语义一致的主题——通过对文本的小块进行语义嵌入并按语义相似性拆分文本
- en: Obtaining titles and summaries from chunks — which required customizing the
    prompt instead of using the default LangChain summarize chain
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据块中获取标题和摘要——这需要自定义提示，而不是使用默认的 LangChain 摘要链
- en: Calibrating the Louvain community-detection algorithm — hyperparameters like
    resolution and proximity bonus ensure the generated topic clusters are plausible
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 校准 Louvain 社区检测算法——如分辨率和接近度奖励等超参数确保生成的话题簇是合理的
- en: Distinct topic titles — concurrently generating all topic titles which required
    chunk titles
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同的话题标题——同时生成所有话题标题，这需要块标题
- en: Once again, you can check out the entire source code on the [GitHub repo](https://github.com/thamsuppp/llm_summary_medium).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，你可以在 [GitHub 仓库](https://github.com/thamsuppp/llm_summary_medium) 查看整个源代码
- en: 'If you found this post helpful:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章对你有帮助：
- en: 'do check out my other Medium articles: [technical tips on how to build an AI
    app as a data scientist](/from-data-scientist-to-ai-developer-lessons-building-an-generative-ai-web-app-in-2023-95959a00a474),
    [generating music using deep learning](https://medium.com/towards-data-science/generating-music-using-deep-learning-cb5843a9d55e)'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请查看我在 Medium 上的其他文章：[作为数据科学家构建 AI 应用的技术提示](/from-data-scientist-to-ai-developer-lessons-building-an-generative-ai-web-app-in-2023-95959a00a474)、[使用深度学习生成音乐](https://medium.com/towards-data-science/generating-music-using-deep-learning-cb5843a9d55e)
- en: try out my [app](http://podsmartai.com) — [Podsmart](http://podsmartai.com)
    transcribes and summarizes podcasts and YouTube videos, saving busy intellectuals
    hours of listening
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 试试我的 [app](http://podsmartai.com)——[Podsmart](http://podsmartai.com) 可以转录和总结播客和
    YouTube 视频，为忙碌的知识分子节省听音时间
- en: follow me on [LinkedIn](https://www.linkedin.com/in/isaacthamhy/) or [Twitter/X](https://twitter.com/thamsuppp),
    and reach out via messages or comments! I’ve love to bounce ideas about all things
    data science and AI
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关注我在 [LinkedIn](https://www.linkedin.com/in/isaacthamhy/) 或 [Twitter/X](https://twitter.com/thamsuppp)，通过消息或评论与我联系！我很乐意讨论关于数据科学和
    AI 的各种想法
- en: Thanks for reading!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
