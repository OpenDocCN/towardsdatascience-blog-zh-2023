- en: Distributed Llama 2 on CPUs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 CPU 上分布式运行 Llama 2
- en: 原文：[https://towardsdatascience.com/distributed-llama-2-on-cpus-via-llama-cpp-pyspark-65736e9f466d?source=collection_archive---------2-----------------------#2023-08-02](https://towardsdatascience.com/distributed-llama-2-on-cpus-via-llama-cpp-pyspark-65736e9f466d?source=collection_archive---------2-----------------------#2023-08-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/distributed-llama-2-on-cpus-via-llama-cpp-pyspark-65736e9f466d?source=collection_archive---------2-----------------------#2023-08-02](https://towardsdatascience.com/distributed-llama-2-on-cpus-via-llama-cpp-pyspark-65736e9f466d?source=collection_archive---------2-----------------------#2023-08-02)
- en: '*A toy example of bulk inference on commodity hardware using Python, via llama.cpp
    and PySpark.*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*一个使用 Python 在普通硬件上进行批量推理的玩具示例，通过 llama.cpp 和 PySpark。*'
- en: '[](https://jonathanapple.medium.com/?source=post_page-----65736e9f466d--------------------------------)[![Jonathan
    Apple](../Images/82095d4a9ffde8260eb679f76a2f162c.png)](https://jonathanapple.medium.com/?source=post_page-----65736e9f466d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65736e9f466d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65736e9f466d--------------------------------)
    [Jonathan Apple](https://jonathanapple.medium.com/?source=post_page-----65736e9f466d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jonathanapple.medium.com/?source=post_page-----65736e9f466d--------------------------------)[![Jonathan
    Apple](../Images/82095d4a9ffde8260eb679f76a2f162c.png)](https://jonathanapple.medium.com/?source=post_page-----65736e9f466d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65736e9f466d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65736e9f466d--------------------------------)
    [Jonathan Apple](https://jonathanapple.medium.com/?source=post_page-----65736e9f466d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d68a0f98df3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-llama-2-on-cpus-via-llama-cpp-pyspark-65736e9f466d&user=Jonathan+Apple&userId=7d68a0f98df3&source=post_page-7d68a0f98df3----65736e9f466d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----65736e9f466d--------------------------------)
    ·6 min read·Aug 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65736e9f466d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-llama-2-on-cpus-via-llama-cpp-pyspark-65736e9f466d&user=Jonathan+Apple&userId=7d68a0f98df3&source=-----65736e9f466d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d68a0f98df3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-llama-2-on-cpus-via-llama-cpp-pyspark-65736e9f466d&user=Jonathan+Apple&userId=7d68a0f98df3&source=post_page-7d68a0f98df3----65736e9f466d---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----65736e9f466d--------------------------------)
    ·6分钟阅读·2023年8月2日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65736e9f466d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-llama-2-on-cpus-via-llama-cpp-pyspark-65736e9f466d&user=Jonathan+Apple&userId=7d68a0f98df3&source=-----65736e9f466d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65736e9f466d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-llama-2-on-cpus-via-llama-cpp-pyspark-65736e9f466d&source=-----65736e9f466d---------------------bookmark_footer-----------)![](../Images/875651a67a41a94af74d7dfb027b9fbc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65736e9f466d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdistributed-llama-2-on-cpus-via-llama-cpp-pyspark-65736e9f466d&source=-----65736e9f466d---------------------bookmark_footer-----------)![](../Images/875651a67a41a94af74d7dfb027b9fbc.png)'
- en: '*Image by author via DALL-E*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者通过 DALL-E 制作的图像*'
- en: Why?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么？
- en: This exercise is about using [Llama 2](https://github.com/facebookresearch/llama),
    an LLM (Large Language Model) from [Meta AI](https://ai.meta.com/llama/), to summarize
    many documents at once. The scalable summarization of unstructured, semi-structured,
    and structured text can exist as a [feature by itself,](https://medium.com/analytics-vidhya/text-summarization-using-nlp-3e85ad0c6349)
    and also be part of data pipelines that feed into downstream machine learning
    models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习旨在使用 [Llama 2](https://github.com/facebookresearch/llama)，一个来自 [Meta AI](https://ai.meta.com/llama/)
    的 LLM（大型语言模型），一次性总结多个文档。大规模的非结构化、半结构化和结构化文本摘要可以作为 [一个独立的特性，](https://medium.com/analytics-vidhya/text-summarization-using-nlp-3e85ad0c6349)
    也可以作为数据管道的一部分，为下游机器学习模型提供数据。
- en: 'Specifically, we want to prove the simultaneous feasibility of:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们希望证明以下情况的同时可行性：
- en: Running Llama 2 on **CPUs** (i.e., removing GPU capacity constraints)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 **CPUs** 上运行 Llama 2（即，去除 GPU 能力限制）
- en: Smooth integration of an LLM with [**Apache Spark**](https://spark.apache.org/)
    (a key part of Big Data ecosystems)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 [**Apache Spark**](https://spark.apache.org/) 的平滑集成（大数据生态系统的关键部分）
- en: '**No usage of third-party endpoints** (i.e., models must run locally due to
    air-gapped infrastructure or confidentiality requirements)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不使用第三方端点**（即，由于隔离基础设施或保密要求，模型必须在本地运行）'
- en: '**How?**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**如何？**'
- en: A lot of the hard work has already been done for us!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 很多艰巨的工作已经为我们完成了！
- en: The [llama.cpp project](https://github.com/ggerganov/llama.cpp) enables running
    *simplified* LLMs on CPUs by reducing the resolution ([“quantization”](https://ggml.ai/))
    of their numeric weights. These ready-to-use model files are [easily available](https://huggingface.co/TheBloke).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[llama.cpp 项目](https://github.com/ggerganov/llama.cpp) 通过减少其数值权重的分辨率（[“量化”](https://ggml.ai/)）来实现
    LLM 在 CPU 上的 *简化* 运行。这些现成的模型文件[易于获取](https://huggingface.co/TheBloke)。'
- en: Next, the [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) bindings
    provide simple access to using **llama.cpp** from within Python.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，[llama-cpp-python](https://github.com/abetlen/llama-cpp-python) 绑定提供了从 Python
    内部使用 **llama.cpp** 的简单方法。
- en: Finally, Spark’s `applyInPandas()` ([docs](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.GroupedData.applyInPandas.html))
    enables splitting giant data sources into [Pandas](https://pandas.pydata.org/)-sized
    chunks and processing them independently. Note that this approach *can* be an
    anti-pattern if vectorized Spark functions can accomplish the same result, but
    in our case, we’re basically using Spark as a simple orchestrator to [scale out](/the-beginners-guide-to-distributed-computing-6d6833796318)
    our **llama.cpp** usage. There’s likely more efficient ways to use **llama.cpp**
    in batch processing, but this one is attractive given the simplicity and automatic
    benefits of Spark’s fault tolerance and scalability.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Spark 的 `applyInPandas()` ([文档](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.GroupedData.applyInPandas.html))
    可以将巨大的数据源分割成 [Pandas](https://pandas.pydata.org/) 大小的块并独立处理。请注意，如果向量化的 Spark 函数能够实现相同的结果，这种方法
    *可能* 是一种反模式，但在我们的案例中，我们基本上是将 Spark 作为一个简单的协调器来[扩展](/the-beginners-guide-to-distributed-computing-6d6833796318)我们对
    **llama.cpp** 的使用。可能还有更高效的批处理方式来使用 **llama.cpp**，但考虑到 Spark 的容错和可扩展性的简易性和自动化好处，这种方式很有吸引力。
- en: Plan
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计划
- en: '![](../Images/9b9e891e959bf66a4db3f5e1fcbaeff5.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b9e891e959bf66a4db3f5e1fcbaeff5.png)'
- en: Creative Commons License ([CC BY-SA 3.0](https://commons.wikimedia.org/wiki/Category:War_and_Peace#/media/File:War_and_Peace_book.JPG))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 创意共享许可证 ([CC BY-SA 3.0](https://commons.wikimedia.org/wiki/Category:War_and_Peace#/media/File:War_and_Peace_book.JPG))
- en: 'As a fun test, we’ll be using Llama 2 to summarize Leo Tolstoy’s [War and Peace](https://gutenberg.org/cache/epub/2600/pg2600.txt),
    a 1200+ page novel with over 360 chapters. We’ll treat each chapter as a document.
    Note that Llama 2 already “knows” about the novel; asking it about a key character
    generates this output (using `llama-2–7b-chat.ggmlv3.q8_0.bin`):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个有趣的测试，我们将使用 Llama 2 来总结列夫·托尔斯泰的[《战争与和平》](https://gutenberg.org/cache/epub/2600/pg2600.txt)，这是一本超过
    1200 页的小说，包含超过 360 章。我们将把每章当作一个文档。注意，Llama 2 已经“知道”这部小说；询问它关于一个关键角色会生成以下输出（使用
    `llama-2–7b-chat.ggmlv3.q8_0.bin`）：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Steps:**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤：**'
- en: Install the 7B quantized chat model and **llama-cpp-python**.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 7B 量化聊天模型和 **llama-cpp-python**。
- en: Download the novel, split by chapter, create a Spark `DataFrame`.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载小说，按章节分割，创建一个 Spark `DataFrame`。
- en: Partition by chapter and generate summaries.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按章节分区并生成摘要。
- en: Installation
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装
- en: Configuring a Spark cluster is outside our scope; I’ll assume you have Spark
    running locally, through a managed service (like [Synapse](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-overview)
    or [Elastic Map Reduce](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark.html)),
    or a custom deployment like [Kubernetes](https://spark.apache.org/docs/3.1.1/running-on-kubernetes.html).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 Spark 集群超出了我们的范围；我将假设你已经在本地、通过托管服务（如 [Synapse](https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-overview)
    或 [Elastic Map Reduce](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark.html)）或像
    [Kubernetes](https://spark.apache.org/docs/3.1.1/running-on-kubernetes.html) 这样的自定义部署中运行了
    Spark。
- en: 'There are two artifacts that need installed on all *worker nodes*, whether
    those nodes are physical machines, VMs, or pods in a serverless pool:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个工件需要安装在所有 *工作节点* 上，无论这些节点是物理机器、虚拟机还是无服务器池中的容器：
- en: LLama 2 model in GGML format (located in `/models`)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GGML 格式的 LLama 2 模型（位于 `/models`）
- en: The **llama-cpp-python** module (installed via `pip`)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**llama-cpp-python** 模块（通过 `pip` 安装）'
- en: 'We’re using the 7B chat “Q8” version of Llama 2, found [here](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main).
    The download links might change, but a single-node, “bare metal” setup is similar
    to below:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的是 Llama 2 的 7B 聊天 “Q8” 版本，可以在 [这里](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main)
    找到。下载链接可能会变化，但单节点的“裸金属”设置类似于下面的内容：
- en: Ensure you can use the model via `python3` and [this example](https://github.com/abetlen/llama-cpp-python#high-level-api).
    To recap, *every* Spark context must be able to read the model from `/models`
    and access the **llama-cpp-python** module.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您可以通过 `python3` 使用模型，并查看 [这个示例](https://github.com/abetlen/llama-cpp-python#high-level-api)。总结一下，*每个*
    Spark 上下文必须能够从 `/models` 读取模型并访问 **llama-cpp-python** 模块。
- en: '**Processing the Novel Text**'
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**处理小说文本**'
- en: The Bash commands below download the novel and print word counts.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 Bash 命令下载小说并打印字数。
- en: Next, we read the text file in Python, removing the Project Gutenberg header
    and footer. We’ll split on the regex `CHAPTER .+` to create a list of chapter
    strings and create a Spark `DataFrame` from them (this code assumes a `SparkSession`
    named `spark`).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在 Python 中读取文本文件，去除 Project Gutenberg 的头部和尾部。我们将通过正则表达式 `CHAPTER .+` 进行分割，创建一个章节字符串的列表，并从中创建一个
    Spark `DataFrame`（此代码假设有一个名为 `spark` 的 `SparkSession`）。
- en: 'The code should produce the following output:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 代码应该产生以下输出：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Great! Now we have a `DataFrame` with 365 rows, each containing the full chapter
    text and number. The final step is creating a new `DataFrame` with summaries of
    each chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！现在我们有了一个包含 365 行的 `DataFrame`，每行都包含完整的章节文本和编号。最后一步是创建一个包含每章总结的新 `DataFrame`。
- en: Spark Processing
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 处理
- en: 'Below is the Python code for generating a **single chapter summary** (see the
    call to `limit(1)` to return a single row). Explanation below the snippet:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成 **单章总结** 的 Python 代码（请参见对 `limit(1)` 的调用以返回单行）。代码片段下方有解释：
- en: The `llama2_summarize()` function is the code that is applied *per-group* by
    Spark. Since we’re grouping by the `chapter` column, the function is called on
    each chapter row; the `df` argument is simply a **Pandas** `DataFrame` with a
    single row. Note that we’re reading the model for *every call* of `llama2_summarize()`;
    this is a shortcut we’re taking for simplicity, but not very efficient.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`llama2_summarize()` 函数是 Spark 按 *每组* 应用的代码。由于我们按 `chapter` 列分组，该函数会在每一章的行上调用；`df`
    参数仅仅是一个 **Pandas** `DataFrame`，其中包含一行数据。请注意，我们在 *每次调用* `llama2_summarize()` 时都会读取模型；这是为了简便采用的快捷方式，但效率并不是很高。'
- en: Finally, using Spark we do the `groupby()` and call `applyInPandas()`, setting
    the schema to include the chapter summary and number.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用 Spark 我们进行 `groupby()` 并调用 `applyInPandas()`，设置模式以包含章节总结和编号。
- en: 'The output (reformatted for readability) looks like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 输出（为了可读性进行重新格式化）如下所示：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: (Note the use of *Napoleon* despite the fact it doesn’t occur in the chapter!
    Again, this is a fun exercise rather than a realistic example using truly unseen
    documents.)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: （请注意使用 *拿破仑*，尽管它在章节中并不存在！再说一次，这是一个有趣的练习，而不是使用真正未见过的文档的现实示例。）
- en: The runtime for this single chapter test is about **2 minutes** on a 64-core
    VM. There are *many* choices we glossed over that affect runtime, such as model
    size/quantization and model parameters. The key result is that by scaling out
    our Spark cluster appropriately, we can summarize *all* chapters in a handful
    of minutes. Processing hundreds of thousands (or even millions!) of documents
    daily is thus possible using large Spark clusters comprised of cheap virtual machines.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单章测试的运行时间约为 **2 分钟**，在一个 64 核心的虚拟机上。我们略过了许多影响运行时间的选择，例如模型大小/量化和模型参数。关键结果是，通过适当扩展我们的
    Spark 集群，我们可以在几分钟内总结 *所有* 章节。因此，使用由廉价虚拟机组成的大型 Spark 集群，每天处理数十万（甚至百万！）的文档是可能的。
- en: Summary
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We haven’t even mentioned adjusting the standard LLM parameters like `temperature`
    and `top_p` which control the “creativity” and randomness of results, or *prompt
    engineering*, which is practically a discipline of its own. We also chose the
    Llama 2 7B model without justification; there might be smaller and more performant
    models or model families more suited to our particular use case.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至没有提到调整标准 LLM 参数如 `temperature` 和 `top_p`，这些参数控制结果的“创造力”和随机性，或者 *提示工程*，这几乎是一门独立的学科。我们也选择了
    Llama 2 7B 模型而没有说明理由；可能还有更小且性能更好的模型或模型系列，更适合我们的特定用例。
- en: 'Instead, we’ve shown how to easily distribute (quantized) LLM workloads using
    Spark with fairly minimal effort. Next steps might include:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们展示了如何使用 Spark 轻松分发（量化）LLM 工作负载，只需付出相当少的努力。接下来的步骤可能包括：
- en: More efficient load/caching of models
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高效的模型加载/缓存
- en: Parameter optimization for different use cases
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对不同使用场景的参数优化
- en: Custom prompts
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义提示
