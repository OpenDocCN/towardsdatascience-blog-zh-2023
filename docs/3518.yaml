- en: Implementing Soft Nearest Neighbor Loss in PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中实现软最近邻损失
- en: 原文：[https://towardsdatascience.com/implementing-soft-nearest-neighbor-loss-in-pytorch-b9ed2a371760?source=collection_archive---------4-----------------------#2023-11-27](https://towardsdatascience.com/implementing-soft-nearest-neighbor-loss-in-pytorch-b9ed2a371760?source=collection_archive---------4-----------------------#2023-11-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/implementing-soft-nearest-neighbor-loss-in-pytorch-b9ed2a371760?source=collection_archive---------4-----------------------#2023-11-27](https://towardsdatascience.com/implementing-soft-nearest-neighbor-loss-in-pytorch-b9ed2a371760?source=collection_archive---------4-----------------------#2023-11-27)
- en: The class neighborhood of a dataset can be learned using soft nearest neighbor
    loss
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集的类邻域可以通过软最近邻损失来学习
- en: '[](https://medium.com/@afagarap?source=post_page-----b9ed2a371760--------------------------------)[![Abien
    Fred Agarap](../Images/8f616478044e721a31c1c1df3d8e8b62.png)](https://medium.com/@afagarap?source=post_page-----b9ed2a371760--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b9ed2a371760--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b9ed2a371760--------------------------------)
    [Abien Fred Agarap](https://medium.com/@afagarap?source=post_page-----b9ed2a371760--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@afagarap?source=post_page-----b9ed2a371760--------------------------------)[![Abien
    Fred Agarap](../Images/8f616478044e721a31c1c1df3d8e8b62.png)](https://medium.com/@afagarap?source=post_page-----b9ed2a371760--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b9ed2a371760--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b9ed2a371760--------------------------------)
    [Abien Fred Agarap](https://medium.com/@afagarap?source=post_page-----b9ed2a371760--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F782adfd45f71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-soft-nearest-neighbor-loss-in-pytorch-b9ed2a371760&user=Abien+Fred+Agarap&userId=782adfd45f71&source=post_page-782adfd45f71----b9ed2a371760---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b9ed2a371760--------------------------------)
    ·8 min read·Nov 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb9ed2a371760&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-soft-nearest-neighbor-loss-in-pytorch-b9ed2a371760&user=Abien+Fred+Agarap&userId=782adfd45f71&source=-----b9ed2a371760---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F782adfd45f71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-soft-nearest-neighbor-loss-in-pytorch-b9ed2a371760&user=Abien+Fred+Agarap&userId=782adfd45f71&source=post_page-782adfd45f71----b9ed2a371760---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b9ed2a371760--------------------------------)
    ·8分钟阅读·2023年11月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb9ed2a371760&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-soft-nearest-neighbor-loss-in-pytorch-b9ed2a371760&user=Abien+Fred+Agarap&userId=782adfd45f71&source=-----b9ed2a371760---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb9ed2a371760&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-soft-nearest-neighbor-loss-in-pytorch-b9ed2a371760&source=-----b9ed2a371760---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb9ed2a371760&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimplementing-soft-nearest-neighbor-loss-in-pytorch-b9ed2a371760&source=-----b9ed2a371760---------------------bookmark_footer-----------)'
- en: In this article, we discuss how to implement the soft nearest neighbor loss
    which we also talked about [here](https://medium.com/towards-data-science/improving-k-means-clustering-with-disentanglement-caf59a8c57bd).
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了如何实现软最近邻损失，我们也在[这里](https://medium.com/towards-data-science/improving-k-means-clustering-with-disentanglement-caf59a8c57bd)谈到过这个话题。
- en: Representation learning is the task of learning the most salient features in
    a given dataset by a deep neural network. It is usually an implicit task done
    in a supervised learning paradigm, and it is a crucial factor in the success of
    deep learning ([Krizhevsky et al., 2012](https://dl.acm.org/doi/abs/10.1145/3065386);
    [He et al., 2016](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html);
    [Simonyan et al., 2014](https://arxiv.org/abs/1409.1556)). In other words, representation
    learning automates the process of feature extraction. With this, we can use the
    learned representations for downstream tasks such as classification, regression,
    and synthesis.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 表示学习是通过深度神经网络学习给定数据集中最显著特征的任务。通常这是在监督学习范式中隐式完成的任务，并且是深度学习成功的关键因素 ([Krizhevsky
    et al., 2012](https://dl.acm.org/doi/abs/10.1145/3065386)；[He et al., 2016](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)；[Simonyan
    et al., 2014](https://arxiv.org/abs/1409.1556))。换句话说，表示学习自动化了特征提取过程。通过这个过程，我们可以将学到的表示用于下游任务，如分类、回归和合成。
- en: '![](../Images/dffcba9ed212b05553d9160844edf567.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dffcba9ed212b05553d9160844edf567.png)'
- en: '*Figure 1\. Illustration from SNNL (*[*Frosst et al., 2019*](http://proceedings.mlr.press/v97/frosst19a/frosst19a.pdf)*).
    By minimizing the soft nearest neighbor loss, the distances among class-similar
    data points (as indicated by their color) are minimized while the distances among
    class-different data points are maximized.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1\. 来源于 SNNL (*[*Frosst et al., 2019*](http://proceedings.mlr.press/v97/frosst19a/frosst19a.pdf)*).
    通过最小化软最近邻损失，类相似数据点（如其颜色所示）之间的距离被最小化，而类不同数据点之间的距离被最大化。*'
- en: We can also influence how the learned representations are formed to cater specific
    use cases. In the case of classification, the representations are primed to have
    data points from the same class to flock together, while for generation (e.g.
    in GANs), the representations are primed to have points of real data flock with
    the synthesized ones.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以影响学习到的表示的形成，以适应特定的应用场景。在分类的情况下，表示会被调整使得同一类的数据点聚集在一起，而在生成（例如在 GAN 中）中，表示会被调整使得真实数据点与合成数据点聚集在一起。
- en: In the same sense, we have enjoyed the use of principal components analysis
    (PCA) to encode features for downstream tasks. However, we do not have any class
    or label information in PCA-encoded representations, hence the performance on
    downstream tasks may be further improved. We can improve the encoded representations
    by approximating the class or label information in it by learning the neighborhood
    structure of the dataset, i.e. which features are clustered together, and such
    clusters would imply that the features belong to the same class as per the clustering
    assumption in the semi-supervised learning literature ([Chapelle et al., 2009](https://ieeexplore.ieee.org/abstract/document/4787647)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们也享受了使用主成分分析（PCA）来编码特征以用于下游任务。然而，在 PCA 编码的表示中没有任何类或标签信息，因此在下游任务上的表现可能会进一步提升。我们可以通过学习数据集的邻域结构来改进编码的表示，即哪些特征被聚集在一起，这样的聚集会暗示这些特征属于同一类，如半监督学习文献中的聚类假设所示
    ([Chapelle et al., 2009](https://ieeexplore.ieee.org/abstract/document/4787647))。
- en: To integrate the neighborhood structure in the representations, manifold learning
    techniques have been introduced such as locally linear embeddings or LLE ([Roweis
    & Saul, 2000](https://www.science.org/doi/abs/10.1126/science.290.5500.2323)),
    neighborhood components analysis or NCA ([Hinton et al., 2004](https://proceedings.neurips.cc/paper_files/paper/2004/file/42fe880812925e520249e808937738d2-Paper.pdf)),
    and t-stochastic neighbor embedding or t-SNE ([Maaten & Hinton, 2008](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf?fbcl=)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将邻域结构整合进表示中，已经引入了流形学习技术，如局部线性嵌入（LLE）([Roweis & Saul, 2000](https://www.science.org/doi/abs/10.1126/science.290.5500.2323)）、邻域组件分析（NCA）([Hinton
    et al., 2004](https://proceedings.neurips.cc/paper_files/paper/2004/file/42fe880812925e520249e808937738d2-Paper.pdf)）和t-随机邻域嵌入（t-SNE）([Maaten
    & Hinton, 2008](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf?fbcl=))。
- en: However, the aforementioned manifold learning techniques have their own drawbacks.
    For instance, both LLE and NCA encode linear embeddings instead of nonlinear embeddings.
    Meanwhile, t-SNE embeddings result to different structures depending on the hyperparameters
    used.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述流形学习技术各有其缺点。例如，LLE 和 NCA 编码的是线性嵌入，而非非线性嵌入。同时，t-SNE 嵌入的结构依赖于所使用的超参数。
- en: To avoid such drawbacks, we can use an improved NCA algorithm which is the *soft
    nearest neighbor loss* or SNNL ([Salakhutdinov & Hinton, 2007](http://proceedings.mlr.press/v2/salakhutdinov07a/salakhutdinov07a.pdf);
    [Frosst et al., 2019](https://proceedings.mlr.press/v97/frosst19a.html)). The
    SNNL improves the NCA algorithm by introducing nonlinearity, and it is computed
    for each hidden layer of a neural network instead of solely on the last encoding
    layer. This loss function is used to optimize the *entanglement* of points in
    a dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种缺陷，我们可以使用改进的NCA算法，即*软最近邻损失*或SNNL（[Salakhutdinov & Hinton, 2007](http://proceedings.mlr.press/v2/salakhutdinov07a/salakhutdinov07a.pdf);
    [Frosst et al., 2019](https://proceedings.mlr.press/v97/frosst19a.html)）。SNNL通过引入非线性改进了NCA算法，它是在神经网络的每一隐藏层上计算的，而不仅仅是最后的编码层。此损失函数用于优化数据集中点的*纠缠*。
- en: In this context, *entanglement* is defined as how close class-similar data points
    to each other are compared to class-different data points. A low entanglement
    means that class-similar data points are much closer to each other than class-different
    data points (see Figure 1). Having such a set of data points will render downstream
    tasks much easier to accomplish with an even better performance. Frosst et al.
    (2019) expanded the SNNL objective by introducing a temperature factor *T*. Thus
    giving us the following as the final loss function,
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*纠缠*定义为类相似的数据点彼此之间的接近程度，相较于类不同的数据点。低纠缠意味着类相似的数据点彼此之间要比类不同的数据点更接近（见图1）。拥有这样的数据点集合将使下游任务更容易完成，且性能更佳。Frosst等人（2019）通过引入温度因子*T*扩展了SNNL目标。因此，我们得到以下最终损失函数，
- en: '![](../Images/a50f465990d5f1e33763426cb103a67c.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a50f465990d5f1e33763426cb103a67c.png)'
- en: Figure 2\. The soft nearest neighbor loss function. Figure by the author.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 软最近邻损失函数。图由作者提供。
- en: where *d* is a distance metric on either raw input features or hidden layer
    representations of a neural network, and *T* is the temperature factor that is
    directly proportional to the distances among data points in a hidden layer. For
    this implementation, we use the cosine distance as our distance metric for more
    stable computations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*d*是原始输入特征或神经网络隐藏层表示上的距离度量，*T*是与隐藏层中数据点之间的距离直接成正比的温度因子。对于此实现，我们使用余弦距离作为我们的距离度量，以获得更稳定的计算。
- en: '![](../Images/b86cead71297c76297474ae763bb464a.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b86cead71297c76297474ae763bb464a.png)'
- en: Figure 3\. The cosine distance formula. Figure by the author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 余弦距离公式。图由作者提供。
- en: The purpose of this article is to help readers understand and implement the
    soft nearest neighbor loss, and so we shall dissect the loss function in order
    to understand it better.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是帮助读者理解和实现软最近邻损失，因此我们将详细分析损失函数以更好地理解它。
- en: Distance Metric
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 距离度量
- en: The first thing we should compute are the distances among data points, that
    are either the raw input features or hidden layer representations of the network.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该首先计算的是数据点之间的距离，这些距离可以是原始输入特征或网络的隐藏层表示。
- en: '![](../Images/e85ff680e27cec97a8863df628054780.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e85ff680e27cec97a8863df628054780.png)'
- en: Figure 4\. The first step in computing SNNL is to compute the distance metric
    for the input data points. Figure by the author.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 计算SNNL的第一步是计算输入数据点的距离度量。图由作者提供。
- en: 'For our implementation, we use the cosine distance metric (Figure 3) for more
    stable computations. At the time being, let us ignore the denoted subsets *ij*
    and *ik* in the figure above, and let us just focus on computing the cosine distance
    among our input data points. We accomplish this through the following PyTorch
    code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实现，我们使用余弦距离度量（图3）以获得更稳定的计算。暂时，我们忽略上图中标记的子集*ij*和*ik*，只专注于计算输入数据点之间的余弦距离。我们通过以下PyTorch代码实现：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the code snippet above, we first normalize the input features in lines 1
    and 2 using Euclidean norm. Then in line 3, we get the conjugate transpose of
    the second set of the normalized input features. We compute the conjugate transpose
    to [account for complex vectors](https://math.stackexchange.com/questions/273527/cosine-similarity-between-complex-vectors).
    In lines 4 and 5, we compute the cosine similarity and distance of the input features.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码片段中，我们首先在第1和第2行使用欧几里得范数对输入特征进行归一化。然后在第3行，我们获取归一化输入特征第二组的共轭转置。我们计算共轭转置以[考虑复数向量](https://math.stackexchange.com/questions/273527/cosine-similarity-between-complex-vectors)。在第4和第5行，我们计算输入特征的余弦相似度和距离。
- en: Concretely, consider the following set of features,
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，考虑以下特征集，
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the distance metric we defined above, we gain the following distance matrix,
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们上面定义的距离度量，我们得到以下距离矩阵，
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Sampling Probability
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采样概率
- en: We can now compute the matrix that represents the probability of picking each
    feature given its pairwise distances to all other features. This is simply the
    probability of picking *i* points based on the distances between *i* and *j* or
    *k* points.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算表示选择每个特征的概率的矩阵，给定其与所有其他特征的成对距离。这仅仅是选择*i*点的概率，基于*i*与*j*或*k*点之间的距离。
- en: '![](../Images/f8b096316279538b34805a0ceec0be1b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8b096316279538b34805a0ceec0be1b.png)'
- en: Figure 5\. The second step is to compute the sampling probability of picking
    points based on their distances. Figure by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图5。第二步是计算基于距离的采样概率。图由作者提供。
- en: 'We can compute this through the following code:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码计算这一点：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The code first calculates the exponential of the negative of the distance matrix
    divided by the temperature factor, scaling the values to positive values. The
    temperature factor dictates how to control the importance given to the distances
    between pairs of points, for instance, at low temperatures, the loss is dominated
    by small distances while actual distances between widely separated representations
    become less relevant.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先计算距离矩阵的负指数除以温度因子，将值缩放到正值。温度因子决定如何控制给定点对之间距离的重要性，例如，在低温下，损失由小距离主导，而广泛分隔表示之间的实际距离变得不那么重要。
- en: Prior to the subtraction of `torch.eye(features.shape[0])` (aka diagonal matrix),
    the tensor was as follows,
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在减去`torch.eye(features.shape[0])`（即对角矩阵）之前，张量如下，
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We subtract a diagonal matrix from the distance matrix to remove all self-similarity
    terms (i.e. the distance or similarity of each point to itself).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从距离矩阵中减去一个对角矩阵，以去除所有自相似项（即每个点到自身的距离或相似度）。
- en: 'Next, we can compute the sampling probability for each pair of data points
    through the following code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以通过以下代码计算每对数据点的采样概率：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Masked Sampling Probability
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩码采样概率
- en: So far, the sampling probability we have computed does not contain any label
    information. We integrate the label information into the sampling probability
    by masking it with the dataset labels.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们计算的采样概率不包含任何标签信息。我们通过用数据集标签掩盖采样概率来整合标签信息。
- en: '![](../Images/82e4c0d5d90bba3e2035e5668cff0b57.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82e4c0d5d90bba3e2035e5668cff0b57.png)'
- en: Figure 6\. We use the label information to isolate the probabilities for points
    belonging to the same class. Figure by the author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图6。我们利用标签信息来隔离属于同一类别的点的概率。图由作者提供。
- en: 'First, we have to derive a pairwise matrix out of the label vectors:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须从标签向量中推导出一个成对矩阵：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We apply the masking matrix to use the label information to isolate the probabilities
    for points that belong to the same class:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用掩码矩阵，利用标签信息来隔离属于同一类别的点的概率：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we compute the sum probability for sampling a particular feature by computing
    the sum of the masked sampling probability per row,
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过计算每行的掩码采样概率的总和来计算特定特征的总采样概率，
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Finally, we can compute the logarithm of the sum of the sampling probabilities
    for features for computational convenience with an additional computational stability
    variable, and get the average to act as the nearest neighbor loss for the network,
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以计算采样概率总和的对数，为计算便利性添加额外的计算稳定变量，并求平均作为网络的最近邻损失，
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can now string these components together in a forward pass function to compute
    the soft nearest neighbor loss across all layers of a deep neural network,
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将这些组件组合在一起，在前向传递函数中计算整个深度神经网络的软最近邻损失，
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Visualizing Disentangled Representations
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化解耦表示
- en: We trained an autoencoder with the soft nearest neighbor loss, and visualize
    its learned disentangled representations. The autoencoder had (x-500–500–2000-d-2000–500–500-x)
    units, and was trained on a small labelled subset of the MNIST, Fashion-MNIST,
    and EMNIST-Balanced datasets. This is to simulate the scarcity of labelled examples
    since autoencoders are supposed to be unsupervised models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用软最近邻损失训练了一个自编码器，并可视化了其学习到的解缠表示。该自编码器包含 (x-500–500–2000-d-2000–500–500-x)
    单元，并在 MNIST、Fashion-MNIST 和 EMNIST-Balanced 数据集的小型标注子集上进行训练。这是为了模拟标注样本的稀缺性，因为自编码器通常是无监督模型。
- en: '![](../Images/87e9b4cbadd1d80a4990a3e4a11f456f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87e9b4cbadd1d80a4990a3e4a11f456f.png)'
- en: Figure 7\. 3D visualization comparing the original representation and the disentangled
    latent representation of the three datasets. To achieve this visualization, the
    representations were encoded using t-SNE with perplexity = 50 and learning rate
    = 10, optimized for 5,000 iterations. Figure by the author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7. 3D 可视化比较了三个数据集的原始表示和解缠潜在表示。为了实现这种可视化，表示使用 t-SNE 编码，困惑度 = 50，学习率 = 10，优化
    5000 次迭代。图由作者提供。
- en: We only visualized an arbitrarily chosen 10 clusters for easier and cleaner
    visualization of the EMNIST-Balanced dataset. We can see in the figure above that
    the latent code representation became more clustering-friendly by having a set
    of well-defined clusters as indicated by cluster dispersion and correct cluster
    assignments as indicated by cluster colors.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅可视化了任意选择的 10 个簇，以便更简单、更清晰地展示 EMNIST-Balanced 数据集。从上图可以看出，潜在代码表示变得更适合聚类，通过集群分散度和正确的集群分配（由集群颜色指示）体现了良好的定义簇。
- en: Closing Remarks
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束语
- en: In this article, we dissected the soft nearest neighbor loss function as to
    how we could implement it in PyTorch.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们详细分析了软最近邻损失函数，并讨论了如何在 PyTorch 中实现它。
- en: The soft nearest neighbor loss was first introduced by [Salakhutdinov & Hinton
    (2007)](http://proceedings.mlr.press/v2/salakhutdinov07a.html) where it was used
    to compute the loss on the latent code (bottleneck) representation of an autoencoder,
    and then the said representation was used for downstream kNN classification task.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 软最近邻损失首次由 [Salakhutdinov & Hinton (2007)](http://proceedings.mlr.press/v2/salakhutdinov07a.html)
    引入，用于计算自编码器潜在代码（瓶颈）表示上的损失，然后将该表示用于下游的 kNN 分类任务。
- en: '[Frosst, Papernot, & Hinton (2019)](https://proceedings.mlr.press/v97/frosst19a.html)
    then expanded the soft nearest neighbor loss by introducing a temperature factor
    and by computing the loss across all layers of a neural network.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[Frosst, Papernot, & Hinton (2019)](https://proceedings.mlr.press/v97/frosst19a.html)
    通过引入温度因子并计算神经网络所有层的损失，扩展了软最近邻损失。'
- en: Finally, we employed an annealing temperature factor for the soft nearest neighbor
    loss to further improve the learned disentangled representations of a network,
    and also speed up the disentanglement process ([Agarap & Azcarraga, 2020](https://arxiv.org/abs/2006.04535)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们采用了退火温度因子来进一步改善网络的学习解缠表示，并加速解缠过程 ([Agarap & Azcarraga, 2020](https://arxiv.org/abs/2006.04535))。
- en: The full code implementation is available in [GitLab](https://gitlab.com/afagarap/pt-snnl).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码实现可在 [GitLab](https://gitlab.com/afagarap/pt-snnl) 上获得。
- en: References
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Agarap, Abien Fred, and Arnulfo P. Azcarraga. “Improving k-means clustering
    performance with disentangled internal representations.” *2020 International Joint
    Conference on Neural Networks (IJCNN)*. IEEE, 2020.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarap, Abien Fred, 和 Arnulfo P. Azcarraga. “通过解缠内部表示来改善 k-means 聚类性能。” *2020
    国际神经网络联合会议 (IJCNN)*. IEEE, 2020.
- en: 'Chapelle, Olivier, Bernhard Scholkopf, and Alexander Zien. “Semi-supervised
    learning (chapelle, o. et al., eds.; 2006)[book reviews].” *IEEE Transactions
    on Neural Networks* 20.3 (2009): 542–542.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chapelle, Olivier, Bernhard Scholkopf 和 Alexander Zien. “半监督学习 (chapelle, o.
    等, 编；2006)[书评]。” *IEEE 神经网络交易* 20.3 (2009): 542–542.'
- en: Frosst, Nicholas, Nicolas Papernot, and Geoffrey Hinton. “Analyzing and improving
    representations with the soft nearest neighbor loss.” *International conference
    on machine learning*. PMLR, 2019.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frosst, Nicholas, Nicolas Papernot 和 Geoffrey Hinton. “分析和改进软最近邻损失的表示。” *国际机器学习会议*.
    PMLR, 2019.
- en: Goldberger, Jacob, et al. “Neighbourhood components analysis.” *Advances in
    neural information processing systems*. 2005.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldberger, Jacob 等. “邻域组件分析。” *神经信息处理系统进展*. 2005.
- en: He, Kaiming, et al. “Deep residual learning for image recognition.” *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 2016.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, Kaiming, 等. “用于图像识别的深度残差学习。” *IEEE 计算机视觉与模式识别会议论文集*。2016年。
- en: Hinton, G., et al. “Neighborhood components analysis.” *Proc. NIPS*. 2004.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton, G., 等. “邻域组件分析。” *NIPS 会议论文集*。2004年。
- en: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification
    with deep convolutional neural networks.” *Advances in neural information processing
    systems* 25 (2012).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky, Alex, Ilya Sutskever, 和 Geoffrey E. Hinton. “使用深度卷积神经网络进行 ImageNet
    分类。” *神经信息处理系统进展* 25 (2012)。
- en: 'Roweis, Sam T., and Lawrence K. Saul. “Nonlinear dimensionality reduction by
    locally linear embedding.” *science* 290.5500 (2000): 2323–2326.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roweis, Sam T., 和 Lawrence K. Saul. “通过局部线性嵌入进行非线性维度约简。” *科学* 290.5500 (2000):
    2323–2326。'
- en: Salakhutdinov, Ruslan, and Geoff Hinton. “Learning a nonlinear embedding by
    preserving class neighbourhood structure.” *Artificial Intelligence and Statistics*.
    2007.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salakhutdinov, Ruslan, 和 Geoff Hinton. “通过保留类别邻域结构来学习非线性嵌入。” *人工智能与统计学*。2007年。
- en: Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for
    large-scale image recognition.” *arXiv preprint arXiv:1409.1556* (2014).
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan, Karen, 和 Andrew Zisserman. “用于大规模图像识别的非常深的卷积网络。” *arXiv 预印本 arXiv:1409.1556*
    (2014)。
- en: Van der Maaten, Laurens, and Geoffrey Hinton. “Visualizing data using t-SNE.”
    *Journal of machine learning research* 9.11 (2008).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van der Maaten, Laurens, 和 Geoffrey Hinton. “使用 t-SNE 进行数据可视化。” *机器学习研究杂志* 9.11
    (2008)。
