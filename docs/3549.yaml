- en: Combine Multiple LoRA Adapters for Llama 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/combine-multiple-lora-adapters-for-llama-2-ea0bef9025cf?source=collection_archive---------4-----------------------#2023-11-30](https://towardsdatascience.com/combine-multiple-lora-adapters-for-llama-2-ea0bef9025cf?source=collection_archive---------4-----------------------#2023-11-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Add skills to your LLM without fine-tuning new adapters
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----ea0bef9025cf--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----ea0bef9025cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea0bef9025cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea0bef9025cf--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----ea0bef9025cf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombine-multiple-lora-adapters-for-llama-2-ea0bef9025cf&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----ea0bef9025cf---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea0bef9025cf--------------------------------)
    ·12 min read·Nov 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fea0bef9025cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombine-multiple-lora-adapters-for-llama-2-ea0bef9025cf&user=Benjamin+Marie&userId=ad2a414578b3&source=-----ea0bef9025cf---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea0bef9025cf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcombine-multiple-lora-adapters-for-llama-2-ea0bef9025cf&source=-----ea0bef9025cf---------------------bookmark_footer-----------)![](../Images/6c4a40091827bce0f2a546522f563d4a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Image by the author — Made with an image from [Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Fully fine-tuning a pre-trained large language model (LLM) for different tasks
    is very costly. Instead, we can freeze the parameters of the LLM while only fine-tuning
    a few million trainable parameters added through a LoRA adapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we only need to fine-tune an adapter to get the model to perform
    a target task. For instance, if we want to turn a pre-trained LLM into a translation
    model, we would fine-tune an adapter for translation. We can fine-tune one adapter
    for each ask that we want the LLM to perform.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '*But can we combine several adapters to get one single multi-task adapter?*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*但是我们能否将几个适配器组合成一个单一的多任务适配器？*'
- en: For instance, if we have one adapter for translation and one adapter for summarization,
    can we combine both of them so that the LLM can do translation and summarization?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，如果我们有一个用于翻译的适配器和一个用于摘要的适配器，我们能否将它们组合起来，以便 LLM 能够进行翻译和摘要？
- en: In this article, I show how to combine multiple LoRA adapters into a single
    multi-task adapter. We will see that it is very simple and that the resulting
    adapter can be as good as the adapters used for the combination.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我展示了如何将多个 LoRA 适配器组合成一个多任务适配器。我们将看到这非常简单，并且最终的适配器可以和用于组合的适配器一样好。
- en: Using Llama 2 7B, we will see how to combine an adapter fine-tuned for translation
    with another adapter fine-tuned for chat. With the resulting adapter, we will
    be able to make a…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Llama 2 7B，我们将看到如何将一个针对翻译的微调适配器与另一个针对聊天的微调适配器结合起来。通过最终的适配器，我们将能够进行……
