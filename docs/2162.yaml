- en: 'T5: Text-to-Text Transformers (Part Two)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/t5-text-to-text-transformers-part-two-837ba23a9eb4?source=collection_archive---------12-----------------------#2023-07-05](https://towardsdatascience.com/t5-text-to-text-transformers-part-two-837ba23a9eb4?source=collection_archive---------12-----------------------#2023-07-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Optimal transfer learning for large language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----837ba23a9eb4--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----837ba23a9eb4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----837ba23a9eb4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----837ba23a9eb4--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----837ba23a9eb4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transformers-part-two-837ba23a9eb4&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----837ba23a9eb4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----837ba23a9eb4--------------------------------)
    ·14 min read·Jul 5, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F837ba23a9eb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transformers-part-two-837ba23a9eb4&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----837ba23a9eb4---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F837ba23a9eb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ft5-text-to-text-transformers-part-two-837ba23a9eb4&source=-----837ba23a9eb4---------------------bookmark_footer-----------)![](../Images/16eb0b06a8e38a4121e6c959124618c5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/text?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  prefs: []
  type: TYPE_NORMAL
- en: 'The proposal of [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [5] led to the popularization of transfer learning approaches for natural language
    processing (NLP). Due to the widespread availability of unlabeled text on the
    internet, we could easily *(i)* pre-train large transformer models over large
    amounts of raw text and *(ii)* fine-tune these models to accurately solve downstream
    tasks. This approach was incredibly effective, but its newfound popularity led
    many alternative methods and modifications to be proposed. With all these new
    methods becoming available, one could easily begin to wonder: *What are the best
    practices for transfer learning in NLP?*'
  prefs: []
  type: TYPE_NORMAL
- en: This question was answered by analysis performed with the unified text-to-text
    transformer (T5) model. T5 reformulates all tasks (during both pre-training and
    fine-tuning) with a text-to-text format, meaning that the model receives textual
    input and produces textual output. Using this unified format, T5 can analyze various
    different transfer learning settings, allowing many approaches to be compared.
    In a [previous newsletter](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part),
    we learned about the format, architecture, and overall approach of the T5 model.
  prefs: []
  type: TYPE_NORMAL
- en: In this newsletter, we will outline the analysis performed by T5, including
    an empirical comparison different pre-training…
  prefs: []
  type: TYPE_NORMAL
