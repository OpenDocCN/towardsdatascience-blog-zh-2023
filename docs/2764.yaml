- en: Auto-Tuning for Deep Neural Network Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/auto-tuning-for-deep-neural-network-deployment-ff2324cb41d?source=collection_archive---------6-----------------------#2023-09-01](https://towardsdatascience.com/auto-tuning-for-deep-neural-network-deployment-ff2324cb41d?source=collection_archive---------6-----------------------#2023-09-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What, why, and most importantly… how?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pecciaf.medium.com/?source=post_page-----ff2324cb41d--------------------------------)[![Federico
    Peccia](../Images/48ad8401c28e87717718f58336cc64cf.png)](https://pecciaf.medium.com/?source=post_page-----ff2324cb41d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ff2324cb41d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ff2324cb41d--------------------------------)
    [Federico Peccia](https://pecciaf.medium.com/?source=post_page-----ff2324cb41d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce527bed0faf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fauto-tuning-for-deep-neural-network-deployment-ff2324cb41d&user=Federico+Peccia&userId=ce527bed0faf&source=post_page-ce527bed0faf----ff2324cb41d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ff2324cb41d--------------------------------)
    ·7 min read·Sep 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fff2324cb41d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fauto-tuning-for-deep-neural-network-deployment-ff2324cb41d&user=Federico+Peccia&userId=ce527bed0faf&source=-----ff2324cb41d---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff2324cb41d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fauto-tuning-for-deep-neural-network-deployment-ff2324cb41d&source=-----ff2324cb41d---------------------bookmark_footer-----------)![](../Images/58fccebb5870c0dfc860e5b9927f920f.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [S. Tsuchiya](https://unsplash.com/@s_tsuchiya?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the metrics used to compare different Neural Network (NN) architectures
    is the time it takes to train them. Does it take hours? Days? *Weeks*? Usually,
    this can be improved just by updating the hardware used to train them. Replace
    lesser GPUs with more powerful ones, parallelize the training across multiple
    GPUs, etc. Something similar happens with the inference step. Will we deploy our
    trained network on an embedded device, like a microcontroller? Or are we going
    to run it on a mobile device? Perhaps the network is just too big, and we need
    an embedded GPU or even a server-size GPU to execute it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's select one of them. We take our NN, compile it for our device, and test
    how fast it runs. Oh no! It does not meet our latency requirements! We needed
    the NN to run faster than 1 second, and our NN took 2 seconds to run! What are
    the options now?
  prefs: []
  type: TYPE_NORMAL
- en: '**Replace the device with a more powerful one:** This can be very problematic,
    specifically when there are hard application constraints. Perhaps you are only
    allowed to use specific, already certified hardware. Or you have difficult energy
    constraints to meet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduce the complexity of the NN:** This may also be difficult because you
    can lose…'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
