- en: Auto-Tuning for Deep Neural Network Deployment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/auto-tuning-for-deep-neural-network-deployment-ff2324cb41d?source=collection_archive---------6-----------------------#2023-09-01](https://towardsdatascience.com/auto-tuning-for-deep-neural-network-deployment-ff2324cb41d?source=collection_archive---------6-----------------------#2023-09-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What, why, and most importantly… how?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pecciaf.medium.com/?source=post_page-----ff2324cb41d--------------------------------)[![Federico
    Peccia](../Images/48ad8401c28e87717718f58336cc64cf.png)](https://pecciaf.medium.com/?source=post_page-----ff2324cb41d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ff2324cb41d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ff2324cb41d--------------------------------)
    [Federico Peccia](https://pecciaf.medium.com/?source=post_page-----ff2324cb41d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce527bed0faf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fauto-tuning-for-deep-neural-network-deployment-ff2324cb41d&user=Federico+Peccia&userId=ce527bed0faf&source=post_page-ce527bed0faf----ff2324cb41d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ff2324cb41d--------------------------------)
    ·7 min read·Sep 1, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fff2324cb41d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fauto-tuning-for-deep-neural-network-deployment-ff2324cb41d&user=Federico+Peccia&userId=ce527bed0faf&source=-----ff2324cb41d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fff2324cb41d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fauto-tuning-for-deep-neural-network-deployment-ff2324cb41d&source=-----ff2324cb41d---------------------bookmark_footer-----------)![](../Images/58fccebb5870c0dfc860e5b9927f920f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [S. Tsuchiya](https://unsplash.com/@s_tsuchiya?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the metrics used to compare different Neural Network (NN) architectures
    is the time it takes to train them. Does it take hours? Days? *Weeks*? Usually,
    this can be improved just by updating the hardware used to train them. Replace
    lesser GPUs with more powerful ones, parallelize the training across multiple
    GPUs, etc. Something similar happens with the inference step. Will we deploy our
    trained network on an embedded device, like a microcontroller? Or are we going
    to run it on a mobile device? Perhaps the network is just too big, and we need
    an embedded GPU or even a server-size GPU to execute it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 用来比较不同神经网络（NN）架构的指标之一是它们的训练时间。需要几个小时？几天？*几周*？通常情况下，通过更新用于训练的硬件可以改善这一点。用更强大的GPU替换较弱的GPU，将训练并行化到多个GPU等等。推理步骤也有类似的情况。我们会将训练好的网络部署到嵌入式设备上，比如微控制器吗？还是要在移动设备上运行它？也许网络太大了，我们需要一个嵌入式GPU甚至是服务器规模的GPU来执行它。
- en: Let's select one of them. We take our NN, compile it for our device, and test
    how fast it runs. Oh no! It does not meet our latency requirements! We needed
    the NN to run faster than 1 second, and our NN took 2 seconds to run! What are
    the options now?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选其中一个。我们拿出我们的NN，为我们的设备编译它，然后测试它的运行速度。哦，不好了！它不能满足我们的延迟要求！我们需要NN在1秒内运行，而我们的NN需要2秒！现在有什么选择呢？
- en: '**Replace the device with a more powerful one:** This can be very problematic,
    specifically when there are hard application constraints. Perhaps you are only
    allowed to use specific, already certified hardware. Or you have difficult energy
    constraints to meet.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用更强大的设备替换：** 这可能会带来很大问题，特别是当存在严格的应用约束条件时。也许你只能使用特定的已认证硬件。或者你有难以满足的能量限制。'
- en: '**Reduce the complexity of the NN:** This may also be difficult because you
    can lose…'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少NN的复杂性：** 这也可能很难，因为你可能会失去...'
