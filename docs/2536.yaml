- en: Regulating Generative AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监管生成式AI
- en: 原文：[https://towardsdatascience.com/regulating-generative-ai-e8b22525d71a?source=collection_archive---------4-----------------------#2023-08-08](https://towardsdatascience.com/regulating-generative-ai-e8b22525d71a?source=collection_archive---------4-----------------------#2023-08-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/regulating-generative-ai-e8b22525d71a?source=collection_archive---------4-----------------------#2023-08-08](https://towardsdatascience.com/regulating-generative-ai-e8b22525d71a?source=collection_archive---------4-----------------------#2023-08-08)
- en: How Well Do LLMs Comply with the EU AI Act?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）在多大程度上遵守欧盟AI法案？
- en: '[](https://medium.com/@davidsweenor?source=post_page-----e8b22525d71a--------------------------------)[![David
    Sweenor](../Images/7dbb5c549ab67bc78f906fb707969ff6.png)](https://medium.com/@davidsweenor?source=post_page-----e8b22525d71a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e8b22525d71a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e8b22525d71a--------------------------------)
    [David Sweenor](https://medium.com/@davidsweenor?source=post_page-----e8b22525d71a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@davidsweenor?source=post_page-----e8b22525d71a--------------------------------)[![David
    Sweenor](../Images/7dbb5c549ab67bc78f906fb707969ff6.png)](https://medium.com/@davidsweenor?source=post_page-----e8b22525d71a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e8b22525d71a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e8b22525d71a--------------------------------)
    [**David Sweenor**](https://medium.com/@davidsweenor?source=post_page-----e8b22525d71a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fec7aed1f3ef1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregulating-generative-ai-e8b22525d71a&user=David+Sweenor&userId=ec7aed1f3ef1&source=post_page-ec7aed1f3ef1----e8b22525d71a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e8b22525d71a--------------------------------)
    ·7 min read·Aug 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe8b22525d71a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregulating-generative-ai-e8b22525d71a&user=David+Sweenor&userId=ec7aed1f3ef1&source=-----e8b22525d71a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fec7aed1f3ef1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregulating-generative-ai-e8b22525d71a&user=David+Sweenor&userId=ec7aed1f3ef1&source=post_page-ec7aed1f3ef1----e8b22525d71a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e8b22525d71a--------------------------------)
    ·7分钟阅读·2023年8月8日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe8b22525d71a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregulating-generative-ai-e8b22525d71a&user=David+Sweenor&userId=ec7aed1f3ef1&source=-----e8b22525d71a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8b22525d71a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregulating-generative-ai-e8b22525d71a&source=-----e8b22525d71a---------------------bookmark_footer-----------)![](../Images/60e39e9aeb8d30a09e4dab9420ebc048.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe8b22525d71a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fregulating-generative-ai-e8b22525d71a&source=-----e8b22525d71a---------------------bookmark_footer-----------)![](../Images/60e39e9aeb8d30a09e4dab9420ebc048.png)'
- en: Photo Courtesy of Author — David. E. Sweenor
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供 — **David. E. Sweenor**
- en: As generative artificial intelligence (AI) remains on center stage, there is
    a growing call for regulating this technology since it can negatively impact a
    large population quickly. Impacts could take the form of discrimination, perpetuating
    stereotypes, privacy violations, negative biases, and undermine basic human values.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成式人工智能（AI）成为焦点，对于监管这一技术的呼声日益增高，因为它可能会迅速对大量人群产生负面影响。其影响可能表现为歧视、延续刻板印象、侵犯隐私、负面偏见，并削弱基本人类价值观。
- en: 'In June 2023, the US government announced a set of [voluntary AI guidelines](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/)
    that several prominent companies agreed to follow — including Anthropic, Meta
    (Facebook), Google, Amazon, OpenAI, and Microsoft to name a few.[1] This is a
    great step for the US, but unfortunately, it has always lagged behind the European
    Union in AI regulations. In my previous post [Generative AI Ethics: Key Considerations
    in the Age of Autonomous Content](https://medium.com/towards-data-science/generative-ai-ethics-b2db92ecb909),
    I explored the EU AI Ethics Framework and provided a set of considerations for
    implementing the framework when large language models (LLMs) are used. This blog
    focuses on the draft EU AI Act and how well LLMs abide by the draft legislation.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年6月，美国政府发布了一套[自愿性AI指导方针](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/)，几家知名公司同意遵循，包括Anthropic、Meta（Facebook）、Google、Amazon、OpenAI和Microsoft等。[1]
    这对美国来说是一个重要步骤，但不幸的是，美国在AI法规方面一直落后于欧盟。在我之前的文章[生成AI伦理：在自主内容时代的关键考量](https://medium.com/towards-data-science/generative-ai-ethics-b2db92ecb909)中，我探讨了欧盟的AI伦理框架，并提供了一些在使用大型语言模型（LLM）时实施该框架的考虑。这篇博客专注于欧盟AI法案草案及LLM如何遵守该草案法规。
- en: EU AI Act
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欧盟AI法案
- en: In June 2023, the [EU passed the worlds first draft regulation on AI](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence).
    Building upon the AI ethics framework ratified in 2019, the EU’s priority is to
    guarantee that AI systems used in the EU are “safe, transparent, traceable, non-discriminatory‌,
    and environmentally friendly.”[2] To avoid detrimental consequences, the EU framework
    insists that humans remain involved in AI systems. In other words, companies can’t
    simply let AI and automation run itself.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年6月，[欧盟通过了世界上第一部AI草案法规](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)。在2019年批准的AI伦理框架基础上，欧盟的优先任务是确保在欧盟使用的AI系统“安全、透明、可追溯、非歧视性和环境友好”。[2]
    为了避免不利后果，欧盟框架坚持要求人类参与AI系统。换句话说，公司不能简单地让AI和自动化运行自己。
- en: 'The proposed law segments AI into three different categories depending on the
    risk they may pose to people — each risk level requires a different level of regulation.
    If this plan is accepted, it would be the first set of AI regulations in the world.
    The three risk tiers identified by the EU are: unacceptable risk, high risk, and
    limited risk.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的法律将AI根据其对人类可能构成的风险分为三个不同的类别 — 每个风险级别都需要不同程度的监管。如果这一计划获得批准，它将成为世界上第一个AI法规。欧盟识别的三个风险层级是：不可接受的风险、高风险和有限风险。
- en: '**Unacceptable risk**: using technology that is harmful and poses a threat
    to human beings will be prohibited. Such examples could include the cognitive
    influence of individuals or certain vulnerable classes; ranking of people based
    on their social standing, and using facial recognition for real-time surveillance
    and remote identity identification en masse. Now, we all know militaries around
    the world are focused on autonomous weapons, but I digress.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**不可接受的风险**：使用可能对人类有害并构成威胁的技术将被禁止。此类例子可能包括对个体或某些弱势群体的认知影响；基于社会地位对人们进行排名；以及大规模使用面部识别进行实时监视和远程身份识别。现在，我们都知道世界各国的军事力量正在专注于自主武器，但我在这里岔开了话题。'
- en: '**High risk**: AI systems that could have a deleterious effect on safety or
    basic rights and freedoms are classified into two different categories by the
    EU. The first category is AI that is embedded in retail products that currently
    falls under the EU’s product safety regulations. This includes toys, airplanes,
    automobiles, medical equipment, elevators, and so forth. The second category will
    need to be registered in an EU database. This includes technology like biometrics,
    critical infrastructure operations, training and education, employment-related
    activities, policing, border control, and legal analysis of the law.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**高风险**：欧盟对可能对安全或基本权利和自由产生有害影响的AI系统进行了分类。第一类是嵌入在零售产品中的AI，目前属于欧盟的产品安全法规范范围。这包括玩具、飞机、汽车、医疗设备、电梯等。第二类需要在欧盟数据库中注册。这包括生物识别技术、关键基础设施运营、培训与教育、与就业相关的活动、警务、边境控制以及法律分析等技术。'
- en: '**Limited risk**: at the very least, low risk systems must meet standards for
    transparency and openness that would give people the chance to make knowledgeable
    decisions. The EU stipulates that users should be notified whenever they are engaging
    with AI. They also require that models should be created in a way so they don’t
    create unlawful material. They also require that model makers disclose what (if
    any) copyrighted material was used in their training.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**有限风险**：至少，低风险系统必须符合透明度和开放性的标准，以便让人们有机会做出明智的决策。欧盟规定，用户在与 AI 互动时应被通知。他们还要求模型的创建方式应避免生成非法内容，并要求模型制作者披露训练中使用的（如果有的话）版权材料。'
- en: The EU AI Act will next need to be negotiated among member countries so they
    can vote on the final form of the law. The EU is targeting the end of the year
    (2023) for ratification.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟 AI 法案接下来需要在成员国之间进行谈判，以便他们能够对法律的最终形式进行投票。欧盟计划在年底（2023）完成批准。
- en: Now, let’s turn to how current LLMs adhere to the draft act.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看当前 LLM 如何遵守草案法案。
- en: LLM Compliance with the Draft EU AI Act
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 对草案 EU AI 法案的合规性
- en: Researchers at Stanford University’s Center for Research on Foundation Models
    (CRFM) and Institute for Human-Centered Artifical Intelligence (HAI) recently
    published a paper titled [Do Foundation Models Comply with the Draft EU AI Act](https://crfm.stanford.edu/2023/06/15/eu-ai-act.html)?
    They extracted twenty-two requirements from the act, categorized them‌, and then
    created a 5-point rubric for twelve of the twenty-two requirements. All of the
    research, including criteria, rubrics, and scores are available on [GitHub under
    the MIT License](https://github.com/stanford-crfm/TransparencyIndex/tree/main).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福大学的基础模型研究中心（CRFM）和以人为本的人工智能研究所（HAI）最近发布了一篇题为[《基础模型是否符合草案 EU AI 法案》](https://crfm.stanford.edu/2023/06/15/eu-ai-act.html)的论文。他们从该法案中提取了二十二项要求，对其进行了分类，并为其中十二项要求创建了一个
    5 分制的评分标准。所有研究内容，包括标准、评分标准和分数，都可以在[GitHub 上 MIT 许可证下获取](https://github.com/stanford-crfm/TransparencyIndex/tree/main)。
- en: The research team mapped the legislative requirements into the categories seen
    in Table 1.1\. Now,‌ it should be noted that the team only evaluated twelve of
    the twenty-two total requirements identified. In the end, the team selected the
    twelve requirements that were most easily assessable based on publicly available
    data and documentation provided by the model makers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 研究团队将立法要求映射到表 1.1 中所示的类别中。需要注意的是，团队仅评估了二十二项要求中的十二项。最终，团队选择了基于公开数据和模型制作者提供的文档中最易评估的十二项要求。
- en: '**Table 1.1: LLM Compliance Table Summary**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 1.1：LLM 合规性表总结**'
- en: '![](../Images/086eb000e4bf00adbff2e28588c8db82.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/086eb000e4bf00adbff2e28588c8db82.png)'
- en: 'Source: Stanford CRFM, and HAI Scorecard. Used Under MIT License.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：斯坦福 CRFM 和 HAI 评分卡。使用 MIT 许可证。
- en: For those who many not be aware, the Stanford team has also painstakingly cataloged
    over a hundred LLM datasets, models, and applications which can be found on their
    [ecosystem graphs](https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table).
    To make things manageable, the researchers analyzed “10 foundation model providers–and
    their flagship foundation models–with 12 of the Act’s requirements for foundation
    models based on their [our] rubrics.”[3]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些可能不太了解的人，斯坦福团队还 painstakingly 编制了超过一百个 LLM 数据集、模型和应用程序，这些内容可以在他们的[生态系统图](https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table)上找到。为了便于管理，研究人员分析了“10
    个基础模型提供者及其旗舰基础模型，与 12 项法案要求进行对比，基于他们的[评分标准]”。
- en: The researchers looked at models from OpenAI, Anthropic, Google, Meta, Stability.ai,
    and others. Based on their analysis, their research resulted in the following
    scorecard.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员审视了来自 OpenAI、Anthropic、Google、Meta、Stability.ai 等公司的模型。根据他们的分析，研究结果得出了以下评分卡。
- en: '**Figure 1.2: Grading Foundation Model Providers’ Compliance with the Draft
    EU AI Act**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.2：基础模型提供者对草案 EU AI 法案的合规性评分**'
- en: '![](../Images/39d25951b3aa03fde8389628e2d79ae5.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39d25951b3aa03fde8389628e2d79ae5.png)'
- en: 'Source: Stanford CRFM, and HAI Scorecard. Used Under MIT License.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：斯坦福 CRFM 和 HAI 评分卡。使用 MIT 许可证。
- en: Overall, the researchers noted that there was quite a bit of variability in
    model compliance across ‌providers (and these were only twelve of the twenty-two
    requirements) with “some providers ​​score less than 25% (AI21 Labs, Aleph Alpha,
    Anthropic) and only one provider scores at least 75% (Hugging Face/BigScience)
    at present.”[4]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，研究人员指出，各个提供商之间模型合规性存在相当大的差异（这些仅是二十二个要求中的十二个），其中“一些提供商得分低于25%（AI21 Labs、Aleph
    Alpha、Anthropic），目前只有一个提供商得分至少为75%（Hugging Face/BigScience）。”[4]
- en: 'I’d encourage you to read the full study, but the researchers stated that there
    is considerable room for improvement across all providers. They also identified
    ‌several key ‘persistent challenges’ ‌which include:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你阅读完整的研究报告，但研究人员指出，各个提供商之间还有相当大的改进空间。他们还识别出几个关键的“持续挑战”，包括：
- en: '● **Ambiguous Copyright Issues**: Most of the foundation models were trained
    on data sourced from the internet, with a significant portion likely protected
    by copyright. However, most providers do not clarify the copyright status of the
    training data. The legal implications of using and reproducing copyrighted data,
    particularly when considering licensing terms, are not well-defined and are currently
    under active litigation in the United States (see [Washington Post — AI learned
    from their work. Now they want compensation.](https://www.washingtonpost.com/technology/2023/07/16/ai-programs-training-lawsuits-fair-use/)
    [Reuters — US judge finds flaws in artists’ lawsuit against AI companies](https://www.reuters.com/legal/litigation/us-judge-finds-flaws-artists-lawsuit-against-ai-companies-2023-07-19/)).
    We’ll have to see how this plays out over time.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ● **模糊的版权问题**：大多数基础模型的训练数据来源于互联网，其中很大一部分可能受到版权保护。然而，大多数提供商并未明确训练数据的版权状态。使用和复制受版权保护的数据的法律影响，特别是在考虑许可条款时，并不明确，目前在美国正在积极诉讼中（参见
    [华盛顿邮报 — AI 从他们的工作中学习了，现在他们想要赔偿](https://www.washingtonpost.com/technology/2023/07/16/ai-programs-training-lawsuits-fair-use/)
    [路透社 — 美国法官发现艺术家对 AI 公司诉讼存在缺陷](https://www.reuters.com/legal/litigation/us-judge-finds-flaws-artists-lawsuit-against-ai-companies-2023-07-19/)）我们还需要观察这件事如何发展。
- en: '● **Lack of Risk Mitigation Disclosure**: As mentioned in the intro, AI has
    the potential to negatively impact many people rapidly, so understanding LLM risks
    is critical. However, nearly all of the foundation model providers ignore the
    risk disclosures identified in the draft legislation. Although many providers
    list risks, very few‌ detail the steps they’ve taken to mitigate identified risks.
    Although not a generative AI case, there is a recent lawsuit against US health
    insurer Cigna Healthcare, which alleges that they used AI to deny payments ([Axios
    — AI lawsuits spread to health](https://www.axios.com/2023/07/25/ai-lawsuits-health-cigna-algorithm-payment-denial)).
    Bill Gates penned a great article titled [The risks of AI are real but manageable](https://www.gatesnotes.com/The-risks-of-AI-are-real-but-manageable),
    which I encourage you to read.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ● **缺乏风险缓解披露**：如引言所述，AI有可能迅速对许多人产生负面影响，因此了解LLM的风险至关重要。然而，几乎所有的基础模型提供商都忽视了草案立法中识别的风险披露。虽然许多提供商列出了风险，但很少有提供商详细说明他们采取了哪些措施来缓解识别出的风险。尽管不是生成性AI案例，但最近对美国健康保险公司Cigna
    Healthcare提起了诉讼，指控他们使用AI来拒绝支付（[Axios — AI 诉讼扩展到健康领域](https://www.axios.com/2023/07/25/ai-lawsuits-health-cigna-algorithm-payment-denial)）。比尔·盖茨写了一篇题为
    [AI 的风险是真实的但可以管理的](https://www.gatesnotes.com/The-risks-of-AI-are-real-but-manageable)
    的好文章，推荐你阅读。
- en: '● **Evaluation and Auditing Deficit**: There’s a dearth of consistent benchmarks
    for evaluating the performance of foundation models, especially in areas like
    potential misuse or model robustness. The United States’ CHIPS and Science Act
    has put forth a mandate for the National Institute of Standards and Technology
    (NIST) to create standardized evaluations for AI models. The ability to evaluate
    and monitor models was the focus of my [GenAIOps framework](https://medium.com/towards-data-science/genaiops-evolving-the-mlops-framework-b0012f936379)
    that I recently discussed. Eventually, we’ll see GenAIOps, DataOps, and DevOps
    come together under a common framework, but we are a ways off.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ● **评估和审计不足**：在评估基础模型的性能，特别是在潜在误用或模型鲁棒性等领域，缺乏一致的基准。美国的《芯片与科学法案》要求国家标准与技术研究院（NIST）为AI模型创建标准化评估。我最近讨论的
    [GenAIOps 框架](https://medium.com/towards-data-science/genaiops-evolving-the-mlops-framework-b0012f936379)
    就专注于评估和监控模型。最终，我们将看到GenAIOps、DataOps和DevOps在一个共同的框架下汇聚，但我们还需一段时间才能实现。
- en: '● **Inconsistent Energy Consumption Reports**: I think many of us have experienced
    the recent heat waves across the world. For LLMs, foundation model providers are
    quite varied when it comes to reporting energy use and related emissions. In fact,
    the researchers cite other research that suggests that we don’t even know how
    to measure and account for energy usage yet. Nnlabs.org reported the following
    “According to OpenAI, GPT-2, which has 1.5 billion parameters, required 355 years
    of single-processor computing time and consumed 28,000 kWh of energy to train.
    In comparison, GPT-3, which has 175 billion parameters, required 355 years of
    single-processor computing time and consumed 284,000 kWh of energy to train, which
    is 10 times more energy than GPT-2\. BERT, which has 340 million parameters, required
    4 days of training on 64 TPUs and consumed 1,536 kWh of energy.”[5]'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ● **不一致的能源消耗报告**：我认为我们许多人都经历过最近全球范围内的热浪。对于大型语言模型（LLMs）来说，基础模型提供商在报告能源使用和相关排放方面差异很大。事实上，研究人员引用了其他研究，表明我们甚至不知道如何测量和计算能源使用。Nnlabs.org
    报告了以下内容：“根据 OpenAI 的数据，具有 15 亿个参数的 GPT-2 需要 355 年的单处理器计算时间，并消耗了 28,000 千瓦时的能源进行训练。相比之下，具有
    1750 亿个参数的 GPT-3 需要 355 年的单处理器计算时间，并消耗了 284,000 千瓦时的能源进行训练，是 GPT-2 的 10 倍。具有 3.4
    亿个参数的 BERT 需要 64 个 TPUs 上训练 4 天，并消耗了 1,536 千瓦时的能源。”[5]
- en: In addition to the above, there are many other issues to contend with when implementing
    generative AI with your organization.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述问题之外，实施生成性人工智能时还有许多其他问题需要解决。
- en: Summary
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Based on the research, there is still a long road ahead for providers and adopters
    of generative AI technologies. Law makers, system designers, governments, and
    organizations need to work together to address these important issues. As a starting
    point, we can make sure that we are transparent in our design, implementation,
    and use of AI systems. For regulated industries, this could be a challenge as
    LLMs oftentimes have billions of parameters. Billions! How can something be interpretable
    and transparent with that many factors? These systems need to have clear, unambiguous
    documentation and we need to respect intellectual property rights. To support
    environmental, social, and corporate governance (ESG), we will also need to design
    a standardized framework for measuring and reporting energy consumption. Most
    importantly, AI systems need to be secure, respect privacy, and uphold human values.
    We need to take a human-centered approach to AI.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据研究，生成性人工智能技术的提供者和采纳者仍面临很长的道路。立法者、系统设计者、政府和组织需要共同努力，解决这些重要问题。作为起点，我们可以确保在设计、实施和使用人工智能系统时保持透明。对于受监管的行业，这可能是一个挑战，因为大型语言模型（LLMs）往往拥有数十亿个参数。数十亿！在如此众多的因素下，如何确保系统的可解释性和透明度？这些系统需要具备清晰、明确的文档，并且我们需要尊重知识产权。为了支持环境、社会和公司治理（ESG），我们还需要设计一个标准化的能源消耗测量和报告框架。最重要的是，人工智能系统需要安全、尊重隐私，并维护人类价值观。我们需要采取以人为本的人工智能方法。
- en: 'If you want to learn more about Artificial Intelligence, check out my book
    [Artificial Intelligence: An Executive Guide to Make AI Work for Your Business
    on Amazon](https://www.amazon.com/Artificial-Intelligence-Executive-Guide-Business/dp/B09X4KTYS4)
    or the [AI narrated audio book on Google Play](https://play.google.com/store/audiobooks/details/Artificial_Intelligence_An_Executive_Guide_to_Make?id=AQAAAECisyHzkM&hl=en_US&gl=US&pli=1).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于人工智能的内容，可以查看我在亚马逊上的书籍 [《人工智能：使 AI 为你的业务服务的高管指南》](https://www.amazon.com/Artificial-Intelligence-Executive-Guide-Business/dp/B09X4KTYS4)
    或 [Google Play 上的 AI 解说有声书](https://play.google.com/store/audiobooks/details/Artificial_Intelligence_An_Executive_Guide_to_Make?id=AQAAAECisyHzkM&hl=en_US&gl=US&pli=1)。
- en: '[1] Shear, Michael D., Cecilia Kang, and David E. Sanger. 2023\. “Pressured
    by Biden, A.I. Companies Agree to Guardrails on New Tools.” The New York Times,
    July 21, 2023, sec. U.S. [https://www.nytimes.com/2023/07/21/us/politics/ai-regulation-biden.html](https://www.nytimes.com/2023/07/21/us/politics/ai-regulation-biden.html).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Shear, Michael D., Cecilia Kang, 和 David E. Sanger. 2023年。“在拜登的压力下，人工智能公司同意对新工具设立保护措施。”《纽约时报》，2023年7月21日，U.S.
    部分 [https://www.nytimes.com/2023/07/21/us/politics/ai-regulation-biden.html](https://www.nytimes.com/2023/07/21/us/politics/ai-regulation-biden.html)。'
- en: '[2] European Parliament. 2023\. “EU AI Act: First Regulation on Artificial
    Intelligence | News | European Parliament.” [www.europarl.europa.eu.](http://www.europarl.europa.eu.)
    August 6, 2023\. [https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 欧洲议会. 2023年。“欧盟AI法案：首个人工智能法规 | 新闻 | 欧洲议会。” [www.europarl.europa.eu.](http://www.europarl.europa.eu.)
    2023年8月6日。[https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)。'
- en: '[3] Bommasani, Rishi, Kevin Klyman, Daniel Zhang, and Percy Liang. 2023\. “Stanford
    CRFM.” Crfm.stanford.edu. June 15, 2023\. [https://crfm.stanford.edu/2023/06/15/eu-ai-act.html](https://crfm.stanford.edu/2023/06/15/eu-ai-act.html).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bommasani, Rishi, Kevin Klyman, Daniel Zhang, 和 Percy Liang. 2023年。“斯坦福CRFM。”
    Crfm.stanford.edu. 2023年6月15日。[https://crfm.stanford.edu/2023/06/15/eu-ai-act.html](https://crfm.stanford.edu/2023/06/15/eu-ai-act.html)。'
- en: '[4] Bommasani, Rishi, Kevin Klyman, Daniel Zhang, and Percy Liang. 2023\. “Stanford
    CRFM.” Crfm.stanford.edu. June 15, 2023\. [https://crfm.stanford.edu/2023/06/15/eu-ai-act.html](https://crfm.stanford.edu/2023/06/15/eu-ai-act.html).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bommasani, Rishi, Kevin Klyman, Daniel Zhang, 和 Percy Liang. 2023年。“斯坦福CRFM。”
    Crfm.stanford.edu. 2023年6月15日。[https://crfm.stanford.edu/2023/06/15/eu-ai-act.html](https://crfm.stanford.edu/2023/06/15/eu-ai-act.html)。'
- en: '[5] ai. 2023\. “Power Requirements to Train Modern LLMs.” Nnlabs.org. March
    5, 2023\. [https://www.nnlabs.org/power-requirements-of-large-language-models/](https://www.nnlabs.org/power-requirements-of-large-language-models/).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] ai. 2023年。“训练现代大语言模型的电力需求。” Nnlabs.org. 2023年3月5日。[https://www.nnlabs.org/power-requirements-of-large-language-models/](https://www.nnlabs.org/power-requirements-of-large-language-models/)。'
