- en: How to Chunk Text Data — A Comparative Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a?source=collection_archive---------0-----------------------#2023-07-20](https://towardsdatascience.com/how-to-chunk-text-data-a-comparative-analysis-3858c4a0997a?source=collection_archive---------0-----------------------#2023-07-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring distinct approaches to text chunking.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://solano-todeschini.medium.com/?source=post_page-----3858c4a0997a--------------------------------)[![Solano
    Todeschini](../Images/75e871340659c8df37f558b74c9d73c5.png)](https://solano-todeschini.medium.com/?source=post_page-----3858c4a0997a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3858c4a0997a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3858c4a0997a--------------------------------)
    [Solano Todeschini](https://solano-todeschini.medium.com/?source=post_page-----3858c4a0997a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F618a52c38c0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-chunk-text-data-a-comparative-analysis-3858c4a0997a&user=Solano+Todeschini&userId=618a52c38c0c&source=post_page-618a52c38c0c----3858c4a0997a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3858c4a0997a--------------------------------)
    ·17 min read·Jul 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3858c4a0997a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-chunk-text-data-a-comparative-analysis-3858c4a0997a&user=Solano+Todeschini&userId=618a52c38c0c&source=-----3858c4a0997a---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3858c4a0997a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-chunk-text-data-a-comparative-analysis-3858c4a0997a&source=-----3858c4a0997a---------------------bookmark_footer-----------)![](../Images/dfd6dd3b7af559b26e33e94122605cf6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image compiled by the author. Pineapple image from Canva.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ‘Text chunking’ process in Natural Language Processing (NLP) involves the
    conversion of unstructured text data into meaningful units. This seemingly simple
    task belies the complexity of the various methods employed to achieve it, each
    with its strengths and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, these methods typically fall into one of two categories. The
    first, rule-based methods, hinge on the use of explicit separators such as punctuation
    or space characters, or the application of sophisticated systems like regular
    expressions, to partition text into chunks. The second category, semantic clustering
    methods, leverages the inherent meaning embedded in the text to guide the chunking
    process. These might utilize machine learning algorithms to discern context and
    infer natural divisions within the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we’ll explore and compare these two distinct approaches to
    text chunking. We’ll represent rule-based methods with NLTK, Spacy, and Langchain,
    and contrast this with two different semantic clustering techniques: KMeans and
    a custom technique for Adjacent Sentence Clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to equip practitioners with a clear understanding of each method’s
    pros, cons, and ideal use cases to enable better decision-making in their NLP
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: In Brazilian slang, “abacaxi,” which translates to “pineapple,” signifies “something
    that doesn’t yield a good outcome, a tangled mess, or something that is no good.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Use Cases for Text Chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Text chunking can be used by several different applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text Summarization**: By breaking down large bodies of text into manageable
    chunks, we can summarize each section individually, leading to a more accurate
    overall summary.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sentiment Analysis**: Analyzing the sentiment of shorter, coherent chunks
    can often yield more precise results than analyzing an entire document.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Information Extraction**: Chunking helps in locating specific entities or
    phrases within text, enhancing the process of information retrieval.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Text Classification**: Breaking down text into chunks allows classifiers
    to focus on smaller, contextually meaningful units rather than entire documents,
    which can improve performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Machine Translation**: Translation systems often operate on chunks of text
    rather than on individual words or whole documents. Chunking can aid in maintaining
    the coherence of the translated text.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding these use cases can help in choosing the most suitable chunking
    technique for your specific project.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Different Methods for Semantic Chunking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this part of the article, we will compare popular methods for semantic chunking
    of unstructured text: NLTK Sentence Tokenizer, Langchain Text Splitter, KMeans
    Clustering, and Clustering Adjacent Sentences based on similarity.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we’re gonna evaluate this technique using a text extracted
    from a PDF, processing it into sentences and their clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The data we used was a PDF exported from [Brazil’s Wikipedia page](https://en.wikipedia.org/wiki/Brazil).
  prefs: []
  type: TYPE_NORMAL
- en: 'For extracting text from PDF and split into sentences with NLTK,we use the
    following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Like that, we end with a string `text` with 210964 characters of length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a sample of the Wiki text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: NLTK Sentence Tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Natural Language Toolkit (NLTK) provides a useful function for splitting
    text into sentences. This sentence tokenizer divides a given block of text into
    its component sentences, which can then be used for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s an example of using the NLTK sentence tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This returns a list of 2670 `sentences` extracted from the input text with a
    mean of 78 characters per sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating NLTK Sentence Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While the NLTK Sentence Tokenizer is a straightforward and efficient way to
    divide a large body of text into individual sentences, it does come with certain
    limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Language Dependency**: The NLTK Sentence Tokenizer relies heavily on the
    language of the text. It performs well with English but may not provide accurate
    results with other languages without additional configuration.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Abbreviations and Punctuation**: The tokenizer can occasionally misinterpret
    abbreviations or other punctuation at the end of a sentence. This can lead to
    fragments of sentences being treated as independent sentences.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Lack of Semantic Understanding**: Like most tokenizers, the NLTK Sentence
    Tokenizer does not consider the semantic relationship between sentences. Therefore,
    a context that spans multiple sentences might be lost in the tokenization process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spacy Sentence Splitter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spacy, another powerful NLP library, provides a sentence tokenization function
    that relies heavily on linguistic rules. It is a similar approach to NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Implementing Spacy’s sentence splitter is quite straightforward. Here’s how
    to do it in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This returns a list of 2336 `sentences` extracted from the input text with a
    mean of 89 characters per sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating Spacy Sentence Splitter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spacy’s sentence splitter tends to create smaller chunks compared to the Langchain
    Character Text Splitter, as it strictly adheres to sentence boundaries. This can
    be advantageous when smaller text units are necessary for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Like NLTK, however, Spacy’s performance depends on the quality of the input
    text. For poorly punctuated or structured text, the identified sentence boundaries
    might not always be accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we’ll see how Langchain provides a framework for chunking text data and
    further compare it with NLTK and Spacy.
  prefs: []
  type: TYPE_NORMAL
- en: Langchain Character Text Splitter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Langchain Character Text Splitter works by recursively dividing the text
    at specific characters. It is especially useful for generic text.
  prefs: []
  type: TYPE_NORMAL
- en: The splitter is defined by a list of characters. It attempts to split the text
    based on these characters until the generated chunks meet the desired size criterion.
    The default list is [“\n\n”, “\n”, “ ”, “”], aiming to keep paragraphs, sentences,
    and words together as much as possible to maintain semantic coherence.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider the following example, where we split the sample text extracted from
    our PDF using this method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we end up with 3205 chunks of text, represented by the `texts` list.
    65.8 charactersis the mean for each chunk here — a bit less thank NLTK's mean
    (79 characters).
  prefs: []
  type: TYPE_NORMAL
- en: '**Changing Parameters and Using ''\n'' Separator:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a more customized approach on the Langchain Splitter, we can alter the `chunk_size`
    and `chunk_overlap` parameters according to our needs. Additionally, we can specify
    only one character (or set of characters) for the splitting operation, such as
    `\n`. This will guide the splitter to separate the text into chunks only at the
    new line characters.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider an example where we set `chunk_size` to 300, `chunk_overlap`
    to 30, and only use `\n` as the separator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s compare some outputs from the standard set of parameters with the
    custom parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can already see that these custom parameters yield much bigger chunks and
    therefore keep more content than the default set of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Langchain Character Text Splitter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After splitting the text into chunks using different parameters, we obtain
    two lists of chunks: `texts` and `custom_texts`, containing 3205 and 1404 text
    chunks, respectively. Now, let''s plot the distribution of chunk lengths for these
    two scenarios to better understand the impact of changing the parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bc0fe338ed1534c308f134cf80fa36b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1**: Distribution plot of chunk lengths for Langchain splitter with
    different parameters (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: In this histogram, the x-axis represents the chunk lengths, while the y-axis
    represents the frequency of each length. The blue bars represent the distribution
    of chunk lengths for the original parameters, and the orange bars represent the
    distribution of the custom parameters. By comparing these two distributions, we
    can see how the changes in parameters affected the resulting chunk lengths.
  prefs: []
  type: TYPE_NORMAL
- en: '*Remember, the ideal distribution depends on the specific requirements of your
    text-processing task. You might want smaller, more numerous chunks if you’re dealing
    with fine-grained analysis or larger, fewer chunks for broader semantic analysis.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Langchain Character Text Splitter vs. NLTK and Spacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier, we generated 3205 chunks using the Langchain splitter with its default
    parameters. The NLTK Sentence Tokenizer, on the other hand, split the same text
    into a total of 2670 sentences.
  prefs: []
  type: TYPE_NORMAL
- en: To get a more intuitive understanding of the difference between these methods,
    we can visualize the distribution of chunk lengths. The following plot shows the
    densities of chunk lengths for each method, allowing us to see how the lengths
    are distributed and where most of the lengths lie.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55d688776a0dcc8cd5b779ba04b3d185.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 2**: Distribution plot of chunk lengths resulting from Langchain Splitter
    with custom parameters vs. NLTK and Spacy (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: From Figure 1, we can see that the Langchain splitter results in a much more
    concise density of cluster lengths and has a tendency to have more of longer clusters
    whereas NLTK and Spacy seem to produce very similar outputs in terms of cluster
    length, preferring smaller sentences while having lots of outliers with lengths
    that can reach up to 1400 characters — and a tendency of decreasing length.
  prefs: []
  type: TYPE_NORMAL
- en: KMeans Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentence Clustering is a technique that involves grouping sentences based on
    their semantic similarity. By using sentence embeddings and a clustering algorithm
    such as K-means, we can implement Sentence Clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is a simple example code snippet using the Python library `sentence-transformers`
    for generating sentence embeddings and `scikit-learn` for K-means clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see here that the steps for clustering a list of sentences are:'
  prefs: []
  type: TYPE_NORMAL
- en: Load a Sentence Transform model. In this case, we're using `all-MiniLM-L6-v2`
    from sentence-transformers/all-MiniLM-L6-v2 in [HuggingFace](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define your sentences and generate their embeddings with the `encode()` method
    from the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then you define your clustering technique and number of clusters (we're using
    KMeans with 3 clusters here) and finally fit it into the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluating KMeans Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: And finally we plot a WordCloud for each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Below we have the WordCloud plots for the generated clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d315eac1d84c9a0a389376dedeab4441.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3**: Word Cloud plot for KMeans clustering — cluster 0 (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e9fcb7f2635eb6512d863ef5f8aa009.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4**: Word Cloud plot for KMeans clustering — cluster 1 (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa981bfdb20ca0356275a6cbc557f8a6.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 5**: Word Cloud plot for KMeans clustering — cluster 2 (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: In our analysis of the word cloud for the KMeans clustering, it’s evident that
    each cluster distinctively differentiates based on the semantics of its most frequent
    words. This demonstrates a strong semantic differentiation amongst clusters. Moreover,
    a noticeable variation in cluster sizes is observed, indicating a significant
    disparity in the number of sequences each cluster comprises.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of KMeans Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sentence clustering, although beneficial, does have a few notable drawbacks.
    The primary limitations include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loss of Sentence Order**: Sentence clustering doesn’t retain the original
    sequence of sentences, which could distort the natural flow of the narrative.
    ** This is very important**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Computational Efficiency**: KMeans can be computationally intensive and slow,
    especially with large text corpora or when working with a larger number of clusters.
    This can be a significant drawback for real-time applications or when handling
    big data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clustering Adjacent Sentences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To overcome some of the limitations of KMeans clustering, especially the loss
    of sentence order, an alternative approach could be clustering adjacent sentences
    based on their semantic similarity. The fundamental premise of this approach is
    that two sentences that appear consecutively in a text are more likely to be semantically
    related than two sentences that are farther apart.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s an expanded implementation of this heuristics using Spacy sentences
    as inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*Key takeaways from this code:*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text Processing**: Each text chunk is passed to the `process` function. This
    function uses the SpaCy library to create sentence embeddings, which are used
    to represent the semantic meaning of each sentence in the text chunk.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cluster Creation**: The `cluster_text` function forms clusters of sentences
    based on the cosine similarity of their embeddings. If the cosine similarity is
    less than a specified threshold, a new cluster begins.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Length Check**: The code then checks the length of each cluster. If a cluster
    is too short (less than 60 characters) or too long (more than 3000 characters),
    the threshold is adjusted and the process repeats for that particular cluster
    until an acceptable length is achieved.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s take a look at some of the output chunks from this approach and compare
    them to Langchain Splitter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, now let’s compare the distribution of chunk lengths of the `final_texts`
    (from the adjacent sequence clustering approach) with the distributions from the
    Langchain Character Text Splitter and NLTK Sentence Tokenizer. To do this, we''ll
    first need to calculate the lengths of the chunks in `final_texts`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now plot the distributions of all three methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/296e3efe4e40f3ca545e546ceb143638.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3**: Distribution plot of chunk lengths resulting from all the different
    methods tested (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: From Figure 6, we can derive that the Langchain splitter, using its predefined
    chunk size, creates a uniform distribution, implying consistent chunk lengths.
  prefs: []
  type: TYPE_NORMAL
- en: The Spacy Sentence Splitter and the NLTK Sentence Tokenizer, on the other hand,
    seem to prefer smaller sentences, though with many larger outliers, indicating
    their reliance on linguistic cues to determine splits and potentially produce
    irregularly sized chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the custom Adjacent Sequence Clustering approach, which clusters based
    on semantic similarity, exhibits a more varied distribution. This could be indicative
    of a more context-sensitive approach, maintaining the coherence of content within
    chunks while allowing for more flexibility in size.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating Adjacent Sequence Clustering Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Adjacent Sequence Clustering Approach brings **unique benefits**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Contextual Coherence**: Generates thematically consistent chunks by considering
    semantic and contextual coherence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Flexibility**: Balances context preservation and computational efficiency,
    providing adjustable chunk sizes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Threshold Tuning**: Allows users to fine-tune the chunking process according
    to their needs, by adjusting the similarity threshold.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sequence Preservation**: Retains the original order of sentences in the text,
    essential for sequential language models and tasks where text order matters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Comparing Text Chunking Methods: Summary of Insights'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Langchain Character Text Splitter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This method provides consistent chunk lengths, yielding a uniform distribution.
    This could be beneficial when a standard size is necessary for downstream processing
    or analysis. The approach is less sensitive to the specific linguistic structure
    of the text, focusing more on producing chunks of a predefined character length.
  prefs: []
  type: TYPE_NORMAL
- en: NLTK Sentence Tokenizer and Spacy Sentence Splitter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These approaches exhibit a preference for smaller sentences but include many
    larger outliers. While this can result in more linguistically coherent chunks,
    it can also lead to high variability in chunk size.
  prefs: []
  type: TYPE_NORMAL
- en: These methods can yield good results that can serve as inputs to downstream
    tasks too.
  prefs: []
  type: TYPE_NORMAL
- en: Adjacent Sequence Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This method generates a more varied distribution, indicative of its context-sensitive
    approach. By clustering based on semantic similarity, it ensures that the content
    within each chunk is coherent while allowing for flexibility in chunk size. This
    method may be advantageous when it is important to preserve the semantic continuity
    of text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more visual and abstract (or silly) representation, let''s look at Figure
    7 below and try to figure out which kind of pineapple "cut" would better represent
    the approaches discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89004ea38220a7f04c73cac9d5616f0f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 7**: Different methods of text chunking shown as pineapple cuts (Image
    compiled by the author. Pineapple image from Canva)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing them in order:'
  prefs: []
  type: TYPE_NORMAL
- en: Cut number 1 would represent a rule-based approach, in which you can just "peel
    off" the "junk" text you want based on filters or regular expressions. *Lot's
    of work* to do the whole pineapple tho, since it also retains a lot of outliers
    with a much bigger context size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Langchain would be like cut number 2\. Very similar pieces in size but not holding
    the entire desired context (it's a triangle, so it could be a watermelon as well).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cut number 3 is definitely KMeans. You may even group only what makes sense
    for you — the juiciest part — but you won't get its core. Without it, the chunks
    lose all the structure and meaning. I think it takes a lot of work to do that
    as well… *especially for bigger pineapples*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, cut number 4 illustrates the Adjacent Sentence Clustering method. The
    size of the chunks can vary but they often maintain contextual information, similar
    to uneven pineapple pieces that still indicate the fruit’s overall structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***TL;DR****: In this article, we’ve compared three text chunking methods and
    their unique benefits. Langchain offers consistent chunk sizes, but the linguistic
    structure takes a back seat. NLTK and Spacy give linguistically coherent chunks,
    yet the size varies considerably. Adjacent Sequence Clustering clusters based
    on semantic similarity, providing content coherence with flexible chunk sizes.
    Ultimately, the optimal choice hinges on your specific needs, including linguistic
    coherence, uniformity in chunk size, and available computational power.*'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Follow me on [Linkedin](https://www.linkedin.com/in/solano-todeschini/)!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
