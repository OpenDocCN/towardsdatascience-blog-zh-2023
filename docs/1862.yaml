- en: What is Bayes Error?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-is-bayes-error-4bfadcc9c0ad?source=collection_archive---------5-----------------------#2023-06-06](https://towardsdatascience.com/what-is-bayes-error-4bfadcc9c0ad?source=collection_archive---------5-----------------------#2023-06-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A simple introduction to a fundamental concept of machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@wanshunwong?source=post_page-----4bfadcc9c0ad--------------------------------)[![Wanshun
    Wong](../Images/42d967999b28ba8ab207f1858e6a4e6b.png)](https://medium.com/@wanshunwong?source=post_page-----4bfadcc9c0ad--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bfadcc9c0ad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bfadcc9c0ad--------------------------------)
    [Wanshun Wong](https://medium.com/@wanshunwong?source=post_page-----4bfadcc9c0ad--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb145fb04b8bd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-bayes-error-4bfadcc9c0ad&user=Wanshun+Wong&userId=b145fb04b8bd&source=post_page-b145fb04b8bd----4bfadcc9c0ad---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bfadcc9c0ad--------------------------------)
    ·6 min read·Jun 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4bfadcc9c0ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-bayes-error-4bfadcc9c0ad&user=Wanshun+Wong&userId=b145fb04b8bd&source=-----4bfadcc9c0ad---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4bfadcc9c0ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-bayes-error-4bfadcc9c0ad&source=-----4bfadcc9c0ad---------------------bookmark_footer-----------)![](../Images/7227c8d6f3145f701b9b4f4de11a018e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: When making different decisions and estimations regarding a machine learning
    project, such as
  prefs: []
  type: TYPE_NORMAL
- en: deciding whether to start / keep working on the project or not,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: estimating the business impact of the project,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: choosing the main strategy for improving the model performance,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: one of the most important considerations is how much room for model performance
    improvement is there. For example, suppose we have a binary classification model
    and its accuracy is 85%. We might then think that there is still plenty of room
    for improvement, and promise our boss at least a 5% increase in accuracy in a
    couple of weeks. However, this thought process of going from “85% accuracy” to
    “plenty of room for improvement” implicitly assumes the best possible model performance
    is 100% accuracy. Unfortunately, such assumptions are often not true, resulting
    in us having a misunderstanding of our project and making bad decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we will focus on the binary classification setting, and will
    use error rate (which is 1 - accuracy) as our model performance metric. Then,
    in order to have a good estimation of the room for reducing the model error rate,
    we will make use of a concept known as the Bayes Error (also known as the Bayes
    Error Rate).
  prefs: []
  type: TYPE_NORMAL
- en: Definition of Bayes Error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Bayes Error of a dataset is the lowest possible error rate that any model
    can achieve. In particular, if the Bayes Error is non-zero, then the two classes
    have some overlaps, and even the best model will make some wrong predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many possible reasons for a dataset to have a non-zero Bayes Error.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Poor data quality**: Some images in a computer vision dataset are very blurry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mislabelled data**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The labelling process is inconsistent**: When deciding whether a job applicant
    should proceed to the next round of interview, different interviewers might have
    different opinions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The data generating process is inherently stochastic**: Predicting heads
    or tails from coin flipping.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information missing from the feature vectors**: When predicting whether a
    baby has certain genetic traits or not, the feature vector contains information
    about the father but not the information about the mother.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, it is impossible to compute the exact value of the Bayes Error.
    However, there exist several estimation methods. The method that we are going
    to introduce is the simplest one, and it is based on soft labels.
  prefs: []
  type: TYPE_NORMAL
- en: Soft Labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let us denote the two classes of our dataset by 0 and 1\. The class label
    of every instance in our dataset is in the set {0, 1}, and there is no middle
    ground. In literature, this is known as hard labels (to contrast with soft labels).
  prefs: []
  type: TYPE_NORMAL
- en: 'Soft labels generalize hard labels by allowing middle ground and by incorporating
    our confidence (and uncertainty) about the class labels. It is defined as the
    probability of an instance belonging to class 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In particular, *s_i* takes value in the interval [0, 1]. Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '*s_i* = 1 means we are 100% confident that the instance belongs to class 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s_i* = 0 means we are 100% confident that the instance belongs to class 0,
    because the probability of it belonging to class 1 is 0%.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*s_i* = 0.6 means we think it is more likely for the instance to be in class
    1, but we are not very sure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that we can always convert soft labels to hard labels by checking *s_i*
    > 0.5 or not.
  prefs: []
  type: TYPE_NORMAL
- en: How to obtain soft labels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several common ways of obtaining soft labels:'
  prefs: []
  type: TYPE_NORMAL
- en: The most obvious way is to ask our dataset annotator to provide both the class
    label and his/her confidence level about the label.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we have multiple annotators, we can ask them to provide hard labels for each
    instance. Then we can use the proportions of the hard labels as soft labels. E.g.
    If we have 5 annotators, 4 of them think *x_i* belongs to class 1, and the remaining
    one thinks *x_i* belongs to class 0, then *s_i* = 0.8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the class labels are derived from some data sources, then we can use the
    same data sources to compute the soft labels. E.g. We want to predict whether
    a student can pass an exam or not. Suppose the total score of the exam is 100,
    and a passing score is 50 or greater. Hence, the hard labels are obtained simply
    by checking if *score* ≥ 50\. To compute the soft labels, we can apply a calibration
    method such as Platt scaling to *score*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating Bayes Error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Intuitively speaking, it is not hard to believe that Bayes Error and soft labels
    are correlated. After all, if there is uncertainty about the class labels, then
    it makes sense that even the best model will make some wrong predictions. The
    formula for estimating the Bayes Error using soft labels is very straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: which is the average of min(*s_i*, 1 - *s_i*). The simplicity of this formula
    makes it easy to use and applicable to many datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Concrete Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First let us consider the extreme case where the soft labels are either 0 or
    1\. This means we are 100% certain about the class labels. The term min(*s_i*,
    1 - *s_i*) is always 0, hence *β* is also 0\. This agrees with our intuition that
    the best model will be able to avoid making wrong predictions for this dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider a more interesting case where we have 10 instances, and the soft labels
    are 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1\. Then
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Making Use of Bayes Error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having a good estimation of the Bayes Error not only allows us to understand
    our dataset more, but also helps us in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the room for model performance improvement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us revisit the example given in the introduction of this article. Our model
    has accuracy 85%, which means the error rate is 15%. Suppose the Bayes Error is
    estimated to be 13%. In this case the room for improvement is actually only 2%.
    Most importantly, we should not promise our boss a 5% improvement in model performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Determine if we need a new dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Very often we have some minimum model performance requirements for our machine
    learning projects. For example, our model error rate is required to be ≤ 10%,
    so that the customer support team won’t be overloaded. If the Bayes Error of our
    dataset is estimated to be 13%, then instead of working on our model we should
    look for a new dataset. Maybe we need better cameras and sensors to collect data,
    or maybe we need new data sources to add more independent variables to our feature
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Understand the bias-variance tradeoff
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose our model has training error 8% and test error 10%. If we know the Bayes
    Error is close to 0%, then we can conclude that both the training error and the
    test error are large. Therefore, we should try to reduce the bias of our model.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if the Bayes Error is 7%, then
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: and we should work on the variance part instead.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Bayes Error estimation formula above is introduced in [2]. We refer to that
    paper for various theoretic properties of the formula such as the rate of convergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The lecture [Nuts and Bolts of Applying Deep Learning](https://www.youtube.com/watch?v=F1ka6a13S9I&t=3040s)
    by Andrew Ng talks about using human level performance as a proxy for the Bayes
    Error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Bayes Error quantifies the irreducible error of a given task. The decomposition
    of model error into bias, variance, and irreducible error for zero-one loss function
    (and other loss functions) is studied in [1].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3] shows that classifiers trained on soft labels generalize better to out-of-sample
    datasets, and are more resistant to adversarial attacks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: P. Domingos. [A Unified Bias-Variance Decomposition and its Applications](https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf)
    (2000), ICML 2000.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T. Ishida, I. Yamane, N. Charoenphakdee, G. Niu, and M. Sugiyama. [Is the Performance
    of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes
    Error in Binary Classification](https://openreview.net/pdf?id=FZdJQgy05rz) (2023),
    ICLR 2023.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: J.C. Peterson, R.M. Battleday, T.L. Griffiths, and O. Russakovsky. [Human uncertainty
    makes classification more robust](https://openaccess.thecvf.com/content_ICCV_2019/papers/Peterson_Human_Uncertainty_Makes_Classification_More_Robust_ICCV_2019_paper.pdf)
    (2019), ICCV 2019.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
