- en: A Foundation Model for Satellite Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-foundation-model-for-satellite-images-dbf356c746a9?source=collection_archive---------8-----------------------#2023-11-04](https://towardsdatascience.com/a-foundation-model-for-satellite-images-dbf356c746a9?source=collection_archive---------8-----------------------#2023-11-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Prithvi-100M IBM Geospatial AI Foundation Model for NASA Earth Observation
    Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@caroline.arnold_63207?source=post_page-----dbf356c746a9--------------------------------)[![Caroline
    Arnold](../Images/2560e106ba9deda7889c7d253792d814.png)](https://medium.com/@caroline.arnold_63207?source=post_page-----dbf356c746a9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dbf356c746a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dbf356c746a9--------------------------------)
    [Caroline Arnold](https://medium.com/@caroline.arnold_63207?source=post_page-----dbf356c746a9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9367198e7a3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-satellite-images-dbf356c746a9&user=Caroline+Arnold&userId=9367198e7a3c&source=post_page-9367198e7a3c----dbf356c746a9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dbf356c746a9--------------------------------)
    ·7 min read·Nov 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdbf356c746a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-satellite-images-dbf356c746a9&user=Caroline+Arnold&userId=9367198e7a3c&source=-----dbf356c746a9---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdbf356c746a9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-satellite-images-dbf356c746a9&source=-----dbf356c746a9---------------------bookmark_footer-----------)![](../Images/b28106f03767604cb36e6ac0c33a59e3.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Satellite image of the Karavasta Lagoon in Albania, 2017\. Image credit: [https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2017/03/karavasta_lagoon_albania/16854373-1-eng-GB/Karavasta_Lagoon_Albania.jpg](https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2017/03/karavasta_lagoon_albania/16854373-1-eng-GB/Karavasta_Lagoon_Albania.jpg).
    Contains modified Copernicus Sentinel data.'
  prefs: []
  type: TYPE_NORMAL
- en: Foundation models are flexible deep learning algorithms that are designed for
    general tasks, rather than being immediately focused on specific tasks. Trained
    on large amounts of unlabeled data, they can be applied to a variety of downstream
    tasks with minimal fine-tuning. Foundation models are well known from natural
    language processing ([BERT](https://huggingface.co/bert-base-uncased), GPT-x),
    and image processing ([DALL-E](https://huggingface.co/dalle-mini/dalle-mini)).
  prefs: []
  type: TYPE_NORMAL
- en: In August 2023, NASA and IBM released the Geospatial AI Foundation Model for
    NASA Earth Observation Data. The model is available open source on [Huggingface](https://huggingface.co/ibm-nasa-geospatial)
    under the name of Prithvi, the Hindu goddess of Mother Earth. It has been trained
    on NASA satellite data — [according to IBM](https://research.ibm.com/blog/nasa-hugging-face-ibm),
    *more than 250 Petabyte* of data are available.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, we discuss
  prefs: []
  type: TYPE_NORMAL
- en: The NASA Harmonized Sentinel-2 Landsat dataset used for training,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture of the Prithvi-100M Geospatial AI Foundation Model,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training process on IBM’s Vela supercomputer,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example applications: flooding and crop type identification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Geospatial AI Foundation Model has been trained on [NASA Harmonized LandSat
    Sentinel-2 data](https://docs.sentinel-hub.com/api/latest/data/hls/).
  prefs: []
  type: TYPE_NORMAL
- en: '[Sentinel-2](https://en.wikipedia.org/wiki/Sentinel-2) is a satellite mission
    coordinated by the European Space Agency, with two satellites currently in orbit
    taking high-resolution images of the Earth. It focuses on land, coastal areas,
    and selected open waters. The Landsat satellites were launched by NASA to record
    surface reflectance. The harmonized data combine the input from both sensors,
    resulting in a spatial resolution of about 30 meters and an average revisit time
    of two to three days. This resolution is sufficient for agricultural monitoring,
    land use classification, and natural disaster detection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Standard photographs are made up of three colors: red, green, and blue. The
    Sentinel-2 data is available in a total of 13 “colors”, so-called *bands*, spanning
    the visible, near-infrared, and shortwave infrared range of the electromagnetic
    spectrum. Selected bands can be used to identify different things, e.g. the infrared
    bands contain information about the vegetation. For background, see [this post](https://gisgeography.com/sentinel-2-bands-combinations/)
    on Sentinel-2 band combinations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f16808f35a9ef0e6db6646f1f45e7a32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'False-color infrared image of a Hawaiian airport. Image source: ESA sentinel
    satellite imagery, CC BY-SA 4.0 <[https://creativecommons.org/licenses/by-sa/4.0](https://creativecommons.org/licenses/by-sa/4.0)>,
    via Wikimedia Commons.'
  prefs: []
  type: TYPE_NORMAL
- en: Clouds block the view of Earth observation satellites. To counteract this effect,
    Sentinel-2 provides a band that can be used to identify cloud cover. The affected
    pixels are masked so as not to confuse the image processing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: As such, Sentinel-2 and Landsat data are unlabeled. It requires significant
    human effort and expertise to provide a pixel-wise classification of land use
    categories. Foundation models are highly versatile and extract structure from
    data without requiring labeled data for the initial stage of the training procedure.
    Therefore, they seem very promising for Earth observation data.
  prefs: []
  type: TYPE_NORMAL
- en: Model Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Prithvi-100M Geospatial AI Foundation Model builds on a temporal vision
    transformer and on a masked autoencoder. The model card is shown on Huggingface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e08769dab80b596aebf0969672e4b16a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Model card for Prithvi-100M on Huggingface. Image source: [https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/blob/main/GFM.png](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/blob/main/GFM.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The model accepts Landsat images in video format as input. Images taken at the
    same location are loaded as a time series, while static images can be processed
    by setting the time series length to 1\. The bands correspond to the channels
    of the vision transformer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Vision Transformer**'
  prefs: []
  type: TYPE_NORMAL
- en: In 2020, a team from Google Research showed that transformers could be applied
    not only to natural language processing, but also to images [(Dosovitsky et al,
    ICLR 2020)](https://arxiv.org/pdf/2010.11929.pdf). Until then, convolutional neural
    networks had been the *de facto* standard for image processing.
  prefs: []
  type: TYPE_NORMAL
- en: The vision transformer first cuts images into small patches, similar to the
    tokenization of sentences for a language processing transformer. Then, learnable
    embeddings and positional encodings are added. In the original paper, it was shown
    that with large amounts of training data, the vision transformer can outperform
    typical computer vision architectures such as ResNet.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://yurkovak.medium.com/vision-transformer-vit-under-the-magnifying-glass-part-1-70be8d6661a7?source=post_page-----dbf356c746a9--------------------------------)
    [## Vision Transformer (ViT) under the magnifying glass, Part 1'
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: yurkovak.medium.com](https://yurkovak.medium.com/vision-transformer-vit-under-the-magnifying-glass-part-1-70be8d6661a7?source=post_page-----dbf356c746a9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Masked Autoencoder**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Prithvi-100M masked autoencoder is based on the original implementation
    by He et al (2021), [https://arxiv.org/pdf/2111.06377.pdf](https://arxiv.org/pdf/2111.06377.pdf).
    The concept is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: Random patches in an image are masked. The autoencoder learns to predict the
    missing pixels. This is similar to the training of large language models, where
    the model learns to predict missing words from a sentence.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the original paper, 2D images with RGB (red, green, blue) color channels
    are considered. The difference between training on language data and image data
    is extensively discussed in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder works only on the unmasked patches, which saves computation time.
    The embedding is taken care of by providing a linear projection of the individual
    patches, containing learnable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Position embedding is important so that the algorithm knows where a patch is
    in the original image. In the case of the Masked Autoencoder, position embedding
    is provided by a 2D sine-cosine function that is typically used in transformer
    models. It encodes the position of a patch in a 2D grid of the overall image.
    The positional embedding may contain learnable parameters, but this does not appear
    to be the case in the implementation in the [MAE repository](https://github.com/facebookresearch/mae/blob/efb2a8062c206524e35e47d04501ed4f544c0ae8/util/pos_embed.py#L38).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3834b13d42f6bfe665feeea3305d63f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Masked autoencoder in action. Left: Masked patches of the original image. Middle.
    Reconstruction. Right: Ground truth. Image source: [https://arxiv.org/pdf/2111.06377.pdf](https://arxiv.org/pdf/2111.06377.pdf)
    (Figure 2)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Changes to the MAE Architecture**'
  prefs: []
  type: TYPE_NORMAL
- en: In order to process time series of satellite data with more channels, the NASA
    and IBM team made several changes to the Masked Autoencoder architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The 2D patch embedding was changed to a 3D patch embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 2D positional embedding was changed to a 3D positional embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The patch creation takes into account the 3D nature of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides RGB colors, one near-infrared and two short-wave infrared bands were
    added
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function**'
  prefs: []
  type: TYPE_NORMAL
- en: The mean squared error (MSE) loss is used for training by comparing the original
    and the reconstructed images pixel by pixel.
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The model training procedure is described on the IBM blog: [https://research.ibm.com/blog/nasa-hugging-face-ibm](https://research.ibm.com/blog/nasa-hugging-face-ibm).
    Unfortunately, not a lot of detail is provided. IBM mentions, however, that they
    trained on their company’s AI supercomputer Vela. [Vela](https://research.ibm.com/blog/AI-supercomputer-Vela-GPU-cluster)
    is a fully cloud-based supercomputer that operates exclusively for IBM Research.'
  prefs: []
  type: TYPE_NORMAL
- en: The supercomputer consists of 200 nodes. Each node is equipped with 8 NVIDIA
    A100 GPUs, each with 80 GB of GPU memory. The node RAM is 1.5 TB, and four 3.2
    TB local hard drives are available. This accounts for the large datasets that
    need to be handled to train foundation models. The nodes are connected with a
    network that can transfer up to 100 GB/second.
  prefs: []
  type: TYPE_NORMAL
- en: Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Prithvi-100M Geospatial AI Foundation Model can be applied to a variety
    of downstream tasks. We focus on two tasks: Flooding and crop type identification.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flooding**'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping the original encoder part of Prithvi-100M, the model is now adapted
    to predict the extension of flooding in a satellite image. Details are described
    on [Huggingface](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-sen1floods11).
    The [Sen1Floods11](https://github.com/cloudtostreet/Sen1Floods11) dataset is used
    for finetuning, covering 11 flood events on six continents.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83cc4025a63d4e795321d2d1a3479a8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finetuning the Geospatial AI Foundation Model for flood detection. Image source:
    [https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-sen1floods11/blob/main/sen1floods11-finetuning.png](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-sen1floods11/blob/main/sen1floods11-finetuning.png)'
  prefs: []
  type: TYPE_NORMAL
- en: In order to prepare Prithvi-100M for the downstream task, the embedding shape
    needs to be converted back to the original image shape. Then, a final 2D convolutional
    layer is added that applies the task-specific classification.
  prefs: []
  type: TYPE_NORMAL
- en: Each pixel in the image is classified as being either water or non-water (land).
    Since this is a classification problem, the binary cross-entropy loss is used.
    Only one image is processed at a time, so the time series functionality of Prithvi-100M
    is not used here.
  prefs: []
  type: TYPE_NORMAL
- en: The authors report a mean accuracy of 93% and mean intersection over union of
    86% for a holdout flood event in Bolivia.
  prefs: []
  type: TYPE_NORMAL
- en: A demo page is provided where users can upload their own Sentinel-2 images and
    ask Prithvi-100M to identify flooding.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20093b5ca8b238b689f912a7d4da62e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Snapshot of the demo for flood identification. Black pixels correspond to land,
    white pixels to water. Image source: [https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-sen1floods11-demo](https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-sen1floods11-demo)
    (Using India_900498_S2Hand.tif)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Crop type identification**'
  prefs: []
  type: TYPE_NORMAL
- en: To leverage the time series functionality, the authors provide a demo for crop
    type identification. The crop type ground truth is given by labeled images. This
    is a multi-class classification problem, and cross-entropy loss is used for training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/790eec887281215a817d52b393442904.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Multi-temporal crop type classification as a downstream task for Prithvi-100M.
    Image source: [https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification/blob/main/multi_temporal_crop_classification.png](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification/blob/main/multi_temporal_crop_classification.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors report different accuracies for the different crop types. On average,
    the accuracy is 64%, and intersection over union is 46%. However, the authors
    note that the ground truth is noisy, and more accurate labels would help to improve
    this downstream task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e53d8ac5ed3840af8ae01d0223a6c806.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Crop type demo for Prithvi-100M. The three figures on the left show the time
    series of satellite images. The right figure shows the model prediction, with
    each pixel colored according to the crop type. Image source: [https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification-demo](https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-multi-temporal-crop-classification-demo)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have covered the Geospatial AI Foundation Model, currently (2023) the largest
    geospatial model on Huggingface under the name Prithvi-100M. The model is developed
    by IBM Research and NASA and uses the Landsat dataset for training.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered the training data, architecture, and training procedure of the
    Geospatial AI Foundation Model. The model is available as open source and can
    be fine-tuned for more specific tasks. Flood detection and crop type identification
    applications have shown the great potential of the Geospatial AI Foundation Model.
  prefs: []
  type: TYPE_NORMAL
- en: Since Sentinel-2 data is available for individual non-commercial use, it would
    be possible for interested users to create their own model tuned to a specific
    downstream task. In a future post, I will show how to fine-tune the Geospatial
    AI Foundation Model for vegetation identification and super-resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Jakubik et al, Prithvi-100M, [https://github.com/NASA-IMPACT/hls-foundation-os](https://github.com/NASA-IMPACT/hls-foundation-os),
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prithvi-100M on Huggingface: [https://huggingface.co/ibm-nasa-geospatial](https://huggingface.co/ibm-nasa-geospatial)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentinel-2 satellite bands: [https://gisgeography.com/sentinel-2-bands-combinations/](https://gisgeography.com/sentinel-2-bands-combinations/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al (2021), “Masked Autoencoders are Scalable Vision Learners”, [https://arxiv.org/pdf/2111.06377.pdf](https://arxiv.org/pdf/2111.06377.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dosovitskiy et al (2020), “An Image is Worth 16x16 Words: Transformers for
    Image Recognition at Scale”, ICLR 2020, [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](/environmental-data-science-an-introduction-127b4b3422dc?source=post_page-----dbf356c746a9--------------------------------)
    [## Environmental Data Science: An Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: Examples, challenges, and perspectives for working with environmental data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/environmental-data-science-an-introduction-127b4b3422dc?source=post_page-----dbf356c746a9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
