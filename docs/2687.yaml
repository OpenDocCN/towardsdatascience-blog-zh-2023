- en: Can We Stop LLMs from Hallucinating?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/can-we-stop-llms-from-hallucinating-17c4ebd652c6?source=collection_archive---------6-----------------------#2023-08-24](https://towardsdatascience.com/can-we-stop-llms-from-hallucinating-17c4ebd652c6?source=collection_archive---------6-----------------------#2023-08-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Opinion
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*One of the greatest barriers to widespread LLM adoption may be inherently
    unsolvable.*'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@juras.jursenas?source=post_page-----17c4ebd652c6--------------------------------)[![Juras
    Juršėnas](../Images/eb2ca720f2c8688dbf8079879c028d12.png)](https://medium.com/@juras.jursenas?source=post_page-----17c4ebd652c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----17c4ebd652c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----17c4ebd652c6--------------------------------)
    [Juras Juršėnas](https://medium.com/@juras.jursenas?source=post_page-----17c4ebd652c6--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3041473d9e3c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-stop-llms-from-hallucinating-17c4ebd652c6&user=Juras+Jur%C5%A1%C4%97nas&userId=3041473d9e3c&source=post_page-3041473d9e3c----17c4ebd652c6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----17c4ebd652c6--------------------------------)
    ·5 min read·Aug 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F17c4ebd652c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-stop-llms-from-hallucinating-17c4ebd652c6&user=Juras+Jur%C5%A1%C4%97nas&userId=3041473d9e3c&source=-----17c4ebd652c6---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F17c4ebd652c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-we-stop-llms-from-hallucinating-17c4ebd652c6&source=-----17c4ebd652c6---------------------bookmark_footer-----------)![](../Images/09bc28622320bd28b5b369807b75f40d.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Google DeepMind](https://unsplash.com/@googledeepmind?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: While Large Language Models (LLMs) have captured the attention of nearly everyone,
    wide-scale deployment of such technology is slightly limited due to a rather annoying
    aspect of it — these models tend to hallucinate. In simple terms, they sometimes
    just make things up, and worst of all, it often looks highly convincing.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinations, frequent or not, bring with them two major issues. They can’t
    be directly implemented in many sensitive or brittle fields where a single mistake
    can be highly costly. In addition, it sows general distrust as users are expected
    to verify everything coming out of an LLM, which, at least in part, defeats the
    purpose of such technology.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Academia seems to also think that hallucinations are a major problem, as there
    are dozens of research papers in 2023 discussing and attempting to solve the issue.
    I, however, [would tend to agree with Yann LeCun](https://www.youtube.com/watch?v=mViTAXCg1xQ&feature=youtu.be),
    Meta’s Chief AI Scientist, that the hallucinations are not resolvable at all.
    We would need a complete revamp of the technology to eliminate the issue.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Hallucinating false statements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two important aspects to any LLM which, I think, make hallucinations
    unsolvable. Starting with the rather obvious technological underpinning, LLMs,
    like any other machine learning model, are stochastic in nature. In simple terms,
    they make predictions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: While they’re certainly much more advanced than “glorified autocomplete,” the
    underlying technology still uses statistical predictions about tokens. It’s both
    one of the strengths and weaknesses of LLMs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: On the strong part, we have seen how amazingly good they are at predicting what
    should come after an input (barring any intentional attempt to ruin an output).
    Users can make several types of mistakes, such as leaving in a typo, misunderstanding
    the meaning of a word, etc., and LLMs are still likely to get the output right.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Back in the day, when the first text-based games were created, users were asked
    to input commands without any errors or room for interpretation. A command such
    as “move north” would error out if the user inputted “move morth”. An LLM, however,
    might be able to infer the meaning in both cases. In that sense, the technology
    is truly fascinating.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Yet, it also showcases a weakness. Any input has a wide potential decision tree
    for token choice. In simple terms, there’s always a huge range of ways a model
    can create an output. Out of that large range, a relatively small bit is the “correct”
    decision.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: While there are numerous [optimization options available](https://github.com/hwchase17/langchain),
    the problem itself is not solvable. For example, if we increase the likelihood
    of providing one specific answer, the LLM becomes a lookup table, so we’d want
    to keep a balance. The underlying technology is simply based on stochastic predictions,
    and there has to be some room for a wider range of output tokens provided.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: But there’s another problem that LLMs cannot solve, at least in their current
    state. It’s a bit more ephemeral and abstract as it relates to epistemology, the
    field of philosophy that studies the nature of knowledge. On the face of it, the
    problem is simple — how do we know which statements are true, and how do we gain
    such knowledge? After all, a hallucination is simply a set of false statements
    *post-hoc*, so if we could create a way for the model to verify that it has made
    a false statement and remove it, that would solve the problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Separating hallucinations from truthful statements
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following in the footsteps of philosophy, we can separate two types of possible
    statements — analytic and synthetic. The former are statements that are true by
    virtue of definition (one of the most common examples is “a bachelor is an unmarried
    man”). In simple terms, we can find statements that are true by analyzing the
    language itself, and no external experience is required.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic statements are any statements that are true by virtue of some form
    of experience, such as “there is an apple on the table in front of me.” There’s
    no way to know whether such a statement is true without referring to direct experience.
    Pure linguistic analysis does no good in determining whether it is true or false.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: I should note that the distinction between these statements has been hotly contested
    for hundreds of years, but the discussion is largely irrelevant for LLMs. As their
    name might state, they’re a highly advanced linguistic analysis and prediction
    machine.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Following the distinction between the two types, we can see that LLMs would
    have little to no issue with analytic statements (or at least as much as humans
    do). Yet, they have no access to experience or the world at large. There’s no
    way for them to know that some statements are true by virtue of an event.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The major issue is that the number of analytic statements is significantly smaller
    than the set of all synthetic statements. Since an LLM has no way of verifying
    whether these statements are true, we, as humans, have to provide them with such
    information.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: As such, LLMs run into a challenge. The set of all possible outputs will always
    have some number of synthetic statements, but to the model, all of them are truth-value
    agnostic. In simple terms, “Julius Caesar’s assassin was Brutus” (there were many,
    but for this case, it doesn’t matter) and “Julius Caesar’s assassin was Abraham
    Lincoln” are equivalent to a model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: A counterargument might be that we have not had any direct experience about
    those events, either. We just read about them in books. But the discovery of the
    truthfulness of the statement is based on a reconstruction of surviving accounts
    and a wide range of other archaeological evidence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: A simpler example of an (albeit less relevant) statement would be “it is raining
    today.” Such statements are impossible to determine as true for an LLM at all
    as it needs access to real-world experience at the moment of query.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的（虽然相关性较低）例子是“今天在下雨。” 对于大型语言模型来说，这样的陈述是无法确定其真实性的，因为它需要在查询时接触到现实世界的经验。
- en: In one sense, the epistemological problem is self-solving. Our literary corpus
    would make the output that “Julius Caesar’s assassin was Brutus” significantly
    more likely due to it being present more frequently. Yet, again, the problem is
    that such a self-solving solution relies on training an LLM on absolutely all
    available textual information, which, obviously, is impossible. Additionally,
    that would make other, less truthful outputs not entirely absent from the set
    of all possible outputs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，认识论问题是自我解决的。我们的文学语料库会使“凯撒的刺客是布鲁图斯”这种输出变得显著更可能，因为它出现得更频繁。然而，再次强调的是，这种自我解决的方法依赖于在绝对所有可用文本信息上训练大型语言模型，这显然是不可能的。此外，这也会使其他不那么真实的输出仍然存在于所有可能输出的集合中。
- en: As such, data quality becomes an important factor, but that quality can only
    be judged by human observers. Even in cases where models are trained on enormous
    amounts of data, there’s a certain selection process that takes place, which means
    that the error rate for synthetic statements cannot be eliminated.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据质量变得非常重要，但这种质量只能由人类观察者来判断。即使模型在大量数据上进行训练，仍然会有一定的选择过程，这意味着合成陈述的错误率无法消除。
- en: Conclusion
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: I believe that the problem of stopping models from hallucinating is unsolvable.
    For one, the technology itself is based on a stochastic process, which inevitably,
    over a large number of outputs, will lead to erroneous predictions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，阻止模型产生幻觉的问题是无法解决的。一方面，技术本身基于随机过程，这不可避免地会在大量输出中导致错误的预测。
- en: In addition to the technological hurdle, there’s the question of whether LLMs
    can make truth-value judgments about statements, which, again, I believe is impossible
    as they have no access to the real world. The issue is slightly attenuated by
    various search engine functions that are now available for many LLMs, according
    to which they may verify certain statements.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了技术难题，还有一个问题是大型语言模型是否可以对陈述做出真实性判断，我再次认为这是不可能的，因为它们无法接触到现实世界。这个问题在很多大型语言模型现在提供的各种搜索引擎功能中稍微得到了缓解，这些功能可以验证某些陈述。
- en: It might be possible, however, to collect a database against which statements
    can be tested, but that would require something beyond the technology itself,
    which leads us back to the initial problem.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可能会有一种方法是收集一个可以测试陈述的数据库，但这需要超出技术本身的东西，这将我们带回到最初的问题。
