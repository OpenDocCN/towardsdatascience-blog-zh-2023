- en: 'Inside GPT — I : Understanding the text generation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/inside-gpt-i-1e8840ca8093?source=collection_archive---------1-----------------------#2023-08-21](https://towardsdatascience.com/inside-gpt-i-1e8840ca8093?source=collection_archive---------1-----------------------#2023-08-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A simple explanation of the model behind ChatGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fatih-demirci?source=post_page-----1e8840ca8093--------------------------------)[![Fatih
    Demirci](../Images/f60108429c4fac601a511f38954982bf.png)](https://medium.com/@fatih-demirci?source=post_page-----1e8840ca8093--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e8840ca8093--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e8840ca8093--------------------------------)
    [Fatih Demirci](https://medium.com/@fatih-demirci?source=post_page-----1e8840ca8093--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·
  prefs: []
  type: TYPE_NORMAL
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe4aaee0b8cc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-i-1e8840ca8093&user=Fatih+Demirci&userId=e4aaee0b8cc3&source=post_page-e4aaee0b8cc3----1e8840ca8093---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e8840ca8093--------------------------------)
    ·11 min read·Aug 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e8840ca8093&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-i-1e8840ca8093&user=Fatih+Demirci&userId=e4aaee0b8cc3&source=-----1e8840ca8093---------------------clap_footer-----------)'
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e8840ca8093&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Finside-gpt-i-1e8840ca8093&source=-----1e8840ca8093---------------------bookmark_footer-----------)![](../Images/96d044269a15eb2a232bf41b6912193c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Generative AI in 19th century (created through midjourney)*'
  prefs: []
  type: TYPE_NORMAL
- en: Regularly engaging with colleagues across diverse domains, I enjoy the challenge
    of conveying machine learning concepts to people who have little to no background
    in data science. Here, I attempt to explain how GPT is wired in simple terms,
    only this time in written form.
  prefs: []
  type: TYPE_NORMAL
- en: Behind ChatGPT’s popular magic, there is an unpopular logic. You write a prompt
    to ChatGPT and it generates text and whether it is accurate, it resembles human
    answers. How is it able to understand your prompt and generate coherent and comprehensible
    answers?
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformer Neural Networks.** The architecture designed to process unstructured
    data in vast amounts, in our case, text. When we say architecture, what we mean
    is essentially a series of mathematical operations that were made in several layers
    in parallel. Through this system of equations, several innovations were introduced
    that helped us overcome the long-existing challenges of text generation. The challenges
    that we were struggling to solve up until 5 years ago.'
  prefs: []
  type: TYPE_NORMAL
- en: If GPT has already been here for 5 years (indeed GPT paper was published in
    2018), isn’t GPT old news? Why has it become immensely popular recently? What
    is the difference between GPT 1, 2, 3, 3.5 (ChatGPT ) and 4?
  prefs: []
  type: TYPE_NORMAL
- en: All GPT versions were built on the same architecture. However each following
    model contained more parameters and trained using larger text datasets. There
    were obviously other novelties introduced by the later GPT releases especially
    in the training processes like reinforcement learning through human feedback which
    we will explain in the 3rd part of this blog series.
  prefs: []
  type: TYPE_NORMAL
- en: '**Vectors, matrices, tensors.** All these fancy words are essentially units
    that contain chunks of numbers. Those numbers go through a series of mathematical
    operations(mostly multiplication and summation) until we reach optimal output
    values, which are the probabilities of the possible outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: Output values? In this sense, it is the text generated by the language model,
    right? Yes. Then, what are the input values? Is it my prompt? Yes, but not entirely.
    So what else is behind?
  prefs: []
  type: TYPE_NORMAL
- en: Before going on to the different text decoding strategies, which will be the
    topic of the following blog post, it is useful to remove the ambiguity. Let’s
    go back to fundamental question that we asked at the start. How does it understand
    human language?
  prefs: []
  type: TYPE_NORMAL
- en: '**Generative Pre-trained Transformers**. Three words that GPT abbreviation
    stands for. We touched the Transformer part above that it represents the architecture
    where heavy calculations are made. But what do we calculate exactly? Where do
    you even get the numbers? It is a language model and all you do is to input some
    text. How can you calculate text?'
  prefs: []
  type: TYPE_NORMAL
- en: Data is agnostic. All data is same whether in the form of text, sound or image.¹
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Tokens**. We split the text into small chunks (tokens) and assign an unique
    number to each one of them(token ID). Models don’t know words, images or audio
    recordings. They learn to represent them in huge series of numbers (parameters)
    that serves us as a tool to illustrate the characteristics of things in numerical
    forms. Tokens are the language units that convey meaning and token IDs are the
    unique numbers that encode tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, how we tokenise the language can vary. Tokenisation can involve splitting
    texts into sentences, words, parts of words(sub-words), or even individual characters.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a scenario where we have 50,000 tokens in our language corpus(similar
    to GPT-2 which has 50,257). How do we represent those units after tokenisation?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Above is an example sentence tokenised into words. Tokenisation approaches can
    differ in their implementation. What’s important for us to understand right now
    is that we acquire numerical representations of language units(tokens) through
    their corresponding token IDs. So, now that we have these token IDs, can we simply
    input them directly into the model where calculations take place?
  prefs: []
  type: TYPE_NORMAL
- en: Cardinality matters in math. 101 and 2493 as token representation will matter
    to model. Because remember, all we are doing is mainly multiplications and summations
    of big chunks of numbers. So multiplying a number with either with 101 or with
    2493 will matter. Then, how can we make sure a token that is represented with
    number 101 is not less important than 2493, just because we happen to tokenise
    it arbitrarily so? How can we encode the words without causing a fictitious ordering?
  prefs: []
  type: TYPE_NORMAL
- en: '**One-hot encoding.** Sparse mapping of tokens. One-hot encoding is the technique
    where we project each token as a binary vector. That means only one single element
    in the vector is 1 (“hot”) and the rest is 0 (“cold”).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed15944a808e67ecb32c4eb54ed5764e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by the author: one-hot encoding vector example'
  prefs: []
  type: TYPE_NORMAL
- en: The tokens are represented with a vector which has length of total token in
    our corpus. In simpler terms, if we have 50k tokens in our language, every token
    is represented by a vector 50k in which only one element is 1 and the rest is
    0\. Since every vector in this projection contains only one non-zero element,
    it is named as sparse representation. However, as you might think this approach
    is very inefficient. Yes, we manage to remove the artificial cardinality between
    the token ids but we can’t extrapolate any information about the semantics of
    the words. We can’t understand whether the word “party” refers to a celebration
    or to a political organisation by using sparse vectors. Besides, representing
    every token with a vector of size 50k will mean, in total of 50k vector of length
    50k. This is very inefficient in terms of required memory and computation. Fortunately
    we have better solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Embeddings**. Dense representation of tokens. Tokenised units pass through
    an embedding layer where each token is transformed into continuous vector representation
    of a fixed size. For example in the case of GPT 3, each token in is represented
    by a vector of 768 numbers. These numbers are assigned randomly which then are
    being learned by the model after seeing lots of data(training).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Above is the embedding vector example of the word “party”.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have 50,000x786 size of vectors which is compare to 50,000x50,000 one-hot
    encoding is significantly more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding vectors will be the inputs to the model. Thanks to dense numerical
    representations we will able to capture the semantics of words, the embedding
    vectors of tokens that are similar will be closer to each other.
  prefs: []
  type: TYPE_NORMAL
- en: How can you measure the similarity of two language unit in context? There are
    several functions that can measure the similarity between the two vectors of same
    size. Let’s explain it with an example.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a simple example where we have the embedding vectors of tokens “cat”
    , “dog”, “car” and “banana”. For simplification let’s use an embedding size of
    4\. That means there will be four learned numbers to represent the each token.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Using the vectors above lets calculate the similarity scores using the cosine
    similarity. Human logic would find the word “dog” and “cat” more similar to each
    other than the words banana a car. Can we expect math to simulate our logic?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the words “cat” and “dog” have very high similarity score whereas
    the words “car” and “banana” have very low. Now imagine embedding vectors of length
    768 instead of 4 for each 50000 token in our language corpus. That’s how we are
    able find the words that are related to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s have a look at the two sentences below which have higher semantic
    complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The word “party” from the first and second sentence conveys different meanings.
    How are large language models capable of mapping out the difference between the
    “party” as a political organisation and “party” as celebrating social event?
  prefs: []
  type: TYPE_NORMAL
- en: Can we distinguish the different meanings of same token by relying on the token
    embeddings? The truth is, although embeddings provide us a range of advantages,
    they are not adequate to disentangle the entire complexity of semantic challenges
    of human language.
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-attention.** The solution was again offered by transformer neural networks.
    We generate new set of weights(another name for parameters) that are namely query,
    key and value matrices. Those weights learn to represent the embedding vectors
    of tokens as a new set of embeddings. How? Simply by taking the weighted average
    of the original embeddings. Each token “attends” to every other token(including
    to itself) in the input sentence and calculates set of attention weights or in
    other word the new so called “*contextual embeddings*”.'
  prefs: []
  type: TYPE_NORMAL
- en: All it does really is to map the importance of the words in the input sentence
    by assigning new set of numbers(attention weights) that are calculated using the
    token embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed88f41fed77de49bb001eb5825d92c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by the author: Attention weights of a token in different contexts (BertViz
    attention-head view)'
  prefs: []
  type: TYPE_NORMAL
- en: Above visualisation demonstrates the “attention” of the token “party” to the
    rest of the tokens in two sentences. The boldness of the connection signals to
    the importance or the relevance of the tokens. Attention and “attending” are the
    terms that refer to a new series of numbers(attention parameters) and their magnitude,
    that we use to represent the importance of words numerically. In the first sentence
    the word “party” attends to the word “celebrate” the most, whereas in the second
    sentence the word “deputy” has the highest attention. That’s how the model is
    able to incorporate the context by examining surrounding words.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned in the attention mechanism we derive new set of weight matrices,
    namely: Query, Key and Value (simply q,k,v). They are cascading matrices of same
    size(usually smaller than the embedding vectors) that are introduced to the architecture
    to capture complexity in the language units. Attention parameters are learned
    in order to demystify the relationship between the words, pairs of words, pairs
    of pairs of words and pairs of pairs of pairs of words and so on. Below is the
    visualisation of the query, key and value matrices in finding the most relevant
    word.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5f2be5cf83c1a11c0fa4bb2dc718135.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by the author: illustration of query key and value matrices and their
    final probabilities(BertViz q,k,v view)'
  prefs: []
  type: TYPE_NORMAL
- en: The visualisation illustrates the q and k vectors as vertical bands, where the
    boldness of each band reflects its magnitude. The connections between tokens signify
    the weights determined by attention, indicating that the q vector for “party”
    aligns most significantly with the k vector for “is”, “deputy” and “respected”.
  prefs: []
  type: TYPE_NORMAL
- en: To make the attention mechanism and the concepts of q, k and v less abstract,
    imagine that you went to a party and heard an amazing song that you fell in love
    with. After the party you are dying to find the song and listen again but you
    only remember barely 5 words from the lyrics and a part of the song melody(query).
    To find the song, you decide to go through the party playlist(keys) and listen(similarity
    function) all the songs in the list that was played at the party. When you finally
    recognise the song, you note the name of the song(value).
  prefs: []
  type: TYPE_NORMAL
- en: One last important trick that transformers introduced is to add the positional
    encodings to the vector embeddings. Simply because we would like to capture the
    position information of the word. It enhances our chances to predict the next
    token more accurately towards to the true sentence context. It is essential information
    because often swapping the words changes the context entirely. For instance, the
    sentences *“Tim chased clouds all his life”* vs *“clouds chased Tim all his life”*
    are absolutely different in essence.
  prefs: []
  type: TYPE_NORMAL
- en: All the mathematical tricks that we explored at a basic level so far, have the
    objective of predicting the next token, given the sequence of input tokens. Indeed,
    GPT is trained on one simple task which is the text generation or in other words
    the next token prediction. At its core of the matter, we measure the probability
    of a token, given the sequence of tokens appeared before it.
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder how do models learn the optimal numbers from randomly assigned
    numbers. It is a topic for another blog post probably however that is actually
    fundamental on understanding. Besides, it is a great sign that you are already
    questioning the basics. To remove unclarity, we use an optimisation algorithm
    that adjusts the parameters based on a metric that is called loss function. This
    metric is calculated by comparing the predicted values with the actual values.
    The model tracks the changes of the metric and depending on how small or large
    the value of loss, it tunes the numbers. This process is done until the loss can
    not be smaller given the rules we set in the algorithm that we call hyperparameters.
    An example hyperparameter can be, how frequently we want to calculate the loss
    and tune the weights. This is the rudimentary idea behind learning.
  prefs: []
  type: TYPE_NORMAL
- en: I hope in this short post, I was able to clear the picture at least a little
    bit. The second part of this blog series will focus on decoding strategies namely
    on why your prompt matters. The third and the last part will be dedicated to key
    factor on ChatGPT’s success which is the reinforcement learning through human
    feedback. Many thanks for the read. Until next time.
  prefs: []
  type: TYPE_NORMAL
- en: '**References:**'
  prefs: []
  type: TYPE_NORMAL
- en: A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser,
    and I. Polosukhin, “Attention Is All You Need,” in Advances in Neural Information
    Processing Systems 30 (NIPS 2017), 2017.
  prefs: []
  type: TYPE_NORMAL
- en: 'J. Vig, “A Multiscale Visualization of Attention in the Transformer Model,”
    In Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations, pp. 37–42, Florence, Italy, Association for
    Computational Linguistics, 2019.'
  prefs: []
  type: TYPE_NORMAL
- en: 'L. Tunstall, L. von Werra, and T. Wolf, “Natural Language Processing with Transformers,
    Revised Edition,” O’Reilly Media, Inc., Released May 2022, ISBN: 9781098136796.'
  prefs: []
  type: TYPE_NORMAL
- en: '[1 -Lazy Programmer Blog](https://lazyprogrammer.me/blog/)'
  prefs: []
  type: TYPE_NORMAL
